{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "import timm\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Add the src directory to the sys.path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# from secret_runway_detection.model import CombinedModel, SegmentationHead\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "\n",
    "# BUFFER_TYPE = 'cross'\n",
    "RUN_PATH = 'esedx12/secret-runway-detection/380m92co'\n",
    "\n",
    "# IS_ON_WANDB = True\n",
    "# RUN_NAME = 'neat-energy-31'\n",
    "# MODEL_NAME = 'model:v7'\n",
    "\n",
    "# # Model input and output dimensions\n",
    "# INPUT_IMAGE_SIDE_LEN_PX = 224  # in pixels\n",
    "# TILES_PER_INPUT_AREA_LEN = 224  # Number of tiles per side in one input area\n",
    "\n",
    "# Threshold for converting model outputs to binary predictions\n",
    "THRESHOLD = 0.5  # Adjust based on validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Path to the trained model checkpoint\n",
    "# MODEL_CHECKPOINT_PATH = '../checkpoints/trained_model.pth'  # Update this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'skilled-glade-49'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the W&B run\n",
    "train_run = wandb.Api().run(RUN_PATH)\n",
    "train_run.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Load the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model:v20, 374.16MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('../artifacts/skilled-glade-49.pth')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from more_itertools import one\n",
    "\n",
    "# Fetch the model artifact from the W&B run\n",
    "artifacts = train_run.logged_artifacts()\n",
    "model_artifacts = [a for a in artifacts if a.type == 'model']\n",
    "artifact = one(model_artifacts)\n",
    "state_dict_dir = artifact.download(root='../artifacts/')\n",
    "state_dict_dir = Path(state_dict_dir)\n",
    "state_dict_path = state_dict_dir / f'{train_run.name}.pth'\n",
    "state_dict_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: simple\n"
     ]
    }
   ],
   "source": [
    "model_type = train_run.config['model_type']\n",
    "print(f\"Model type: {model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too few items in iterable (expected 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/more_itertools/more.py:560\u001b[0m, in \u001b[0;36mone\u001b[0;34m(iterable, too_short, too_long)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     first_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now load the type backbone_config artifact the same way you loaded the model and put it in a temp directory\u001b[39;00m\n\u001b[1;32m      2\u001b[0m backbone_config_artifacts \u001b[38;5;241m=\u001b[39m [a \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artifacts \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackbone_config\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m backbone_config_artifact \u001b[38;5;241m=\u001b[39m \u001b[43mone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackbone_config_artifacts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m backbone_config_dir \u001b[38;5;241m=\u001b[39m backbone_config_artifact\u001b[38;5;241m.\u001b[39mdownload(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../artifacts/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m backbone_config_dir \u001b[38;5;241m=\u001b[39m Path(backbone_config_dir)\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/more_itertools/more.py:562\u001b[0m, in \u001b[0;36mone\u001b[0;34m(iterable, too_short, too_long)\u001b[0m\n\u001b[1;32m    560\u001b[0m     first_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(it)\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 562\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m (\n\u001b[1;32m    563\u001b[0m         too_short \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoo few items in iterable (expected 1)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    564\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    567\u001b[0m     second_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(it)\n",
      "\u001b[0;31mValueError\u001b[0m: too few items in iterable (expected 1)"
     ]
    }
   ],
   "source": [
    "# Now load the type backbone_config artifact the same way you loaded the model and put it in a temp directory\n",
    "backbone_config_artifacts = [a for a in artifacts if a.type == 'backbone_config']\n",
    "backbone_config_artifact = one(backbone_config_artifacts)\n",
    "backbone_config_dir = backbone_config_artifact.download(root='../artifacts/')\n",
    "backbone_config_dir = Path(backbone_config_dir)\n",
    "backbone_config_path = backbone_config_dir / f'{train_run.name}.yaml'\n",
    "backbone_config_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model model:v20\n",
      "wandb-history run-380m92co-history:v0\n"
     ]
    }
   ],
   "source": [
    "list(artifacts)\n",
    "\n",
    "for artifact in artifacts:\n",
    "    print(artifact.type, artifact.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then load the model like this\n",
    "# model = get_model(config['model_type'], BACKBONE_CFG_PATH, backbone_weights_path, output_size=config['resolution']).to(device)\n",
    "\n",
    "from GFM.models import build_model\n",
    "\n",
    "# Load the model\n",
    "model = build_model(model_type, backbone_config_path, state_dict_path, output_size=train_run.config['resolution']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model checkpoint from ../checkpoints dir\n",
    "# backbone = timm.create_model(\n",
    "#         'swin_base_patch4_window7_224',\n",
    "#         pretrained=False,\n",
    "#         num_classes=0,  # Assuming binary classification\n",
    "#     )\n",
    "\n",
    "# segmentation_head = SegmentationHead()\n",
    "\n",
    "# model = CombinedModel(backbone, segmentation_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Model from WandB, which we saved as state dict\n",
    "model.load_state_dict(torch.load(state_dict_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold = train_run.summary['best_threshold']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load validation images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = Path(\n",
    "    f'../training_data_{train_run.config['resolution']}/training_data_{train_run.config[\"training_dataset\"]}')\n",
    "\n",
    "images_dir = train_dir / 'images'\n",
    "labels_dir = train_dir / 'labels'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all filenames in the images directory\n",
    "import re\n",
    "\n",
    "\n",
    "all_filenames = os.listdir(images_dir)\n",
    "\n",
    "# Initialize dictionaries and lists\n",
    "strip_to_files = {}        # For files with strip numbers\n",
    "possibly_empty_files = []  # For 'possibly_empty' files\n",
    "\n",
    "# Regular expression pattern to match filenames with strip numbers\n",
    "pattern = re.compile(r'^area_\\d+_of_strip_(\\d+)\\.npy$')\n",
    "\n",
    "# Process filenames\n",
    "for filename in all_filenames:\n",
    "    if 'possibly_empty' in filename:\n",
    "        # This is a 'possibly_empty' file\n",
    "        possibly_empty_files.append(filename)\n",
    "    else:\n",
    "        # Try to match the pattern to extract strip number\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            strip_number = int(match.group(1))\n",
    "            # Add filename to the list for this strip number\n",
    "            strip_to_files.setdefault(strip_number, []).append(filename)\n",
    "        else:\n",
    "            print(f\"Filename does not match expected pattern: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_strip_numbers = train_run.config['val_strip_numbers']\n",
    "\n",
    "val_files = []\n",
    "for strip_num in val_strip_numbers:\n",
    "    val_files.extend(strip_to_files[strip_num])\n",
    "\n",
    "train_strip_numbers = train_run.config['train_strip_numbers']\n",
    "\n",
    "train_files = []\n",
    "for strip_num in train_strip_numbers:\n",
    "    train_files.extend(strip_to_files[strip_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your transform if you have one; otherwise, set to None\n",
    "from secret_runway_detection.dataset import LandingStripDataset, SegmentationTransform\n",
    "\n",
    "segmentation_transform = SegmentationTransform()  # Replace with your actual transform if any\n",
    "# segmentation_transform = None\n",
    "\n",
    "# Create validation dataset\n",
    "val_dataset = LandingStripDataset(\n",
    "    images_dir=images_dir,\n",
    "    labels_dir=labels_dir,\n",
    "    file_list=val_files,\n",
    "    transform=segmentation_transform\n",
    ")\n",
    "\n",
    "# Create training dataset\n",
    "train_dataset = LandingStripDataset(\n",
    "    images_dir=images_dir,\n",
    "    labels_dir=labels_dir,\n",
    "    file_list=train_files,\n",
    "    transform=segmentation_transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Image, Label and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Function to visualize predictions with optional label overlay\n",
    "def visualize_predictions(model, dataset, num_samples=5, overlay=False):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            sample = dataset[i]\n",
    "            input_image = sample[0].unsqueeze(0)  # Add batch dimension\n",
    "            label = sample[1]\n",
    "            \n",
    "            # Generate prediction\n",
    "            prediction = model(input_image).squeeze(0)  # Remove batch dimension\n",
    "            \n",
    "            # Convert tensors to numpy arrays for visualization\n",
    "            input_image_np = input_image.squeeze(0).numpy().transpose(1, 2, 0)  # HWC format\n",
    "            label_np = label.numpy()\n",
    "            prediction_np = prediction.numpy().squeeze(0)\n",
    "            \n",
    "            # Create subplots\n",
    "            fig, ax = plt.subplots(1, 3 if not overlay else 3, figsize=(15, 5))\n",
    "            \n",
    "            # Display Input Image\n",
    "            ax_idx = 0\n",
    "            ax[ax_idx].imshow(input_image_np)\n",
    "            ax[ax_idx].set_title('Input Image')\n",
    "            ax[ax_idx].axis('off')\n",
    "            \n",
    "            if overlay:\n",
    "                # Overlay Label on Input Image\n",
    "                ax_idx += 1\n",
    "                ax[ax_idx].imshow(input_image_np)\n",
    "                ax[ax_idx].imshow(label_np, cmap='jet', alpha=0.5)  # Adjust alpha for transparency\n",
    "                ax[ax_idx].set_title('Input Image with Label Overlay')\n",
    "                ax[ax_idx].axis('off')\n",
    "                \n",
    "                # Display Prediction\n",
    "                ax_idx += 1\n",
    "                ax[ax_idx].imshow(prediction_np, cmap='gray')\n",
    "                ax[ax_idx].set_title('Prediction')\n",
    "                ax[ax_idx].axis('off')\n",
    "            else:\n",
    "                # Display Label\n",
    "                ax_idx += 1\n",
    "                ax[ax_idx].imshow(label_np, cmap='gray')\n",
    "                ax[ax_idx].set_title('Label')\n",
    "                ax[ax_idx].axis('off')\n",
    "                \n",
    "                # Display Prediction\n",
    "                ax_idx += 1\n",
    "                ax[ax_idx].imshow(prediction_np, cmap='gray')\n",
    "                ax[ax_idx].set_title('Prediction')\n",
    "                ax[ax_idx].axis('off')\n",
    "            \n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions for a few VALIDATION samples\n",
    "visualize_predictions(model, val_dataset, num_samples=5, overlay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions for a few TRAINING samples\n",
    "visualize_predictions(model, train_dataset, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
