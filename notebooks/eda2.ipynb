{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geo Foundation Model\n",
    "https://arxiv.org/pdf/2302.04476\n",
    "\n",
    "For pretraining, we employ 8 NVIDIA V100 GPUs with\n",
    "a batch size of 2048 (128 per GPU) and the image size\n",
    "of 192×192. All pretraining settings are the same as in\n",
    "[43]. For downstream tasks, 4 NVIDIA A10G GPUs are\n",
    "employed. During the pretraining stage, we utilize RGB\n",
    "bands as they are most commonly available among data\n",
    "sources and tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nb by looking for maximum in submission file i found that there are 1540x1540 tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eyeball_validate_train_utils.ipynb',\n",
       " 'etl.py',\n",
       " 'playground.ipynb',\n",
       " 'eda.ipynb',\n",
       " 'wandb',\n",
       " 'train.ipynb',\n",
       " 'etl.ipynb',\n",
       " 'inference.ipynb']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load model: [Errno 2] No such file or directory: 'simmim_pretrain/gfm.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_279227/3433952129.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Path to your gfm.pth model\n",
    "model_path = 'simmim_pretrain/gfm.pth'  # Replace with the actual path\n",
    "\n",
    "try:\n",
    "    # Attempt to load the entire model\n",
    "    # Use 'cuda' if GPU is available\n",
    "    model = torch.load(model_path, map_location='cpu')\n",
    "    print(\"Model loaded successfully!\")\n",
    "    print(model)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_279227/1916446933.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint keys: dict_keys(['model', 'optimizer', 'lr_scheduler', 'max_accuracy', 'epoch', 'config', 'amp'])\n",
      "input shape: torch.Size([1, 3, 224, 224])\n",
      "Model Output: tensor([[-0.4588,  0.1096, -0.6445,  ..., -0.0775,  0.5360,  1.1420]])\n",
      "Model Output Shape: torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import timm  # PyTorch Image Models library\n",
    "\n",
    "# Step 1: Instantiate the Swin Transformer model\n",
    "model = timm.create_model(\n",
    "    'swin_base_patch4_window7_224',  # Base model; adjust if necessary\n",
    "    pretrained=False,\n",
    "    num_classes=0  # Set to 0 for feature extraction; adjust as needed\n",
    ")\n",
    "\n",
    "# Step 2: Load the checkpoint\n",
    "model_path = '../simmim_pretrain/gfm.pth'  # Replace with the actual path\n",
    "checkpoint = torch.load(model_path, map_location='cpu')\n",
    "print(\"Checkpoint keys:\", checkpoint.keys())\n",
    "\n",
    "# Step 3: Extract and clean the state dictionary\n",
    "state_dict = checkpoint['model']\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith('module.'):\n",
    "        new_state_dict[k[len('module.'):]] = v\n",
    "    else:\n",
    "        new_state_dict[k] = v\n",
    "\n",
    "# Step 4: Load the state dictionary into the model\n",
    "# Use strict=True if you are sure of the match\n",
    "model.load_state_dict(new_state_dict, strict=False)\n",
    "model.eval()\n",
    "\n",
    "# Step 5: Prepare a random image tensor\n",
    "batch_size = 1\n",
    "channels = 3\n",
    "height = 224  # Match pretraining image size\n",
    "width = 224   # Match pretraining image size\n",
    "\n",
    "random_image = torch.randn(batch_size, channels, height, width)\n",
    "\n",
    "# Optional: Apply normalization\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "random_image = normalize(random_image)\n",
    "\n",
    "# Step 6: Perform a forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(random_image)\n",
    "\n",
    "print(\"input shape:\", random_image.shape)\n",
    "print(\"Model Output:\", output)\n",
    "print(\"Model Output Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shapefile not found at: pac_2024_training/pac_2024_training.shp\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emil/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# GeoAI Amazon Basin Secret Runway Detection\n",
    "# Data Inspection Script\n",
    "# ============================================\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Environment Setup\n",
    "# ----------------------------\n",
    "\n",
    "# Install required libraries (Uncomment if running for the first time)\n",
    "# !pip install geopandas matplotlib pandas\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Inspecting Training Data\n",
    "# ----------------------------\n",
    "\n",
    "# a. Define the Path to the Training Shapefile\n",
    "training_shapefile_path = os.path.join(\n",
    "    'pac_2024_training', 'pac_2024_training.shp')\n",
    "\n",
    "if not os.path.exists(training_shapefile_path):\n",
    "    print(f\"Training shapefile not found at: {training_shapefile_path}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# b. Load the Training Shapefile\n",
    "training_data = gpd.read_file(training_shapefile_path)\n",
    "print(\"Training Data Loaded Successfully!\")\n",
    "\n",
    "# c. Inspect the First Few Rows\n",
    "print(\"\\nFirst Five Rows of Training Data:\")\n",
    "print(training_data.head())\n",
    "\n",
    "# d. Check Data Structure and CRS\n",
    "print(\"\\nTraining Data Information:\")\n",
    "print(training_data.info())\n",
    "\n",
    "print(\"\\nTraining Data CRS:\")\n",
    "print(training_data.crs)\n",
    "\n",
    "# e. Explore Attribute Columns\n",
    "print(\"\\nTraining Data Columns:\")\n",
    "print(training_data.columns)\n",
    "\n",
    "if 'year' in training_data.columns:\n",
    "    print(\"\\nYear of Detection Counts:\")\n",
    "    print(training_data['year'].value_counts())\n",
    "else:\n",
    "    print(\"\\nNo 'year' column found in training data.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Inspecting Test AOIs\n",
    "# ----------------------------\n",
    "\n",
    "# a. List All Test Shapefiles\n",
    "test_shapefiles_dir = os.path.join('shp_test_AOIs', 'shp')\n",
    "\n",
    "if not os.path.isdir(test_shapefiles_dir):\n",
    "    print(f\"Test shapefiles directory not found at: {test_shapefiles_dir}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "test_shapefiles = [f for f in os.listdir(\n",
    "    test_shapefiles_dir) if f.endswith('.shp')]\n",
    "\n",
    "print(f\"\\nFound {len(test_shapefiles)} Test Shapefiles:\")\n",
    "for shp in test_shapefiles:\n",
    "    print(f\"- {shp}\")\n",
    "\n",
    "# b. Load and Inspect a Specific Test Shapefile\n",
    "if not test_shapefiles:\n",
    "    print(\"No test shapefiles found to inspect.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "test_shapefile_name = test_shapefiles[0]\n",
    "test_shapefile_path = os.path.join(test_shapefiles_dir, test_shapefile_name)\n",
    "\n",
    "test_data = gpd.read_file(test_shapefile_path)\n",
    "print(f\"\\nTest AOI Data Loaded Successfully: {test_shapefile_name}\")\n",
    "\n",
    "# c. Inspect the Test AOI Data\n",
    "print(\"\\nFirst Five Rows of Test AOI Data:\")\n",
    "print(test_data.head())\n",
    "\n",
    "print(\"\\nTest AOI Data Information:\")\n",
    "print(test_data.info())\n",
    "\n",
    "print(\"\\nTest AOI Data CRS:\")\n",
    "print(test_data.crs)\n",
    "\n",
    "\n",
    "# e. Overlay Training Runways on Test AOI\n",
    "if training_data.crs != test_data.crs:\n",
    "    training_data_converted = training_data.to_crs(test_data.crs)\n",
    "    print(\"\\nConverted training data CRS to match test AOI CRS.\")\n",
    "else:\n",
    "    training_data_converted = training_data\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "test_data.plot(ax=ax, color='lightgrey', edgecolor='black',\n",
    "               alpha=0.5, label='Test AOI')\n",
    "training_data_converted.plot(\n",
    "    ax=ax, color='red', edgecolor='black', alpha=0.5, label='Training Runways')\n",
    "plt.title('Training Runways Overlay on Test AOI')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# f. Spatial Relationship Analysis\n",
    "intersections = gpd.sjoin(training_data_converted,\n",
    "                          test_data, how='inner', predicate='intersects')\n",
    "print(\n",
    "    f\"\\nNumber of Training Runways Intersecting {test_shapefile_name}: {len(intersections)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Summary and Relationships\n",
    "# ----------------------------\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(\"\"\"\n",
    "- **Training Data**: Contains polygons of known clandestine runways with attributes such as year of detection. Used to train the detection model.\n",
    "- **Test AOIs**: Define areas where the model needs to detect potential runways for specified years.\n",
    "- **Submission CSVs**:\n",
    "    - `input.csv`: Labeled tile data for training/validation.\n",
    "    - `SampleSubmission.csv`: Template showing the required submission format with a large number of rows.\n",
    "    \n",
    "**Relationships**:\n",
    "- Train the model using training shapefiles and input.csv.\n",
    "- Apply the model to test AOIs to predict runways.\n",
    "- Generate a reduced submission file (~200,000 rows) based on predictions, focusing on detected runways and their buffer regions.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to your AOI shapefile\n",
    "shapefile_path = 'shp_test_AOIs/shp/aoi_2021_04.shp'\n",
    "\n",
    "# Load the shapefile\n",
    "aoi = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Display basic information\n",
    "print(\"=== Shapefile Information ===\")\n",
    "print(aoi.info())\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\n=== First Five Rows ===\")\n",
    "print(aoi.head())\n",
    "\n",
    "# Print Coordinate Reference System (CRS)\n",
    "print(\"\\n=== Coordinate Reference System (CRS) ===\")\n",
    "print(aoi.crs)\n",
    "\n",
    "# Plot the shapefile\n",
    "aoi.plot(color='lightblue', edgecolor='black')\n",
    "plt.title('AOI: aoi_2021_04.shp')\n",
    "plt.xlabel('Easting')\n",
    "plt.ylabel('Northing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, Point  # Imported Point for centroid\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "def parse_tile_identifier(tile_id):\n",
    "    \"\"\"\n",
    "    Parses the tile identifier to extract AOI year, month, row, and column.\n",
    "\n",
    "    Parameters:\n",
    "    - tile_id (str): Tile identifier in the format 'Tileaoi_YY_MM_row_col'\n",
    "\n",
    "    Returns:\n",
    "    - year (str): Four-digit year\n",
    "    - month (str): Two-digit month\n",
    "    - row (int): Tile row index\n",
    "    - col (int): Tile column index\n",
    "    \"\"\"\n",
    "    pattern = r'Tileaoi_(\\d{2})_(\\d{2})_(\\d+)_(\\d+)'\n",
    "    match = re.match(pattern, tile_id)\n",
    "    if not match:\n",
    "        raise ValueError(\n",
    "            \"Tile identifier does not match the expected format 'Tileaoi_YY_MM_row_col'.\")\n",
    "    yy, mm, row, col = match.groups()\n",
    "    year = f\"20{yy}\"\n",
    "    month = mm\n",
    "    return year, month, int(row), int(col)\n",
    "\n",
    "\n",
    "def calculate_tile_extent(aoi_bounds, row, col, total_rows, total_cols):\n",
    "    \"\"\"\n",
    "    Calculates the spatial extent (bounding box) of a specific tile within the AOI.\n",
    "\n",
    "    Parameters:\n",
    "    - aoi_bounds (tuple): (minx, miny, maxx, maxy) of the AOI\n",
    "    - row (int): Tile row index\n",
    "    - col (int): Tile column index\n",
    "    - total_rows (int): Total number of tile rows in the AOI\n",
    "    - total_cols (int): Total number of tile columns in the AOI\n",
    "\n",
    "    Returns:\n",
    "    - tile_polygon (Polygon): Shapely Polygon of the tile's bounding box\n",
    "    \"\"\"\n",
    "    minx, miny, maxx, maxy = aoi_bounds\n",
    "    tile_width = (maxx - minx) / total_cols\n",
    "    tile_height = (maxy - miny) / total_rows\n",
    "\n",
    "    tile_minx = minx + (col - 1) * tile_width\n",
    "    tile_maxx = minx + col * tile_width\n",
    "    tile_miny = miny + (row - 1) * tile_height\n",
    "    tile_maxy = miny + row * tile_height\n",
    "\n",
    "    tile_polygon = Polygon([\n",
    "        (tile_minx, tile_miny),\n",
    "        (tile_minx, tile_maxy),\n",
    "        (tile_maxx, tile_maxy),\n",
    "        (tile_maxx, tile_miny),\n",
    "        # (tile_minx, tile_miny)\n",
    "    ])\n",
    "\n",
    "    return tile_polygon\n",
    "\n",
    "\n",
    "def inspect_tile(tile_id, shapefiles_dir, total_rows, total_cols):\n",
    "    \"\"\"\n",
    "    Inspects a specific tile by plotting it within the corresponding AOI shapefile.\n",
    "\n",
    "    Parameters:\n",
    "    - tile_id (str): Tile identifier (e.g., 'Tileaoi_21_03_1047_1231')\n",
    "    - shapefiles_dir (str): Directory containing AOI shapefiles\n",
    "    - total_rows (int): Total number of tile rows in the AOI\n",
    "    - total_cols (int): Total number of tile columns in the AOI\n",
    "    \"\"\"\n",
    "    # Parse the tile identifier\n",
    "    year, month, row, col = parse_tile_identifier(tile_id)\n",
    "    print(\n",
    "        f\"AOI Year: {year}, AOI Month: {month}, Tile Row: {row}, Tile Column: {col}\")\n",
    "\n",
    "    # Determine the AOI shapefile path\n",
    "    aoi_filename = f\"aoi_{year}_{month}.shp\"\n",
    "    aoi_path = os.path.join(shapefiles_dir, aoi_filename)\n",
    "\n",
    "    if not os.path.exists(aoi_path):\n",
    "        raise FileNotFoundError(f\"AOI shapefile not found: {aoi_path}\")\n",
    "\n",
    "    # Load the AOI shapefile\n",
    "    aoi_gdf = gpd.read_file(aoi_path)\n",
    "    aoi_bounds = aoi_gdf.total_bounds  # [minx, miny, maxx, maxy]\n",
    "\n",
    "    # Calculate the tile's spatial extent\n",
    "    tile_polygon = calculate_tile_extent(\n",
    "        aoi_bounds, row, col, total_rows, total_cols)\n",
    "\n",
    "    # Create a GeoDataFrame for the tile\n",
    "    tile_gdf = gpd.GeoDataFrame([{'geometry': tile_polygon}], crs=aoi_gdf.crs)\n",
    "\n",
    "    # Reproject to Web Mercator for basemap compatibility\n",
    "    aoi_gdf = aoi_gdf.to_crs(epsg=3857)\n",
    "    tile_gdf = tile_gdf.to_crs(epsg=3857)\n",
    "\n",
    "    # Check if the tile is within the AOI\n",
    "    is_within = tile_gdf.within(aoi_gdf.unary_union).iloc[0]\n",
    "    print(f\"Is the tile within the AOI? {'Yes' if is_within else 'No'}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Added: Create and Plot Circle around Tile\n",
    "    # ----------------------------\n",
    "    # Calculate AOI dimensions\n",
    "    aoi_width = aoi_bounds[2] - aoi_bounds[0]\n",
    "    aoi_height = aoi_bounds[3] - aoi_bounds[1]\n",
    "    aoi_average = (aoi_width + aoi_height) / 2\n",
    "\n",
    "    # Define circle radius as 1/8 of the AOI's average dimension\n",
    "    circle_radius = aoi_average / 8\n",
    "\n",
    "    # Get the centroid of the tile\n",
    "    tile_centroid = tile_gdf.geometry.centroid.iloc[0]\n",
    "\n",
    "    # Create a circular polygon around the centroid\n",
    "    circle = tile_centroid.buffer(circle_radius)\n",
    "\n",
    "    # Create a GeoDataFrame for the circle\n",
    "    circle_gdf = gpd.GeoDataFrame([{'geometry': circle}], crs=aoi_gdf.crs)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Added: Create and Plot Circle around Tile\n",
    "    # ----------------------------\n",
    "    # Calculate AOI dimensions\n",
    "    aoi_width = aoi_bounds[2] - aoi_bounds[0]\n",
    "    aoi_height = aoi_bounds[3] - aoi_bounds[1]\n",
    "    aoi_average = (aoi_width + aoi_height) / 2\n",
    "\n",
    "    # Define circle radius as 1/8 of the AOI's average dimension\n",
    "    circle_radius = aoi_average / 8\n",
    "\n",
    "    # Get the centroid of the tile\n",
    "    tile_centroid = tile_gdf.geometry.centroid.iloc[0]\n",
    "\n",
    "    # Create a circular polygon around the centroid\n",
    "    circle = tile_centroid.buffer(circle_radius)\n",
    "\n",
    "    # Create a GeoDataFrame for the circle\n",
    "    circle_gdf = gpd.GeoDataFrame([{'geometry': circle}], crs=aoi_gdf.crs)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Plot AOI, Tile, Circle, and Zoomed Tile\n",
    "    # ----------------------------\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(24, 12))\n",
    "\n",
    "    # Plot 1: Full AOI with Tile and Circle\n",
    "    aoi_gdf.plot(ax=axes[0], color='lightblue',\n",
    "                 edgecolor='black', label='AOI', alpha=0.5)\n",
    "    if is_within:\n",
    "        tile_gdf.plot(ax=axes[0], color='red',\n",
    "                      edgecolor='black', alpha=0.5, label='Tile')\n",
    "    else:\n",
    "        tile_gdf.plot(ax=axes[0], color='gray', edgecolor='black',\n",
    "                      alpha=0.5, label='Tile (Outside AOI)')\n",
    "    circle_gdf.plot(ax=axes[0], facecolor='none',\n",
    "                    edgecolor='yellow', linewidth=2, label='Tile Circle')\n",
    "    ctx.add_basemap(axes[0], source=ctx.providers.Esri.WorldImagery, zoom=12)\n",
    "    axes[0].set_title(f'Inspecting {tile_id} within {aoi_filename}')\n",
    "    axes[0].set_xlabel('Easting')\n",
    "    axes[0].set_ylabel('Northing')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Plot 2: Zoomed-in Tile\n",
    "    tile_gdf.plot(ax=axes[1], color='lightblue',\n",
    "                  edgecolor='black', alpha=0.5, label='Tile')\n",
    "    # bounds = tile_polygon.bounds\n",
    "    # padding_coef = 1\n",
    "    # axes[1].set_xlim(bounds[0] - padding_coef * (bounds[2] - bounds[0]), bounds[2] + padding_coef * (bounds[2] - bounds[0]))\n",
    "    # axes[1].set_ylim(bounds[1] - padding_coef * (bounds[3] - bounds[1]), bounds[3] + padding_coef * (bounds[3] - bounds[1]))\n",
    "    ctx.add_basemap(\n",
    "        axes[1],\n",
    "        source=ctx.providers.Esri.WorldImagery,\n",
    "        zoom=12)  # Higher zoom level for detail\n",
    "    axes[1].set_title(f'Zoomed-in View of {tile_id}')\n",
    "    axes[1].set_xlabel('Easting')\n",
    "    axes[1].set_ylabel('Northing')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return is_within\n",
    "\n",
    "# ----------------------------\n",
    "# Example Usage\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the tile identifier you want to inspect\n",
    "    tile_identifier = 'Tileaoi_21_03_1047_1231'  # Replace with your tile ID\n",
    "\n",
    "    # Define the directory containing AOI shapefiles\n",
    "    shapefiles_dir = 'shp_test_AOIs/shp'  # Adjust if different\n",
    "\n",
    "    # Define the total number of rows and columns in the AOI grid\n",
    "    # **Important:** Replace these with actual numbers based on your tiling system\n",
    "    total_tile_rows = 2000  # Example value\n",
    "    total_tile_cols = 2000  # Example value\n",
    "\n",
    "    # Inspect the tile\n",
    "    try:\n",
    "        within_aoi = inspect_tile(\n",
    "            tile_identifier, shapefiles_dir, total_tile_rows, total_tile_cols)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Path to one of your AOI shapefiles\n",
    "# Replace with the desired shapefile\n",
    "aoi_shapefile = 'shp_test_AOIs/shp/aoi_2021_03.shp'\n",
    "\n",
    "# Load the shapefile\n",
    "aoi_gdf = gpd.read_file(aoi_shapefile)\n",
    "\n",
    "# Inspect the first few rows and columns\n",
    "print(aoi_gdf.head())\n",
    "print(aoi_gdf.columns)\n",
    "print(aoi_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "def lon_to_utm_zone(lon):\n",
    "    \"\"\"Calculate the UTM zone number from longitude, capped between 1 and 60.\"\"\"\n",
    "    zone = int((lon + 180) / 6) + 1\n",
    "    zone = min(max(zone, 1), 60)\n",
    "    return zone\n",
    "\n",
    "def get_utm_crs(lat, lon):\n",
    "    \"\"\"Return the EPSG code for the UTM CRS based on latitude and longitude.\"\"\"\n",
    "    zone = lon_to_utm_zone(lon)\n",
    "    if lat >= 0:\n",
    "        epsg_code = 32600 + zone  # Northern hemisphere\n",
    "    else:\n",
    "        epsg_code = 32700 + zone  # Southern hemisphere\n",
    "    return f'EPSG:{epsg_code}'\n",
    "\n",
    "def calculate_aoi_metrics(shp_dir):\n",
    "    \"\"\"\n",
    "    Calculate area, width, and height for each shapefile in the directory.\n",
    "\n",
    "    Parameters:\n",
    "    - shp_dir (str): Path to the directory containing shapefiles.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing metrics for each AOI.\n",
    "    \"\"\"\n",
    "    # List all .shp files in the directory\n",
    "    shp_files = [f for f in os.listdir(shp_dir) if f.endswith('.shp')]\n",
    "\n",
    "    # Initialize a list to store results\n",
    "    results = []\n",
    "\n",
    "    # Process each shapefile\n",
    "    for shp_file in shp_files:\n",
    "        # Full path to the shapefile\n",
    "        shp_path = os.path.join(shp_dir, shp_file)\n",
    "        \n",
    "        try:\n",
    "            # Read the shapefile using GeoPandas\n",
    "            gdf = gpd.read_file(shp_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {shp_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Ensure the GeoDataFrame has a CRS\n",
    "        if gdf.crs is None:\n",
    "            print(f\"Shapefile {shp_file} has no CRS. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Reproject to WGS84 if not already\n",
    "        if gdf.crs.to_string() != 'EPSG:4326':\n",
    "            gdf = gdf.to_crs('EPSG:4326')\n",
    "\n",
    "        # Select the first geometry (assuming one AOI per shapefile)\n",
    "        try:\n",
    "            runway = gdf.iloc[0]\n",
    "            geometry = runway.geometry\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing geometry in {shp_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Calculate centroid\n",
    "        centroid = geometry.centroid\n",
    "        lon, lat = centroid.x, centroid.y\n",
    "\n",
    "        # Get appropriate UTM CRS\n",
    "        utm_crs = get_utm_crs(lat, lon)\n",
    "\n",
    "        try:\n",
    "            # Reproject to UTM CRS for accurate measurements\n",
    "            gdf_projected = gdf.to_crs(utm_crs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reprojecting {shp_file} to {utm_crs}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Calculate area in square meters\n",
    "        try:\n",
    "            area = gdf_projected['geometry'].area.iloc[0]\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating area for {shp_file}: {e}\")\n",
    "            area = None\n",
    "\n",
    "        # Get the bounding box dimensions (minx, miny, maxx, maxy)\n",
    "        try:\n",
    "            minx, miny, maxx, maxy = gdf_projected.total_bounds\n",
    "            width = maxx - minx  # Width in meters\n",
    "            height = maxy - miny  # Height in meters\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating dimensions for {shp_file}: {e}\")\n",
    "            width = height = None\n",
    "\n",
    "        # Append the results\n",
    "        results.append({\n",
    "            'Shapefile': shp_file,\n",
    "            'Area (sq m)': round(area, 2) if area else None,\n",
    "            'Width (m)': round(width, 2) if width else None,\n",
    "            'Height (m)': round(height, 2) if height else None\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Directory containing the shapefiles\n",
    "    shp_dir = 'shp_test_AOIs/shp'  # Update this path if necessary\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics_df = calculate_aoi_metrics(shp_dir)\n",
    "\n",
    "    # Display the results\n",
    "    print(metrics_df)\n",
    "\n",
    "    # Optionally, save to a CSV file\n",
    "    metrics_df.to_csv('aoi_metrics.csv', index=False)\n",
    "    print(\"Metrics have been saved to 'aoi_metrics.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect_crs.py\n",
    "\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import pyproj\n",
    "import warnings\n",
    "\n",
    "def find_geospatial_files(directory, extensions=['.shp', '.geojson']):\n",
    "    \"\"\"\n",
    "    Recursively find all shapefiles and GeoJSON files in a directory.\n",
    "    Returns a list of file paths.\n",
    "    \"\"\"\n",
    "    geospatial_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if any(file.lower().endswith(ext) for ext in extensions):\n",
    "                geospatial_files.append(os.path.join(root, file))\n",
    "    return geospatial_files\n",
    "\n",
    "def get_shapefile_prj(shp_path):\n",
    "    \"\"\"\n",
    "    Given a shapefile path, return the path to its corresponding .prj file.\n",
    "    \"\"\"\n",
    "    base, _ = os.path.splitext(shp_path)\n",
    "    prj_path = base + '.prj'\n",
    "    if os.path.exists(prj_path):\n",
    "        return prj_path\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def read_prj_crs(prj_path):\n",
    "    \"\"\"\n",
    "    Read the CRS from a .prj file using pyproj.\n",
    "    Returns the EPSG code if possible, else returns the full WKT string.\n",
    "    \"\"\"\n",
    "    with open(prj_path, 'r') as prj_file:\n",
    "        prj_text = prj_file.read()\n",
    "    try:\n",
    "        crs = pyproj.CRS.from_wkt(prj_text)\n",
    "        if crs.to_epsg():\n",
    "            return f\"EPSG:{crs.to_epsg()}\"\n",
    "        else:\n",
    "            return crs.to_string()\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Unable to parse CRS from {prj_path}: {e}\")\n",
    "        return \"Unknown CRS\"\n",
    "\n",
    "def inspect_crs(files, target_crs='EPSG:32718'):\n",
    "    \"\"\"\n",
    "    Inspect and print CRS of given geospatial files.\n",
    "    For shapefiles, ensure that the .prj file matches the CRS inferred by GeoPandas.\n",
    "    For GeoJSON files, confirm they are in the expected CRS (default EPSG:4326).\n",
    "    Warns if discrepancies are found.\n",
    "    \"\"\"\n",
    "    if not files:\n",
    "        print(\"No geospatial files found to inspect.\")\n",
    "        return\n",
    "    \n",
    "    summary = {\n",
    "        'Shapefiles': {'Total': 0, 'With PRJ': 0, 'Without PRJ': 0, 'CRS Match': 0, 'CRS Mismatch': 0},\n",
    "        'GeoJSON': {'Total': 0, 'Expected CRS': 0, 'Different CRS': 0}\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== CRS Inspection Report ===\\n\")\n",
    "    \n",
    "    for file in files:\n",
    "        file_lower = file.lower()\n",
    "        if file_lower.endswith('.shp'):\n",
    "            summary['Shapefiles']['Total'] += 1\n",
    "            prj_path = get_shapefile_prj(file)\n",
    "            print(f\"Inspecting Shapefile: {file}\")\n",
    "            \n",
    "            if prj_path:\n",
    "                summary['Shapefiles']['With PRJ'] += 1\n",
    "                prj_crs = read_prj_crs(prj_path)\n",
    "                print(f\"  .prj CRS: {prj_crs}\")\n",
    "            else:\n",
    "                summary['Shapefiles']['Without PRJ'] += 1\n",
    "                prj_crs = \"Not Defined\"\n",
    "                print(\"  Warning: Missing .prj file.\")\n",
    "            \n",
    "            try:\n",
    "                gdf = gpd.read_file(file)\n",
    "                gdf_crs = gdf.crs.to_string() if gdf.crs else \"Not Defined\"\n",
    "                print(f\"  GeoPandas Inferred CRS: {gdf_crs}\")\n",
    "                \n",
    "                if prj_crs != \"Not Defined\":\n",
    "                    if gdf_crs == prj_crs:\n",
    "                        summary['Shapefiles']['CRS Match'] += 1\n",
    "                        print(\"  Status: ✅ CRS Match\\n\")\n",
    "                    else:\n",
    "                        summary['Shapefiles']['CRS Mismatch'] += 1\n",
    "                        print(\"  Status: ❌ CRS Mismatch\\n\")\n",
    "                        warnings.warn(f\"CRS mismatch in {file}: .prj indicates {prj_crs}, but GeoPandas infers {gdf_crs}.\")\n",
    "                else:\n",
    "                    print(\"  Status: ❌ CRS Undefined\\n\")\n",
    "                    warnings.warn(f\"CRS undefined for {file}.\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"  Error reading {file}: {e}\\n\")\n",
    "                warnings.warn(f\"Error reading {file}: {e}\")\n",
    "        \n",
    "        elif file_lower.endswith('.geojson'):\n",
    "            summary['GeoJSON']['Total'] += 1\n",
    "            print(f\"Inspecting GeoJSON: {file}\")\n",
    "            try:\n",
    "                gdf = gpd.read_file(file)\n",
    "                gdf_crs = gdf.crs.to_string() if gdf.crs else \"Not Defined\"\n",
    "                print(f\"  Inferred CRS: {gdf_crs}\")\n",
    "                \n",
    "                if 'EPSG:4326' == gdf_crs:\n",
    "                    summary['GeoJSON']['Expected CRS'] += 1\n",
    "                    print(\"  Status: ✅ Expected CRS (EPSG:4326)\\n\")\n",
    "                else:\n",
    "                    summary['GeoJSON']['Different CRS'] += 1\n",
    "                    print(\"  Status: ❌ Different CRS than Expected (EPSG:4326)\\n\")\n",
    "                    warnings.warn(f\"GeoJSON {file} has CRS {gdf_crs}, expected EPSG:4326.\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"  Error reading {file}: {e}\\n\")\n",
    "                warnings.warn(f\"Error reading {file}: {e}\")\n",
    "    \n",
    "    # Summary Report\n",
    "    print(\"\\n=== Summary ===\\n\")\n",
    "    print(\"Shapefiles:\")\n",
    "    print(f\"  Total Shapefiles: {summary['Shapefiles']['Total']}\")\n",
    "    print(f\"  With .prj: {summary['Shapefiles']['With PRJ']}\")\n",
    "    print(f\"  Without .prj: {summary['Shapefiles']['Without PRJ']}\")\n",
    "    print(f\"  CRS Match: {summary['Shapefiles']['CRS Match']}\")\n",
    "    print(f\"  CRS Mismatch: {summary['Shapefiles']['CRS Mismatch']}\\n\")\n",
    "    \n",
    "    print(\"GeoJSON Files:\")\n",
    "    print(f\"  Total GeoJSON Files: {summary['GeoJSON']['Total']}\")\n",
    "    print(f\"  Expected CRS (EPSG:4326): {summary['GeoJSON']['Expected CRS']}\")\n",
    "    print(f\"  Different CRS: {summary['GeoJSON']['Different CRS']}\\n\")\n",
    "    \n",
    "    # Warnings\n",
    "    if summary['Shapefiles']['CRS Mismatch'] > 0 or summary['GeoJSON']['Different CRS'] > 0:\n",
    "        print(\"Warnings were issued for CRS discrepancies. Please review the warnings above.\")\n",
    "    else:\n",
    "        print(\"All inspected files have consistent and expected CRSs.\")\n",
    "\n",
    "def main():\n",
    "    # Directories to inspect\n",
    "    training_dirs = ['pac_2024_training/', 'shp_test_AOIs/']\n",
    "    \n",
    "    # Find all geospatial files in these directories\n",
    "    all_files = []\n",
    "    for directory in training_dirs:\n",
    "        if os.path.exists(directory):\n",
    "            print(f\"Searching in Directory: {directory}\")\n",
    "            files = find_geospatial_files(directory)\n",
    "            if files:\n",
    "                print(f\"Found {len(files)} geospatial file(s) in {directory}.\\n\")\n",
    "                all_files.extend(files)\n",
    "            else:\n",
    "                print(f\"No geospatial files found in {directory}.\\n\")\n",
    "        else:\n",
    "            print(f\"Directory {directory} does not exist.\\n\")\n",
    "    \n",
    "    # Inspect CRS of all found files\n",
    "    inspect_crs(all_files)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
