{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Landing Strip Detection Training Pipeline\n",
    "\n",
    "\n",
    "\n",
    " This notebook implements a training pipeline for detecting landing strips using satellite imagery. The pipeline includes:\n",
    "\n",
    "\n",
    "\n",
    " - Loading input landing strip data.\n",
    "\n",
    " - Creating input areas around the landing strips.\n",
    "\n",
    " - Downloading Sentinel-2 imagery from Google Earth Engine.\n",
    "\n",
    " - Preparing a dataset for training.\n",
    "\n",
    " - Loading the Geo Foundation Model (GFM) for transfer learning.\n",
    "\n",
    " - Setting up a training loop with Weights & Biases (wandb) logging.\n",
    "\n",
    "\n",
    "\n",
    " **Note**: Ensure that you have authenticated with Google Earth Engine (GEE) using `ee.Authenticate()` and have initialized it with `ee.Initialize()`. Also, make sure `train_utils.py` is in your working directory or Python path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                     Version     Editable project location\n",
      "--------------------------- ----------- ------------------------------------------\n",
      "aenum                       3.1.15\n",
      "affine                      2.4.0\n",
      "aiohappyeyeballs            2.4.3\n",
      "aiohttp                     3.10.10\n",
      "aiosignal                   1.3.1\n",
      "annotated-types             0.7.0\n",
      "antlr4-python3-runtime      4.9.3\n",
      "asttokens                   2.4.1\n",
      "attrs                       24.2.0\n",
      "bitsandbytes                0.44.1\n",
      "blessings                   1.7\n",
      "branca                      0.8.0\n",
      "cachetools                  5.5.0\n",
      "certifi                     2024.8.30\n",
      "charset-normalizer          3.4.0\n",
      "click                       8.1.7\n",
      "click-plugins               1.1.1\n",
      "cligj                       0.7.2\n",
      "comm                        0.2.2\n",
      "contextily                  1.6.2\n",
      "contourpy                   1.3.0\n",
      "cycler                      0.12.1\n",
      "debugpy                     1.8.7\n",
      "decorator                   5.1.1\n",
      "docker-pycreds              0.4.0\n",
      "docstring_parser            0.16\n",
      "earthengine-api             1.1.3\n",
      "efficientnet_pytorch        0.7.1\n",
      "einops                      0.8.0\n",
      "executing                   2.1.0\n",
      "filelock                    3.16.1\n",
      "fiona                       1.10.1\n",
      "folium                      0.17.0\n",
      "fonttools                   4.54.1\n",
      "frozenlist                  1.4.1\n",
      "fsspec                      2024.9.0\n",
      "geographiclib               2.0\n",
      "geopandas                   1.0.1\n",
      "geopy                       2.4.1\n",
      "gitdb                       4.0.11\n",
      "GitPython                   3.1.43\n",
      "google-api-core             2.21.0\n",
      "google-api-python-client    2.149.0\n",
      "google-auth                 2.35.0\n",
      "google-auth-httplib2        0.2.0\n",
      "google-cloud-core           2.4.1\n",
      "google-cloud-storage        2.18.2\n",
      "google-crc32c               1.6.0\n",
      "google-resumable-media      2.7.2\n",
      "googleapis-common-protos    1.65.0\n",
      "httplib2                    0.22.0\n",
      "huggingface-hub             0.25.2\n",
      "hydra-core                  1.3.2\n",
      "idna                        3.10\n",
      "importlib_resources         6.4.5\n",
      "iniconfig                   2.0.0\n",
      "ipykernel                   6.29.5\n",
      "ipython                     8.28.0\n",
      "jedi                        0.19.1\n",
      "Jinja2                      3.1.4\n",
      "joblib                      1.4.2\n",
      "jsonargparse                4.33.2\n",
      "jupyter_client              8.6.3\n",
      "jupyter_core                5.7.2\n",
      "kiwisolver                  1.4.7\n",
      "kornia                      0.7.3\n",
      "kornia_rs                   0.1.5\n",
      "lightly                     1.5.13\n",
      "lightly-utils               0.0.2\n",
      "lightning                   2.4.0\n",
      "lightning-utilities         0.11.7\n",
      "markdown-it-py              3.0.0\n",
      "MarkupSafe                  3.0.1\n",
      "matplotlib                  3.9.2\n",
      "matplotlib-inline           0.1.7\n",
      "mdurl                       0.1.2\n",
      "mercantile                  1.2.1\n",
      "mpmath                      1.3.0\n",
      "multidict                   6.1.0\n",
      "munch                       4.0.0\n",
      "nest-asyncio                1.6.0\n",
      "networkx                    3.4.1\n",
      "numpy                       2.1.2\n",
      "nvidia-cublas-cu12          12.1.3.1\n",
      "nvidia-cuda-cupti-cu12      12.1.105\n",
      "nvidia-cuda-nvrtc-cu12      12.1.105\n",
      "nvidia-cuda-runtime-cu12    12.1.105\n",
      "nvidia-cudnn-cu12           9.1.0.70\n",
      "nvidia-cufft-cu12           11.0.2.54\n",
      "nvidia-curand-cu12          10.3.2.106\n",
      "nvidia-cusolver-cu12        11.4.5.107\n",
      "nvidia-cusparse-cu12        12.1.0.106\n",
      "nvidia-nccl-cu12            2.20.5\n",
      "nvidia-nvjitlink-cu12       12.6.77\n",
      "nvidia-nvtx-cu12            12.1.105\n",
      "omegaconf                   2.3.0\n",
      "opencv-python               4.10.0.84\n",
      "packaging                   24.1\n",
      "pandas                      2.2.3\n",
      "parso                       0.8.4\n",
      "pexpect                     4.9.0\n",
      "pillow                      10.4.0\n",
      "pip                         24.2\n",
      "platformdirs                4.3.6\n",
      "plotly                      5.24.1\n",
      "pluggy                      1.5.0\n",
      "polars                      1.9.0\n",
      "pretrainedmodels            0.7.4\n",
      "prompt_toolkit              3.0.48\n",
      "propcache                   0.2.0\n",
      "proto-plus                  1.24.0\n",
      "protobuf                    5.28.2\n",
      "psutil                      6.0.0\n",
      "ptyprocess                  0.7.0\n",
      "pure_eval                   0.2.3\n",
      "pyasn1                      0.6.1\n",
      "pyasn1_modules              0.4.1\n",
      "pydantic                    2.9.2\n",
      "pydantic_core               2.23.4\n",
      "Pygments                    2.18.0\n",
      "pyogrio                     0.10.0\n",
      "pyparsing                   3.1.4\n",
      "pyproj                      3.7.0\n",
      "pytest                      8.3.3\n",
      "python-dateutil             2.9.0.post0\n",
      "pytorch-lightning           2.4.0\n",
      "pytz                        2024.2\n",
      "PyYAML                      6.0.2\n",
      "pyzmq                       26.2.0\n",
      "rasterio                    1.3.11\n",
      "requests                    2.32.3\n",
      "rich                        13.9.2\n",
      "rsa                         4.9\n",
      "Rtree                       1.3.0\n",
      "safetensors                 0.4.5\n",
      "scikit-learn                1.5.2\n",
      "scipy                       1.14.1\n",
      "secret_runway_detection     0.1         /home/emil/Desktop/Secret Runway Detection\n",
      "segmentation-models-pytorch 0.3.4\n",
      "sentry-sdk                  2.17.0\n",
      "setproctitle                1.3.3\n",
      "setuptools                  65.5.0\n",
      "shapely                     2.0.6\n",
      "six                         1.16.0\n",
      "smmap                       5.0.1\n",
      "snuggs                      1.4.7\n",
      "stack-data                  0.6.3\n",
      "sympy                       1.13.3\n",
      "tenacity                    9.0.0\n",
      "tensorboardX                2.6.2.2\n",
      "threadpoolctl               3.5.0\n",
      "timm                        0.9.7\n",
      "torch                       2.4.1\n",
      "torchgeo                    0.6.1\n",
      "torchmetrics                1.4.3\n",
      "torchvision                 0.19.1\n",
      "tornado                     6.4.1\n",
      "tqdm                        4.66.5\n",
      "traitlets                   5.14.3\n",
      "triton                      3.0.0\n",
      "typeshed_client             2.7.0\n",
      "typing_extensions           4.12.2\n",
      "tzdata                      2024.2\n",
      "uritemplate                 4.1.1\n",
      "urllib3                     2.2.3\n",
      "wandb                       0.18.3\n",
      "wcwidth                     0.2.13\n",
      "xyzservices                 2024.9.0\n",
      "yacs                        0.1.8\n",
      "yarl                        1.15.0\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'secret_runway_detection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check if the secret_runway_detection package is installed\u001b[39;00m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip list\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msecret_runway_detection\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'secret_runway_detection'"
     ]
    }
   ],
   "source": [
    "# Check if the secret_runway_detection package is installed\n",
    "!pip list\n",
    "\n",
    "import secret_runway_detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emil/Desktop/Secret Runway Detection/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'secret_runway_detection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Add the src directory to the sys.path\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# sys.path.append(os.path.abspath(os.path.join('..', 'src')))\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Import functions and constants from train_utils\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msecret_runway_detection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     landing_strips_to_enclosing_input_areas,\n\u001b[1;32m     27\u001b[0m     input_area_to_input_image,\n\u001b[1;32m     28\u001b[0m     make_label_tensor,\n\u001b[1;32m     29\u001b[0m     TILE_SIDE_LEN,\n\u001b[1;32m     30\u001b[0m     TILES_PER_AREA_LEN,\n\u001b[1;32m     31\u001b[0m     INPUT_IMAGE_HEIGHT,\n\u001b[1;32m     32\u001b[0m     INPUT_IMAGE_WIDTH,\n\u001b[1;32m     33\u001b[0m     RANDOM_SEED\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msecret_runway_detection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LandingStripDataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'secret_runway_detection'"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import ee\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pyproj\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm  # PyTorch Image Models library\n",
    "from shapely.geometry import Polygon, Point\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Add the src directory to the sys.path\n",
    "# sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "\n",
    "# Import functions and constants from train_utils\n",
    "from secret_runway_detection.train_utils import (\n",
    "    landing_strips_to_enclosing_input_areas,\n",
    "    input_area_to_input_image,\n",
    "    make_label_tensor,\n",
    "    TILE_SIDE_LEN,\n",
    "    TILES_PER_AREA_LEN,\n",
    "    INPUT_IMAGE_HEIGHT,\n",
    "    INPUT_IMAGE_WIDTH,\n",
    "    RANDOM_SEED\n",
    ")\n",
    "\n",
    "from secret_runway_detection.dataset import LandingStripDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Configuration and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "# Debug flag: Set to True to run on CPU, False to use GPU if available\n",
    "DEBUG = True\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cpu') if DEBUG else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project='secret-runway-detection', mode='online' if not DEBUG else 'dryrun')\n",
    "\n",
    "# Authenticate and initialize Earth Engine\n",
    "ee.Authenticate()\n",
    "ee.Initialize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Load Landing Strips Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 154 landing strips.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Path to the landing strips shapefile\n",
    "landing_strips_shp = '../pac_2024_training/pac_2024_training.shp'  # Update this path as needed\n",
    "\n",
    "# Load the landing strips shapefile\n",
    "landing_strips = gpd.read_file(landing_strips_shp)\n",
    "\n",
    "# Ensure CRS is WGS84\n",
    "if landing_strips.crs != 'EPSG:4326':\n",
    "    landing_strips = landing_strips.to_crs('EPSG:4326')\n",
    "\n",
    "print(f\"Loaded {len(landing_strips)} landing strips.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Create Input Areas Around Landing Strips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GeoDataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2978472/3329766706.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# %%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Use the function from train_utils to create input areas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_tiles_per_area_side_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTILES_PER_AREA_LEN\u001b[0m  \u001b[0;31m# From train_utils constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0minput_areas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanding_strips_to_enclosing_input_areas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanding_strips\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tiles_per_area_side_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Created {len(input_areas)} input areas.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Secret Runway Detection/src/train_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(landing_strips, num_tiles_per_area_side_len)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0moverlaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# Append the area regardless of overlaps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0minput_areas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_areas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'geometry'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marea_polygon\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total overlapping areas: {overlap_count}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Secret Runway Detection/venv/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'GeoDataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Use the function from train_utils to create input areas\n",
    "num_tiles_per_area_side_len = TILES_PER_AREA_LEN  # From train_utils constants\n",
    "input_areas = landing_strips_to_enclosing_input_areas(landing_strips, num_tiles_per_area_side_len)\n",
    "\n",
    "print(f\"Created {len(input_areas)} input areas.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Define the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Standard ImageNet normalization\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "dataset = LandingStripDataset(input_areas, landing_strips, transform=transform)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. Load the Geo Foundation Model (GFM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2978472/2463872552.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and moved to device.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def load_gfm_model(model_path):\n",
    "    \"\"\"\n",
    "    Loads the Geo Foundation Model (GFM) from a checkpoint.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_path (str): Path to the model checkpoint.\n",
    "    \n",
    "    Returns:\n",
    "    - model (torch.nn.Module): Loaded model.\n",
    "    \"\"\"\n",
    "    model = timm.create_model(\n",
    "        'swin_base_patch4_window7_224',\n",
    "        pretrained=False,\n",
    "        num_classes=1  # Assuming binary classification\n",
    "    )\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    \n",
    "    # Extract the state dictionary\n",
    "    if 'model' in checkpoint:\n",
    "        state_dict = checkpoint['model']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    # Clean the state dictionary (remove 'module.' prefix if present)\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith('module.'):\n",
    "            new_state_dict[k[len('module.'):]] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    \n",
    "    # Load the state dictionary\n",
    "    model.load_state_dict(new_state_dict, strict=False)\n",
    "    model = model.to(device)\n",
    "    print(\"Model loaded and moved to device.\")\n",
    "    return model\n",
    "\n",
    "# Path to the pre-trained GFM model\n",
    "model_path = '../simmim_pretrain/gfm.pth'  # Replace with your actual model path\n",
    "\n",
    "# Load the model\n",
    "model = load_gfm_model(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7. Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Suitable for binary classification with logits\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Optionally, define a learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 8. Training Loop with wandb Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "num_epochs = 10  # Adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze(1)  # Adjust dimensions if necessary\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9 or i == len(dataloader) - 1:  # Log every 10 batches or last batch\n",
    "            avg_loss = running_loss / 10\n",
    "            print(f\"[Epoch {epoch + 1}, Batch {i + 1}] Loss: {avg_loss:.4f}\")\n",
    "            wandb.log({'epoch': epoch + 1, 'batch': i + 1, 'loss': avg_loss})\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Optionally, log learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    wandb.log({'learning_rate': current_lr})\n",
    "    print(f\"Epoch {epoch + 1} completed. Learning Rate: {current_lr}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 9. Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Save the trained model\n",
    "model_save_path = 'trained_model.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to '{model_save_path}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 10. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "print(\"\"\"\n",
    "# Training Summary\n",
    "\n",
    "- **Model**: Swin Transformer (GFM) loaded from pre-trained checkpoint.\n",
    "- **Dataset**: Landing strips with Sentinel-2 imagery.\n",
    "- **Loss Function**: BCEWithLogitsLoss.\n",
    "- **Optimizer**: Adam with learning rate scheduler.\n",
    "- **Logging**: Weights & Biases (wandb) for experiment tracking.\n",
    "- **Device**: {}\n",
    "- **Epochs**: {}\n",
    "\n",
    "Training has been completed and the model has been saved.\n",
    "\"\"\".format(device, num_epochs))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
