{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Landing Strip Detection Training Pipeline\n",
    "\n",
    "\n",
    "\n",
    " This notebook implements a training pipeline for detecting landing strips using satellite imagery. The pipeline includes:\n",
    "\n",
    "\n",
    "\n",
    " - Loading input landing strip data.\n",
    "\n",
    " - Creating input areas around the landing strips.\n",
    "\n",
    " - Downloading Sentinel-2 imagery from Google Earth Engine.\n",
    "\n",
    " - Preparing a dataset for training.\n",
    "\n",
    " - Loading the Geo Foundation Model (GFM) for transfer learning.\n",
    "\n",
    " - Setting up a training loop with Weights & Biases (wandb) logging.\n",
    "\n",
    "\n",
    "\n",
    " **Note**: Ensure that you have authenticated with Google Earth Engine (GEE) using `ee.Authenticate()` and have initialized it with `ee.Initialize()`. Also, make sure `train_utils.py` is in your working directory or Python path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *TODO*\n",
    "* Max value of model outputs can be rather small (in one case, 0.6244). This leads to binary search setting threshold lower, predicting all zeroes\n",
    "* (buffered_labels.float() == 1).float()\n",
    "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
    "(buffered_labels.float() == 1).float().mean()\n",
    "tensor(0.2678) **(!!!)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using Colab\n",
      "/home/emil/Desktop/secret-runway-detection/notebooks\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import timm  # PyTorch Image Models library\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Function to check if running in Colab\n",
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        print(\"Using Colab\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"Not using Colab\")\n",
    "        return False\n",
    "\n",
    "USING_COLAB = is_colab()\n",
    "\n",
    "\n",
    "if USING_COLAB:\n",
    "    # For running on colab\n",
    "    os.chdir('/content/secret-runway-detection/notebooks')\n",
    "    ! pip install -q yacs\n",
    "    from google.colab import drive, userdata\n",
    "    drive.mount('/content/drive')\n",
    "    wandb.login(key=userdata.get('WANDB_API_KEY'))\n",
    "\n",
    "if USING_COLAB and not os.path.exists('/content/secret-runway-detection'):\n",
    "    print(\"Cloning the secret-runway-detection repository...\")\n",
    "    !git clone https://github.com/emilschmitz/secret-runway-detection.git /content/secret-runway-detection\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path.append(os.path.abspath('../GFM'))\n",
    "\n",
    "# Import functions and constants from train_utils\n",
    "from secret_runway_detection.model import (\n",
    "    get_model,\n",
    ")\n",
    "from secret_runway_detection.dataset import LandingStripDataset, SegmentationTransform\n",
    "from secret_runway_detection.train_utils import (\n",
    "    RANDOM_SEED,\n",
    ")\n",
    "from secret_runway_detection.eval_utils import (\n",
    "    compute_validation_accuracy,\n",
    "    compute_baseline_accuracy,\n",
    ")\n",
    "\n",
    "from GFM.models import build_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Configuration and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Debug flag: Set to True to run on CPU, False to use GPU if available\n",
    "# With DEBUG == True, test and train sets are reduced to 10 samples each\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cpu') if DEBUG else torch.device(\n",
    "    'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# logging.getLogger('secret_runway_detection.train_utils').setLevel(logging.DEBUG)\n",
    "logging.getLogger('secret_runway_detection.train_utils').setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    'training_dataset': 'point',\n",
    "    'resolution': 192,\n",
    "    'train_percentage': 0.8,\n",
    "    'model_type': 'simple',\n",
    "    'num_epochs': 50 if not DEBUG else 2,\n",
    "    'batch_size': 32 if USING_COLAB else 4,\n",
    "    'lr_head': 1e-3,\n",
    "    'lr_backbone': 1e-6,\n",
    "    'lr_step_size': 10,\n",
    "    'lr_gamma': 0.3,\n",
    "    'early_stopping_patience': 3,\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project='secret-runway-detection',\n",
    "           mode='online' if not DEBUG else 'dryrun',\n",
    "           dir='..',\n",
    "           tags=[\n",
    "                config['training_dataset'],\n",
    "                'colab' if USING_COLAB else 'local',\n",
    "                'PRETRAINED',\n",
    "           ],\n",
    "           job_type='train',\n",
    "           config=config,\n",
    "           )\n",
    "\n",
    "BACKBONE_CFG_PATH = '../configs/gfm_config.yaml'\n",
    "\n",
    "# Log the config.yaml file as an artifact\n",
    "artifact = wandb.Artifact('gfm_config', type='backbone_config')\n",
    "artifact.add_file(BACKBONE_CFG_PATH)\n",
    "\n",
    "if not wandb.run.name:\n",
    "    wandb.run.name = f\"Run from {pd.Timestamp.now()}\"\n",
    "\n",
    "print(wandb.run.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Load Data into Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_train_dir = Path(f'../training_data_{config[\"resolution\"]}')\n",
    "child_train_dir = Path(f'training_data_{config[\"training_dataset\"]}')\n",
    "train_dir = root_train_dir / child_train_dir\n",
    "\n",
    "if USING_COLAB:\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    print(f\"Created directories: {train_dir} (including parents if they didn't exist)\")\n",
    "\n",
    "    import zipfile\n",
    "\n",
    "    # Define the path to the training data zip on Google Drive\n",
    "    drive_train_zip = Path(f'/content/drive/MyDrive/Secret Runway Detection Challenge/training_data_{config[\"resolution\"]}/training_data_{config[\"training_dataset\"]}.zip')\n",
    "\n",
    "    # Define the destination path where the zip will be copied\n",
    "    dest_train_zip = Path(f'{train_dir}.zip')\n",
    "\n",
    "    # Copy the zip file from Drive to the destination\n",
    "    print(f\"Copying training data from {drive_train_zip} to {dest_train_zip}...\")\n",
    "    !cp \"{drive_train_zip}\" \"{dest_train_zip}\"\n",
    "    print(os.listdir(train_dir))\n",
    "\n",
    "    # Unzip the training data\n",
    "    with zipfile.ZipFile(dest_train_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(train_dir)\n",
    "    \n",
    "    print(\"Training data setup completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 1637\n",
      "Total strips: 113\n",
      "Training files: 1305\n",
      "Testing files: 332\n"
     ]
    }
   ],
   "source": [
    "\n",
    "images_dir = train_dir / 'images'\n",
    "labels_dir = train_dir / 'labels'\n",
    "\n",
    "# Get all filenames in the images directory\n",
    "all_filenames = os.listdir(images_dir)\n",
    "\n",
    "# Initialize dictionaries and lists\n",
    "strip_to_files = {}        # For files with strip numbers\n",
    "possibly_empty_files = []  # For 'possibly_empty' files\n",
    "\n",
    "# Regular expression pattern to match filenames with strip numbers\n",
    "pattern = re.compile(r'^area_\\d+_of_strip_(\\d+)\\.npy$')\n",
    "\n",
    "# Process filenames\n",
    "for filename in all_filenames:\n",
    "    if 'possibly_empty' in filename:\n",
    "        # This is a 'possibly_empty' file\n",
    "        possibly_empty_files.append(filename)\n",
    "    else:\n",
    "        # Try to match the pattern to extract strip number\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            strip_number = int(match.group(1))\n",
    "            # Add filename to the list for this strip number\n",
    "            strip_to_files.setdefault(strip_number, []).append(filename)\n",
    "        else:\n",
    "            print(f\"Filename does not match expected pattern: {filename}\")\n",
    "\n",
    "# List of all unique strip numbers\n",
    "strip_numbers = list(strip_to_files.keys())\n",
    "\n",
    "# Shuffle strip numbers for random splitting\n",
    "random.seed(RANDOM_SEED)  # Ensure reproducibility\n",
    "random.shuffle(strip_numbers)\n",
    "\n",
    "# Calculate split index for strips\n",
    "num_strips = len(strip_numbers)\n",
    "split_index = int(num_strips * config['train_percentage'])\n",
    "\n",
    "# Split strip numbers into train and test sets\n",
    "train_strip_numbers = strip_numbers[:split_index]\n",
    "val_strip_numbers = strip_numbers[split_index:]\n",
    "\n",
    "wandb.config.update({\n",
    "    'num_strips': num_strips,\n",
    "    'train_strip_numbers': train_strip_numbers,\n",
    "    'val_strip_numbers': val_strip_numbers,\n",
    "})\n",
    "\n",
    "# Collect filenames for train and test sets based on strip numbers\n",
    "train_files = []\n",
    "for strip_num in train_strip_numbers:\n",
    "    train_files.extend(strip_to_files[strip_num])\n",
    "\n",
    "val_files = []\n",
    "for strip_num in val_strip_numbers:\n",
    "    val_files.extend(strip_to_files[strip_num])\n",
    "\n",
    "# Now handle the 'possibly_empty' files\n",
    "# Shuffle the possibly_empty files\n",
    "random.shuffle(possibly_empty_files)\n",
    "\n",
    "# Calculate split index for possibly_empty files\n",
    "num_possibly_empty = len(possibly_empty_files)\n",
    "split_index_empty = int(num_possibly_empty * config['train_percentage'])\n",
    "\n",
    "# Split possibly_empty files into train and test sets\n",
    "train_possibly_empty_files = possibly_empty_files[:split_index_empty]\n",
    "val_possibly_empty_files = possibly_empty_files[split_index_empty:]\n",
    "\n",
    "# Add the possibly_empty files to the train and test file lists\n",
    "train_files.extend(train_possibly_empty_files)\n",
    "val_files.extend(val_possibly_empty_files)\n",
    "\n",
    "# Output some information\n",
    "print(f\"Total files: {len(all_filenames)}\")\n",
    "print(f\"Total strips: {len(strip_numbers)}\")\n",
    "print(f\"Training files: {len(train_files)}\")\n",
    "print(f\"Testing files: {len(val_files)}\")\n",
    "\n",
    "# Define your transform if you have one; otherwise, set to None\n",
    "segmentation_transform = None  # Replace with your actual transform if any\n",
    "\n",
    "# Create train dataset\n",
    "train_dataset = LandingStripDataset(\n",
    "    images_dir=images_dir,\n",
    "    labels_dir=labels_dir,\n",
    "    file_list=train_files,\n",
    "    transform=segmentation_transform\n",
    ")\n",
    "\n",
    "# Create test dataset\n",
    "val_dataset = LandingStripDataset(\n",
    "    images_dir=images_dir,\n",
    "    labels_dir=labels_dir,\n",
    "    file_list=val_files,\n",
    "    transform=segmentation_transform\n",
    ")\n",
    "\n",
    "if DEBUG:\n",
    "    train_dataset.samples = train_dataset.samples[:10]\n",
    "    val_dataset.samples = val_dataset.samples[:10]\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=config['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. Load the Geo Foundation Model (GFM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yacs in /home/emil/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages (0.1.8)\n",
      "Requirement already satisfied: PyYAML in /home/emil/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages (from yacs) (6.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install yacs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from ../configs/gfm_config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emil/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading pretrained weights...\n",
      "\n",
      "Missing keys when loading pretrained weights:\n",
      "  - head.weight\n",
      "  - head.bias\n",
      "\n",
      "Unexpected keys when loading pretrained weights:\n",
      "  - mask_token\n",
      "\n",
      "Pretrained weights loaded with some missing/unexpected keys.\n"
     ]
    }
   ],
   "source": [
    "from secret_runway_detection.model import get_model\n",
    "\n",
    "if USING_COLAB:\n",
    "    ! cp -r '/content/drive/MyDrive/Secret Runway Detection Challenge/simmim_pretrain' '../simmim_pretrain'    \n",
    "\n",
    "backbone_weights_path = '../simmim_pretrain/gfm.pth'\n",
    "\n",
    "model = get_model(config['model_type'], BACKBONE_CFG_PATH, backbone_weights_path, output_size=config['resolution']).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone: SwinTransformer\n",
      "segmentation_head: SimpleSegmentationHead\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_children():\n",
    "    print(f\"{name}: {module.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7. Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.backbone.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone parameters: 329\n",
      "New parameters: 52\n",
      "Total parameters: 381\n"
     ]
    }
   ],
   "source": [
    "# Separate parameters for different learning rates\n",
    "backbone_params = []\n",
    "new_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'backbone' in name:\n",
    "        backbone_params.append(param)\n",
    "    else:\n",
    "        new_params.append(param)\n",
    "\n",
    "print(f\"Backbone parameters: {len(backbone_params)}\")\n",
    "print(f\"New parameters: {len(new_params)}\")\n",
    "print(f\"Total parameters: {len(list(model.parameters()))}\")\n",
    "\n",
    "# Define optimizer with differential learning rates\n",
    "optimizer = optim.Adam([\n",
    "    {'params': backbone_params, 'lr': config['lr_backbone']},  # Lower learning rate for pretrained layers\n",
    "    {'params': new_params, 'lr': config['lr_head']}        # Higher learning rate for new layers\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone parameters: 87750176\n",
      "New parameters: 10315177\n",
      "Total parameters: 98065353\n"
     ]
    }
   ],
   "source": [
    "backbone_param_count = sum(p.numel() for p in backbone_params)\n",
    "new_param_count = sum(p.numel() for p in new_params)\n",
    "total_param_count = backbone_param_count + new_param_count\n",
    "\n",
    "wandb.config.update({\n",
    "    'backbone_param_count': backbone_param_count,\n",
    "    'new_param_count': new_param_count,\n",
    "    'total_param_count': total_param_count,\n",
    "})\n",
    "\n",
    "print(\n",
    "    f\"Backbone parameters: {backbone_param_count}\\n\"\n",
    "    f\"New parameters: {new_param_count}\\n\"\n",
    "    f\"Total parameters: {total_param_count}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "# Suitable for binary classification with logits\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optionally, define a learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=config['lr_step_size'], gamma=config['lr_gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Run from 2024-11-09 09:31:13.330082'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 'checkpoints' directory within the current directory\n",
    "os.makedirs('../checkpoints', exist_ok=True)\n",
    "\n",
    "# Define the model save path within the 'checkpoints' directory\n",
    "model_save_path = f'../checkpoints/{wandb.run.name}.pth'\n",
    "\n",
    "wandb.run.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy (all zeros): 0.9960\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataLoader named val_dataloader\n",
    "baseline_acc = compute_baseline_accuracy(val_dataloader)\n",
    "print(f\"Baseline Accuracy (all zeros): {baseline_acc:.4f}\")\n",
    "\n",
    "# Optionally, log to wandb\n",
    "wandb.log({'baseline_accuracy': baseline_acc})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 8. Training Loop with wandb Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Loss: 0.6934\n",
      "Best Validation Accuracy: 0.9960 at Threshold: 0.60\n",
      "Validation loss decreased to 0.6934, saving model to ../checkpoints/Run from 2024-11-09 09:31:13.330082.pth\n",
      "Epoch 2 Validation Loss: 0.7138\n",
      "Best Validation Accuracy: 0.9960 at Threshold: 1.00\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Initialize early stopping variables before the training loop\n",
    "best_val_loss = float('inf')\n",
    "# Number of epochs to wait before stopping\n",
    "patience = config['early_stopping_patience']\n",
    "counter = 0   # Counter for early stopping\n",
    "\n",
    "# Before the training loop, watch the model\n",
    "wandb.watch(model, criterion=criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze(1)  # Adjust dimensions if necessary\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "         # Monitor gradients\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Log every 10 batches or last batch\n",
    "        if (i + 1) % 10 == 0 or i == len(train_dataloader):\n",
    "            avg_loss = running_loss / 10\n",
    "            print(f\"[Epoch {epoch + 1}, Batch {i + 1}] Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # Log metrics to wandb\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'batch': i + 1,\n",
    "                'training_loss': avg_loss,\n",
    "                'learning_rate': optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze(1)  # Adjust dimensions if necessary\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Accumulate validation loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch + 1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Compute validation accuracy and best threshold\n",
    "    best_accuracy, best_threshold = compute_validation_accuracy(model, val_dataloader, device)\n",
    "    print(f\"Best Validation Accuracy: {best_accuracy:.4f} at Threshold: {best_threshold:.2f}\")\n",
    "\n",
    "    # Log validation loss and accuracy to wandb\n",
    "    wandb.log({\n",
    "        'epoch': epoch + 1,\n",
    "        'validation_loss': avg_val_loss,\n",
    "        'validation_accuracy': best_accuracy,\n",
    "        'best_threshold': best_threshold\n",
    "    })\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        # Save the model\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Validation loss decreased to {avg_val_loss:.4f}, saving model to {model_save_path}\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"No improvement in validation loss for {counter} epoch(s).\")\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            # Optionally log early stopping to wandb\n",
    "            wandb.log({'early_stopping_epoch': epoch + 1})\n",
    "            break\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log accuracy improvement over baseline\n",
    "wandb.log({'accuracy_improvement': best_accuracy - baseline_acc})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 9. Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Run from 2024-11-09 09:31:13.330082'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.run.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Artifact model>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a W&B Artifact for the model\n",
    "artifact = wandb.Artifact('model', type='model')\n",
    "\n",
    "# Add the saved model file to the artifact\n",
    "artifact.add_file(model_save_path)\n",
    "\n",
    "# Log the artifact to W&B\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c80b122fe81a4b12977ba11301aeb3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>baseline_accuracy</td><td>▁</td></tr><tr><td>best_threshold</td><td>▁█</td></tr><tr><td>epoch</td><td>▁█</td></tr><tr><td>validation_accuracy</td><td>▁▁</td></tr><tr><td>validation_loss</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>baseline_accuracy</td><td>0.99603</td></tr><tr><td>best_threshold</td><td>1</td></tr><tr><td>epoch</td><td>2</td></tr><tr><td>validation_accuracy</td><td>0.99603</td></tr><tr><td>validation_loss</td><td>0.71379</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync ../wandb/offline-run-20241109_093113-5gj0v1kn<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../wandb/offline-run-20241109_093113-5gj0v1kn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USING_COLAB:\n",
    "    from google.colab import runtime\n",
    "    runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
