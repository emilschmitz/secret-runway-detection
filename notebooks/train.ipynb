{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Landing Strip Detection Training Pipeline\n",
    "\n",
    "\n",
    "\n",
    " This notebook implements a training pipeline for detecting landing strips using satellite imagery. The pipeline includes:\n",
    "\n",
    "\n",
    "\n",
    " - Loading input landing strip data.\n",
    "\n",
    " - Creating input areas around the landing strips.\n",
    "\n",
    " - Downloading Sentinel-2 imagery from Google Earth Engine.\n",
    "\n",
    " - Preparing a dataset for training.\n",
    "\n",
    " - Loading the Geo Foundation Model (GFM) for transfer learning.\n",
    "\n",
    " - Setting up a training loop with Weights & Biases (wandb) logging.\n",
    "\n",
    "\n",
    "\n",
    " **Note**: Ensure that you have authenticated with Google Earth Engine (GEE) using `ee.Authenticate()` and have initialized it with `ee.Initialize()`. Also, make sure `train_utils.py` is in your working directory or Python path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *TODO*\n",
    "* Max value of model outputs can be rather small (in one case, 0.6244). This leads to binary search setting threshold lower, predicting all zeroes\n",
    "* (buffered_labels.float() == 1).float()\n",
    "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
    "(buffered_labels.float() == 1).float().mean()\n",
    "tensor(0.2678) **(!!!)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using Colab\n",
      "/home/emil/Desktop/secret-runway-detection/notebooks\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import timm  # PyTorch Image Models library\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Function to check if running in Colab\n",
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        print(\"Using Colab\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"Not using Colab\")\n",
    "        return False\n",
    "\n",
    "USING_COLAB = is_colab()\n",
    "\n",
    "print(os.getcwd())\n",
    "if USING_COLAB and not os.path.exists('/content/secret-runway-detection'):\n",
    "    print(\"Cloning the secret-runway-detection repository...\")\n",
    "    !git clone https://github.com/emilschmitz/secret-runway-detection.git /content/secret-runway-detection\n",
    "\n",
    "if USING_COLAB:\n",
    "    # For running on colab\n",
    "    os.chdir('/content/secret-runway-detection/notebooks')\n",
    "    ! pip install yacs\n",
    "    from google.colab import drive, userdata\n",
    "    drive.mount('/content/drive')\n",
    "    wandb.login(key=userdata.get('WANDB_API_KEY'))\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path.append(os.path.abspath('../GFM'))\n",
    "\n",
    "# Import functions and constants from train_utils\n",
    "from secret_runway_detection.model import (\n",
    "    get_model,\n",
    ")\n",
    "from secret_runway_detection.dataset import LandingStripDataset, SegmentationTransform\n",
    "from secret_runway_detection.train_utils import (\n",
    "    RANDOM_SEED\n",
    ")\n",
    "\n",
    "from GFM.models import build_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Configuration and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Debug flag: Set to True to run on CPU, False to use GPU if available\n",
    "# With DEBUG == True, test and train sets are reduced to 10 samples each\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cpu') if DEBUG else torch.device(\n",
    "    'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# logging.getLogger('secret_runway_detection.train_utils').setLevel(logging.DEBUG)\n",
    "logging.getLogger('secret_runway_detection.train_utils').setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mesedx12\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../wandb/run-20241108_185443-q37vwrii</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/esedx12/secret-runway-detection/runs/q37vwrii' target=\"_blank\">playful-eon-39</a></strong> to <a href='https://wandb.ai/esedx12/secret-runway-detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/esedx12/secret-runway-detection' target=\"_blank\">https://wandb.ai/esedx12/secret-runway-detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/esedx12/secret-runway-detection/runs/q37vwrii' target=\"_blank\">https://wandb.ai/esedx12/secret-runway-detection/runs/q37vwrii</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    'training_dataset': 'point',\n",
    "    'resolution': 192,\n",
    "    'train_percentage': 0.8,\n",
    "    'model_type': 'simple',\n",
    "    'num_epochs': 50 if not DEBUG else 2,\n",
    "    'batch_size': 32 if USING_COLAB else 4,\n",
    "    'lr_head': 0.001,\n",
    "    'lr_backbone': 0.000001,\n",
    "    'lr_step_size': 10,\n",
    "    'lr_gamma': 0.3,\n",
    "    'early_stopping_patience': 3,\n",
    "}\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project='secret-runway-detection',\n",
    "           mode='online' if not DEBUG else 'dryrun',\n",
    "           dir='..',\n",
    "           tags=[\n",
    "                config['training_dataset'],\n",
    "                'colab' if USING_COLAB else 'local',\n",
    "                'PRETRAINED',\n",
    "           ],\n",
    "           job_type='train',\n",
    "           config=config,\n",
    "           )\n",
    "\n",
    "if not wandb.run.name:\n",
    "    wandb.run.name = f\"Run from {pd.Timestamp.now()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Load Data into Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_train_dir = Path(f'../training_data_{config[\"resolution\"]}')\n",
    "child_train_dir = Path(f'training_data_{config[\"training_dataset\"]}')\n",
    "train_dir = root_train_dir / child_train_dir\n",
    "\n",
    "if USING_COLAB:\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    print(f\"Created directories: {train_dir} (including parents if they didn't exist)\")\n",
    "\n",
    "    import zipfile\n",
    "\n",
    "    # Define the path to the training data zip on Google Drive\n",
    "    drive_train_zip = Path(f'/content/drive/MyDrive/Secret Runway Detection Challenge/training_data_{config[\"resolution\"]}/training_data_{config[\"training_dataset\"]}.zip')\n",
    "\n",
    "    # Define the destination path where the zip will be copied\n",
    "    dest_train_zip = Path(f'{train_dir}.zip')\n",
    "\n",
    "    # Copy the zip file from Drive to the destination\n",
    "    print(f\"Copying training data from {drive_train_zip} to {dest_train_zip}...\")\n",
    "    !cp \"{drive_train_zip}\" \"{dest_train_zip}\"\n",
    "    print(os.listdir(train_dir))\n",
    "\n",
    "    # Unzip the training data\n",
    "    with zipfile.ZipFile(dest_train_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(train_dir)\n",
    "    \n",
    "    print(\"Training data setup completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 1637\n",
      "Total strips: 113\n",
      "Training files: 1305\n",
      "Testing files: 332\n"
     ]
    }
   ],
   "source": [
    "\n",
    "images_dir = train_dir / 'images'\n",
    "labels_dir = train_dir / 'labels'\n",
    "\n",
    "# Get all filenames in the images directory\n",
    "all_filenames = os.listdir(images_dir)\n",
    "\n",
    "# Initialize dictionaries and lists\n",
    "strip_to_files = {}        # For files with strip numbers\n",
    "possibly_empty_files = []  # For 'possibly_empty' files\n",
    "\n",
    "# Regular expression pattern to match filenames with strip numbers\n",
    "pattern = re.compile(r'^area_\\d+_of_strip_(\\d+)\\.npy$')\n",
    "\n",
    "# Process filenames\n",
    "for filename in all_filenames:\n",
    "    if 'possibly_empty' in filename:\n",
    "        # This is a 'possibly_empty' file\n",
    "        possibly_empty_files.append(filename)\n",
    "    else:\n",
    "        # Try to match the pattern to extract strip number\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            strip_number = int(match.group(1))\n",
    "            # Add filename to the list for this strip number\n",
    "            strip_to_files.setdefault(strip_number, []).append(filename)\n",
    "        else:\n",
    "            print(f\"Filename does not match expected pattern: {filename}\")\n",
    "\n",
    "# List of all unique strip numbers\n",
    "strip_numbers = list(strip_to_files.keys())\n",
    "\n",
    "# Shuffle strip numbers for random splitting\n",
    "random.seed(RANDOM_SEED)  # Ensure reproducibility\n",
    "random.shuffle(strip_numbers)\n",
    "\n",
    "# Calculate split index for strips\n",
    "num_strips = len(strip_numbers)\n",
    "split_index = int(num_strips * config['train_percentage'])\n",
    "\n",
    "# Split strip numbers into train and test sets\n",
    "train_strip_numbers = strip_numbers[:split_index]\n",
    "val_strip_numbers = strip_numbers[split_index:]\n",
    "\n",
    "wandb.config.update({\n",
    "    'num_strips': num_strips,\n",
    "    'train_strip_numbers': train_strip_numbers,\n",
    "    'val_strip_numbers': val_strip_numbers,\n",
    "})\n",
    "\n",
    "# Collect filenames for train and test sets based on strip numbers\n",
    "train_files = []\n",
    "for strip_num in train_strip_numbers:\n",
    "    train_files.extend(strip_to_files[strip_num])\n",
    "\n",
    "val_files = []\n",
    "for strip_num in val_strip_numbers:\n",
    "    val_files.extend(strip_to_files[strip_num])\n",
    "\n",
    "# Now handle the 'possibly_empty' files\n",
    "# Shuffle the possibly_empty files\n",
    "random.shuffle(possibly_empty_files)\n",
    "\n",
    "# Calculate split index for possibly_empty files\n",
    "num_possibly_empty = len(possibly_empty_files)\n",
    "split_index_empty = int(num_possibly_empty * config['train_percentage'])\n",
    "\n",
    "# Split possibly_empty files into train and test sets\n",
    "train_possibly_empty_files = possibly_empty_files[:split_index_empty]\n",
    "val_possibly_empty_files = possibly_empty_files[split_index_empty:]\n",
    "\n",
    "# Add the possibly_empty files to the train and test file lists\n",
    "train_files.extend(train_possibly_empty_files)\n",
    "val_files.extend(val_possibly_empty_files)\n",
    "\n",
    "# Output some information\n",
    "print(f\"Total files: {len(all_filenames)}\")\n",
    "print(f\"Total strips: {len(strip_numbers)}\")\n",
    "print(f\"Training files: {len(train_files)}\")\n",
    "print(f\"Testing files: {len(val_files)}\")\n",
    "\n",
    "# Define your transform if you have one; otherwise, set to None\n",
    "segmentation_transform = None  # Replace with your actual transform if any\n",
    "\n",
    "# Create train dataset\n",
    "train_dataset = LandingStripDataset(\n",
    "    images_dir=images_dir,\n",
    "    labels_dir=labels_dir,\n",
    "    file_list=train_files,\n",
    "    transform=segmentation_transform\n",
    ")\n",
    "\n",
    "# Create test dataset\n",
    "val_dataset = LandingStripDataset(\n",
    "    images_dir=images_dir,\n",
    "    labels_dir=labels_dir,\n",
    "    file_list=val_files,\n",
    "    transform=segmentation_transform\n",
    ")\n",
    "\n",
    "if DEBUG:\n",
    "    train_dataset.samples = train_dataset.samples[:10]\n",
    "    val_dataset.samples = val_dataset.samples[:10]\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=config['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. Load the Geo Foundation Model (GFM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yacs in /home/emil/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages (0.1.8)\n",
      "Requirement already satisfied: PyYAML in /home/emil/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages (from yacs) (6.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install yacs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from ../configs/gfm_config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emil/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading pretrained weights...\n",
      "\n",
      "Missing keys when loading pretrained weights:\n",
      "  - head.weight\n",
      "  - head.bias\n",
      "\n",
      "Unexpected keys when loading pretrained weights:\n",
      "  - mask_token\n",
      "\n",
      "Pretrained weights loaded with some missing/unexpected keys.\n"
     ]
    }
   ],
   "source": [
    "from secret_runway_detection.model import get_model\n",
    "\n",
    "if USING_COLAB:\n",
    "    # ! ls '/content/drive/MyDrive/Secret Runway Detection Challenge'\n",
    "    ! cp -r '/content/drive/MyDrive/Secret Runway Detection Challenge/simmim_pretrain' '../simmim_pretrain'    \n",
    "\n",
    "backbone_weights_path = '../simmim_pretrain/gfm.pth'\n",
    "backbone_cfg_path = '../configs/gfm_config.yaml'\n",
    "\n",
    "model = get_model(config['model_type'], backbone_cfg_path, backbone_weights_path, output_size=config['resolution']).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone: SwinTransformer\n",
      "segmentation_head: SimpleSegmentationHead\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_children():\n",
    "    print(f\"{name}: {module.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7. Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.backbone.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone parameters: 329\n",
      "New parameters: 20\n",
      "Total parameters: 349\n"
     ]
    }
   ],
   "source": [
    "# Separate parameters for different learning rates\n",
    "backbone_params = []\n",
    "new_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'backbone' in name:\n",
    "        backbone_params.append(param)\n",
    "    else:\n",
    "        new_params.append(param)\n",
    "\n",
    "print(f\"Backbone parameters: {len(backbone_params)}\")\n",
    "print(f\"New parameters: {len(new_params)}\")\n",
    "print(f\"Total parameters: {len(list(model.parameters()))}\")\n",
    "\n",
    "# Define optimizer with differential learning rates\n",
    "optimizer = optim.Adam([\n",
    "    {'params': backbone_params, 'lr': config['lr_backbone']},  # Lower learning rate for pretrained layers\n",
    "    {'params': new_params, 'lr': config['lr_head']}        # Higher learning rate for new layers\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9922513"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone_param_count = sum(p.numel() for p in backbone_params)\n",
    "new_param_count = sum(p.numel() for p in new_params)\n",
    "total_param_count = backbone_param_count + new_param_count\n",
    "total_param_count\n",
    "new_param_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "# Suitable for binary classification with logits\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optionally, define a learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=config['lr_step_size'], gamma=config['lr_gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'playful-eon-39'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 'checkpoints' directory within the current directory\n",
    "os.makedirs('../checkpoints', exist_ok=True)\n",
    "\n",
    "# Define the model save path within the 'checkpoints' directory\n",
    "model_save_path = f'../checkpoints/{wandb.run.name}.pth'\n",
    "\n",
    "wandb.run.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Ensure numpy is imported\n",
    "\n",
    "def compute_validation_accuracy(model, val_dataloader, device):\n",
    "    \"\"\"\n",
    "    Computes the pixel-wise accuracy over the validation set for multiple thresholds.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model.\n",
    "        val_dataloader: DataLoader for the validation set.\n",
    "        device: The device (CPU or GPU) to perform computations on.\n",
    "\n",
    "    Returns:\n",
    "        best_accuracy (float): The highest accuracy achieved across thresholds.\n",
    "        best_threshold (float): The threshold corresponding to the best accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_pixels = 0\n",
    "    best_accuracy = 0.0\n",
    "    best_threshold = 0.0\n",
    "    thresholds = np.linspace(0.0, 1.0, 11)  # Thresholds from 0.0 to 1.0, inclusive\n",
    "    threshold_correct = {threshold: 0 for threshold in thresholds}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze(1)  # Adjust dimensions if necessary\n",
    "\n",
    "            # Apply sigmoid to get probabilities\n",
    "            probs = torch.sigmoid(outputs)\n",
    "\n",
    "            # Move tensors to CPU and flatten for numpy operations\n",
    "            probs_np = probs.cpu().numpy().flatten()\n",
    "            labels_np = labels.cpu().numpy().flatten()\n",
    "\n",
    "            total_pixels += labels_np.size\n",
    "\n",
    "            # Evaluate at multiple thresholds\n",
    "            for threshold in thresholds:\n",
    "                preds = (probs_np >= threshold).astype(np.uint8)\n",
    "                correct = (preds == labels_np).sum()\n",
    "                threshold_correct[threshold] += correct\n",
    "\n",
    "    # Compute average accuracy for each threshold\n",
    "    for threshold in thresholds:\n",
    "        accuracy = threshold_correct[threshold] / total_pixels\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = threshold\n",
    "\n",
    "    return best_accuracy, best_threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 8. Training Loop with wandb Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch 10] Training Loss: 0.8455\n",
      "[Epoch 1, Batch 20] Training Loss: 0.6845\n",
      "[Epoch 1, Batch 30] Training Loss: 0.6122\n",
      "[Epoch 1, Batch 40] Training Loss: 0.5463\n",
      "[Epoch 1, Batch 50] Training Loss: 0.4717\n",
      "[Epoch 1, Batch 60] Training Loss: 0.4005\n",
      "[Epoch 1, Batch 70] Training Loss: 0.3287\n",
      "[Epoch 1, Batch 80] Training Loss: 0.2651\n",
      "[Epoch 1, Batch 90] Training Loss: 0.2168\n",
      "[Epoch 1, Batch 100] Training Loss: 0.1735\n",
      "[Epoch 1, Batch 110] Training Loss: 0.1401\n",
      "[Epoch 1, Batch 120] Training Loss: 0.1199\n",
      "[Epoch 1, Batch 130] Training Loss: 0.1027\n",
      "[Epoch 1, Batch 140] Training Loss: 0.0817\n",
      "[Epoch 1, Batch 150] Training Loss: 0.0696\n",
      "[Epoch 1, Batch 160] Training Loss: 0.0622\n",
      "[Epoch 1, Batch 170] Training Loss: 0.0577\n",
      "[Epoch 1, Batch 180] Training Loss: 0.0486\n",
      "[Epoch 1, Batch 190] Training Loss: 0.0467\n",
      "[Epoch 1, Batch 200] Training Loss: 0.0479\n",
      "[Epoch 1, Batch 210] Training Loss: 0.0427\n",
      "[Epoch 1, Batch 220] Training Loss: 0.0364\n",
      "[Epoch 1, Batch 230] Training Loss: 0.0337\n",
      "[Epoch 1, Batch 240] Training Loss: 0.0305\n",
      "[Epoch 1, Batch 250] Training Loss: 0.0324\n",
      "[Epoch 1, Batch 260] Training Loss: 0.0288\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Adjust dimensions if necessary\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1603\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1600\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1601\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1603\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1607\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1608\u001b[0m     ):\n\u001b[1;32m   1609\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/secret_runway_detection/model.py:181\u001b[0m, in \u001b[0;36mSimpleSegmentationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# Forward pass through the backbone\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Assuming the backbone returns a single feature map; adjust if multiple\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/GFM/GFM/models/swin_transformer.py:576\u001b[0m, in \u001b[0;36mSwinTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 576\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/GFM/GFM/models/swin_transformer.py:568\u001b[0m, in \u001b[0;36mSwinTransformer.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    565\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_drop(x)\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 568\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)  \u001b[38;5;66;03m# B L C\u001b[39;00m\n\u001b[1;32m    571\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# B C 1\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/GFM/GFM/models/swin_transformer.py:391\u001b[0m, in \u001b[0;36mBasicLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    389\u001b[0m         x \u001b[38;5;241m=\u001b[39m checkpoint\u001b[38;5;241m.\u001b[39mcheckpoint(blk, x)\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/GFM/GFM/models/swin_transformer.py:251\u001b[0m, in \u001b[0;36mSwinTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    248\u001b[0m x_windows \u001b[38;5;241m=\u001b[39m x_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, C)  \u001b[38;5;66;03m# nW*B, window_size*window_size, C\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# W-MSA/SW-MSA\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# nW*B, window_size*window_size, C\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# merge windows\u001b[39;00m\n\u001b[1;32m    254\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m attn_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, C)\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/GFM/GFM/models/swin_transformer.py:140\u001b[0m, in \u001b[0;36mWindowAttention.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    137\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_drop(attn)\n\u001b[1;32m    139\u001b[0m x \u001b[38;5;241m=\u001b[39m (attn \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B_, N, C)\n\u001b[0;32m--> 140\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_drop(x)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize early stopping variables before the training loop\n",
    "best_val_loss = float('inf')\n",
    "# Number of epochs to wait before stopping\n",
    "patience = config['early_stopping_patience']\n",
    "counter = 0   # Counter for early stopping\n",
    "\n",
    "# Before the training loop, watch the model\n",
    "wandb.watch(model, criterion=criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze(1)  # Adjust dimensions if necessary\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "         # Monitor gradients\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Log every 10 batches or last batch\n",
    "        if (i + 1) % 10 == 0 or i == len(train_dataloader):\n",
    "            avg_loss = running_loss / 10\n",
    "            print(f\"[Epoch {epoch + 1}, Batch {i + 1}] Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # Log metrics to wandb\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'batch': i + 1,\n",
    "                'training_loss': avg_loss,\n",
    "                'learning_rate': optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Log gradients\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    wandb.log({f'gradients/{name}': param.grad.abs().mean()})\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze(1)  # Adjust dimensions if necessary\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Accumulate validation loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch + 1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Compute validation accuracy and best threshold\n",
    "    best_accuracy, best_threshold = compute_validation_accuracy(model, val_dataloader, device)\n",
    "    print(f\"Best Validation Accuracy: {best_accuracy:.4f} at Threshold: {best_threshold:.2f}\")\n",
    "\n",
    "    # Log validation loss and accuracy to wandb\n",
    "    wandb.log({\n",
    "        'epoch': epoch + 1,\n",
    "        'validation_loss': avg_val_loss,\n",
    "        'validation_accuracy': best_accuracy,\n",
    "        'best_threshold': best_threshold\n",
    "    })\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        # Save the model\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Validation loss decreased to {avg_val_loss:.4f}, saving model to {model_save_path}\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"No improvement in validation loss for {counter} epoch(s).\")\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            # Optionally log early stopping to wandb\n",
    "            wandb.log({'early_stopping_epoch': epoch + 1})\n",
    "            break\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Find accuracy-optimizing thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 9. Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'playful-eon-39'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.run.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Path is not a file: ../checkpoints/playful-eon-39.pth",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m artifact \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39mArtifact(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Add the saved model file to the artifact\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43martifact\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Log the artifact to W&B\u001b[39;00m\n\u001b[1;32m      8\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog_artifact(artifact)\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/wandb/sdk/artifacts/_validators.py:88\u001b[0m, in \u001b[0;36mensure_not_finalized.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ArtifactFinalizedError(fullname\u001b[38;5;241m=\u001b[39mmethod_fullname, obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/wandb/sdk/artifacts/artifact.py:1215\u001b[0m, in \u001b[0;36mArtifact.add_file\u001b[0;34m(self, local_path, name, is_tmp, skip_cache, policy)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add a local file to the artifact.\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \n\u001b[1;32m   1195\u001b[0m \u001b[38;5;124;03mArguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;124;03m    ValueError: Policy must be \"mutable\" or \"immutable\"\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(local_path):\n\u001b[0;32m-> 1215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath is not a file: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(local_path))\n\u001b[1;32m   1217\u001b[0m name \u001b[38;5;241m=\u001b[39m LogicalPath(name \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(local_path))\n\u001b[1;32m   1218\u001b[0m digest \u001b[38;5;241m=\u001b[39m md5_file_b64(local_path)\n",
      "\u001b[0;31mValueError\u001b[0m: Path is not a file: ../checkpoints/playful-eon-39.pth"
     ]
    }
   ],
   "source": [
    "# Create a W&B Artifact for the model\n",
    "artifact = wandb.Artifact('model', type='model')\n",
    "\n",
    "# Add the saved model file to the artifact\n",
    "artifact.add_file(model_save_path)\n",
    "\n",
    "# Log the artifact to W&B\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3a05e81c294f81be8950aa050a7aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.285 MB of 0.559 MB uploaded\\r'), FloatProgress(value=0.5094777998403308, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>gradients/backbone.head.bias</td><td>▂▁▂█▂▃▃▃▂▂▂▁▃▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▃▁▁▂▁▁</td></tr><tr><td>gradients/backbone.head.weight</td><td>█▂▂▅▁▃▂▂▂▂▂▂▅▂▁▂▄▂▁▃▂▁▂▃▁▂▂▂▁▂▅▁▁▁▂▂▁▁▂▁</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.attn.proj.bias</td><td>█▃▂▁▁▁▁▂▁▁▂▂▂▁▂▁▃▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.attn.proj.weight</td><td>▆█▁▁▁▂▂▃▂▃▄▅▁▄▂▂▂▂▁▁▄▂▃▁▁▂▂▂▂▂▂▁▁▂▃▂▁▁▂▃</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.attn.qkv.bias</td><td>█▁▁▂▁▁▁▁▁▂▂▁▁▂▁▂▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.attn.qkv.weight</td><td>▅▆▂▁▆▂▃▁▁▄▄▂▅▄▂▂█▅▂▂▅▂▂▄▄▂▂▂█▂▁▁▂▂▃▁▂▂▁▇</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.attn.relative_position_bias_table</td><td>▁▂▃▂▁▂▂▄▄▄█▄▄▂▂▂▄▅▂▁▂▂▂▃▃▂▂▂▁▂▄▁▂▂▂▂▂▂▁▂</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.mlp.fc1.bias</td><td>▆▁▁▂▂▂▃▃▃▃▃▃▂▃▄▄▃▂▁▂▂▂▄▃▅▂▄▂▃▂▂▆▂▁█▄▃▂▃▂</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.mlp.fc1.weight</td><td>▇▆▅▃▁▁▂▃▂▃▃▄▆▃▄▄▂█▃▂▃▄▂▃▄▃▅▂▃▁▇▂▃▃▂▁▁▂▂▃</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.mlp.fc2.bias</td><td>█▇▅▄▂▂▂▃▄▂▆▄▃▂▃▄▅▂▂▇▂▂▁▁▁▄▂▂▂▂▁▃▃▃▅▂▁▂▂▃</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.mlp.fc2.weight</td><td>█▃▃▄▃▁▂▂▂▃▄▆▄▂▁▂▂▃▃▁▂▁▂▁▁▂▃▂▃▁▁▁▁▁▃▁▁▂▁▂</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.norm1.bias</td><td>▅▅▄▂▁▁▂▄▂▂▃▂▂▄▂▂▅▁▂▁▄▆▂▂▁█▄▁▂▁▁▁▁▁▁▁▂▂▁▁</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.norm1.weight</td><td>█▃▁▃▁▂▂▂▂▂▂▁▂▂▂▂▁▄▁▂▂▁▂▁▁▁▃▁▂▁▁▂▁▁▁▁▂▁▁▁</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.norm2.bias</td><td>█▁▁▁▃▄▃▆▅▂▅▁▆▅▂▇▂▁▃▂▂▄▃▁▅▁▁▁▃▂▁▄▁▃▂▁▂▁▂▅</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.norm2.weight</td><td>█▃▁▁▁▁▂▂▂▂▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▂▂▁▁▂▁▁</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.attn.proj.bias</td><td>█▆▅▃▂▂▃▃▄▃▃▃▃▃▂▃▂▄▂▂▅▅▂▁▂▁▁▂▁▄▂▂▁▁▂▂▁▁▁▂</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.attn.proj.weight</td><td>▆▇▃▁▂▁▂▃▃▅▃▄▂█▄▄▅▂▅▁▁▂▂▂▇▂▃▂▂▂▁▁▅▁▁▂▁▂▁▁</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.attn.qkv.bias</td><td>▇▁▁▁▂▂▄▃▂▇▃▂▂▃▅▆█▁▂▄▁▁█▂▂▁▄▂▁▁▁▂▃▂▂▃▃▂▄▁</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.attn.qkv.weight</td><td>▆▅▄▄▁▄▃▂▃▃▃▄▂▃▆▅▅▂▆▂▁▂▂█▂▂▂▆▁▃▂▁▁▂▁▂▃▁▁▁</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.attn.relative_position_bias_table</td><td>▇▅█▂▂▂▂▅▄▆▇▃▄▃▆▃▂▆▂▂▁▂▃▂▁▅▄▃▃▁▂▁▂▃▂▁▁▂▂▂</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.mlp.fc1.bias</td><td>▄▃▂▁▂▂▃▂▃▂█▅▄▂▂▃▁▂▁▂▂▂▁▅▂▂▂▂▂▂▁▁▂▃▂▁▂▁▁▁</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.mlp.fc1.weight</td><td>██▁▃▁▂▄▄▃▄▂▁▄▄▂▂▂▂▃▂▂▃▂▂▆▃▂▁▃▃▁▂▂▁▂▅▁▃▃▂</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.mlp.fc2.bias</td><td>▅▂▂▁▃▂▃▂▂▂▂▂█▂▁▂▁▁▅▂▅▂▂▁▂▂▁▂▂▂▄▁▂▄▂▁▂▂▆▁</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.mlp.fc2.weight</td><td>▅▁▂▂▃▃▃▃▃▆▂▂▆█▅▁▃▂▂▁▂▃▁▁▂▅▂▁▂▂▁▁▂▁▄▁▂█▁▂</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.norm1.bias</td><td>█▁▃▁▂▂▂▃▂▃▃▂▄▃▁▆▁▂▁▃▁▃▂▁▁▁▁▁▂▂▂▁▂▂▁▃▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.norm1.weight</td><td>▃▃▁▂▁▁▃▃▃▃▂▁▂▁▂▅▁▁▃▅▂▁▄▁▂▁▂▂█▂▂▂▅▂▁▂▁▃▁▂</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.norm2.bias</td><td>▅█▃▃▂▁▁▂▂▁▂▂▂▂▃▁▂▂▄▅▃▁▁▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.norm2.weight</td><td>▅▅▆▄▂▂▃█▃█▃▃█▆▁▂▁▂▄▃▅█▂▃▂▂▁▆▂▁▁▁▁▁▄▃▁▂▃▂</td></tr><tr><td>gradients/backbone.layers.0.downsample.norm.bias</td><td>█▁▁▁▂▃▂▂▂▂▂▁▂▂▁▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.0.downsample.norm.weight</td><td>█▃▂▂▁▁▁▂▁▁▂▂▂▂▁▁▁▁▁▂▁▂▁▁▁▁▃▁▁▁▁▂▂▁▁▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.0.downsample.reduction.weight</td><td>█▁▁▁▂▂▁▂▃▄▄▂▃▂▄▃▅▃▂▃▁▄▂▁▁▂▁▁▂▂▄▁▁▂▁▁▁▅▂▃</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.attn.proj.bias</td><td>▇▁▁▃▂▃▇▆█▃▅▃▃▂▄▂▅▁▂▃▃▂▂▅▃▂▂▁▂▁▁▃▂▂▁▁▂▁▃▃</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.attn.proj.weight</td><td>▅▁▄▁▂▃▄▃█▃▄▆▆▃▅▄▄▇▃▅▂▂▂▂▂▅▂▂▂▁▂▁▃▂▂▁▁▃▁▃</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.attn.qkv.bias</td><td>▃▂▂▃█▃▄▂▃▃▄▃▃▄▇▂▄▃▁▄▆▁▂▁▂▂▂▂▁▃▃▂▂▁▁▂▃▂▅▂</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.attn.qkv.weight</td><td>▅▂▁▁▁▂▂▃▂▃▃█▄▂▂▂▁▆▂▂▂▁▂▁▂▁▃▂▅▁▂▅▁▃▂▂▂▃▁▁</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.attn.relative_position_bias_table</td><td>▁▃▁▁▂▃▃▂▄▅▅▂▄▁▂▄▂▂▂▃▂▂▆▁▂▃▁▅▁▁▃▁▁█▂▂▁▂▂▂</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.mlp.fc1.bias</td><td>▄▄▁▁▂▆▅▂▂▂▂█▅▃▂▂▇▁▂▂▂▃▂▂▂▁▂▁▁▂▄▂▅▂▂▁▁▂▃▂</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.mlp.fc1.weight</td><td>█▃▁▁▁▁▁▂▂▂▆▃▅▂▅▅▃▁▁▇▁▂▆▁▂▂▁▂▂▄▁▁▁▂▃▁▂▂▂▃</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.mlp.fc2.bias</td><td>▁▁▃▂▂▂▂▂▃▂▃▃█▂▄▂▆▂▃▃▁▂▅▁▄▅▂▃▆▁▂▂▁▂▁▂▂▁▆▁</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.mlp.fc2.weight</td><td>▁▁▅▁▃▂▂▂▃▃▃█▇▄▆▃▂▄▄▃▂▄▂▃▂▁▇▂▁▁▂▂▁▅▂▄▂▃▁▂</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.norm1.bias</td><td>▄▁▃▃▁▂▄▄▅▂▂▂▁▂▆▄█▂▂▂█▂▃▁▂▂▄▂▆▇▄▂▂▃▅▁▂▂▁▄</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.norm1.weight</td><td>▆▄▂▃▆▂▃▅▃▆▃▂▂▂▅▂▃▃▃▂█▂▁▁▃▁▂▂▂▁▂▁▃▂▂▃▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.norm2.bias</td><td>▄▁▂▂▁▃▂▄▃▄▃▂▂▁▂▂█▂▁▂▂▂▂▂▁▃▂▂▂▁▂▂▄▂▂▁▂▁▇▄</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.norm2.weight</td><td>▃▁▄▁▂▂▁▁▂▄▃▃▅▂▂▇▃▃▂▂▂▄▇▁▂█▃▃▁▁▁▁▁▁▁▃▃▁▁▃</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.attn.proj.bias</td><td>▅▂▂▃▄█▆▇▅▄█▃█▃▂▅▂▃▂▂▂▂▂▂▄▂▄▂▃▂▂▂▃▂▃▁▁▂▂▃</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.attn.proj.weight</td><td>█▃▁▁▂▁▂▂▃▃▂▂▃▂▁▁▂▁▂▄▁▂▂▄▁▂▂▂▂▁▁▁▂▁▂▁▂▁▁▂</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.attn.qkv.bias</td><td>█▃▁▁▁▁▂▂▁▄▂▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.attn.qkv.weight</td><td>▇▃▁█▆▃▃▄▆▄▃▅▃▃▇▂▇▂▂▃▄▂▅▂▃▅▂▁▁▂▂▄▁▃▁▂▁▄▃▃</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.attn.relative_position_bias_table</td><td>▄▁▁▄▁▄▂▂▂▃▂▃▂▃▃▃█▃▄▄▄▄▆▄▁▃▃▃▄▃▄▃▃▂▁▂▂▁▂▄</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.mlp.fc1.bias</td><td>█▅▁▇▃▂▃▂▃▃▃▃▃▇▁▇▅▃▂▄▃▇▄▃▂▁▃▁▃▃▄▂▂▁▃▂▃▁▂▃</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.mlp.fc1.weight</td><td>█▂▂▁▃▁▁▃▂▂▂▁▂▂▂▃▂▄▁▁▂▂▁▁▂▁▁▁▂▂▂▂▁▁▁▁▂▁▁▂</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.mlp.fc2.bias</td><td>▆▄▃▂▁▃▄▄▄▂▆▃▃▅▃▃▄▂▂█▄▃▆▂▃▁▁▁▃▂▅▁▂▁▆▂▁▂▂▁</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.mlp.fc2.weight</td><td>▅▂▂▂▁▁▂▃▂▇▃▄▂█▅▁▂▁▃▇▁▂▁▃▂▁▂▃▂▂▆▂▂▁▁▃▁▃▃▄</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.norm1.bias</td><td>▄▁▁▃▁▃▃▄▂▅█▄▂▂▂▃▆▂▂▂▂▅▃▁▂▂▂▁▄▂▂▂▂▂▁▂▁▂▂▃</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.norm1.weight</td><td>█▃▂▁▂▂▁▂▁▁▂▂▂▃▂▁▂▂▁▁▂▂▂▁▁▂▁▂▂▂▂▁▁▁▂▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.norm2.bias</td><td>█▅▄▁▁▂▃▄▃▂▇▆▂▁▂▅▂▄▆▄▃▃▁▁▂▃▁▃▂▅▁▂▂▂▃▂▁▁▂▂</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.norm2.weight</td><td>█▂▂▁▁▂▁▁▁▂▁▂▂▂▂▃▂▂▂▁▁▃▂▁▁▁▁▂▁▁▁▂▁▁▁▁▁▂▂▁</td></tr><tr><td>gradients/backbone.layers.1.downsample.norm.bias</td><td>▅▄▂▆▁▁▂▂▂▂▂▂█▃▄▂▆▂▂▄▄▁▇▅▁▁▄▂▁▁▂▂▃▂▁▁▃▁▁▁</td></tr><tr><td>gradients/backbone.layers.1.downsample.norm.weight</td><td>█▁▁▁▂▂▁▁▂▂▁▁▂▁▁▁▁▂▁▁▁▁▁▂▁▂▁▁▁▁▁▂▁▂▂▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.1.downsample.reduction.weight</td><td>█▁▁▂▂▃▂▁▁▂▃▅▅▇▁▃▄▂▂▂▂▃▂▂▂▂▂▃▄▆▁▂▂▂▁▂▃▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.attn.proj.bias</td><td>█▂▂▁▂▁▁▁▂▂▂▁▁▃▁▃▂▁▁▁▂▁▁▁▂▁▂▁▂▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.attn.proj.weight</td><td>▁▂▁▂▁▁▂▂▂▂▂▂▂█▃▂▃▃▃▁▂▁▂▂▁▁▂▃▁▁▂▂▃▂▁▁▁▁▅▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.attn.qkv.bias</td><td>▅▁▁▁▂▃▂▂▂▃▄▆▃█▃▂█▃▂▂▃▅▂▄▂▃▆▃▂▂▃▃▅▂▁▁▁▇▅▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.attn.qkv.weight</td><td>█▂▁▂▁▁▂▂▂▂▄▂▃▁▃▂▁▂▁▁▃▂▂▁▁▁▂▁▂▂▁▁▁▁▂▁▁▁▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.attn.relative_position_bias_table</td><td>█▁▁▃▂▁▂▂▃▂▅▆▅▂▅▃▃▃▃▂▄▂▃▂▂▃▃▂▂▂▃▁▂▂▂▁▁▂▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.mlp.fc1.bias</td><td>▁▁▆▄▃▃▃█▃▅▂▇▃▂▄▂█▃▃▃▂▂▅▂▃▂▃▃▅▂▁▂▁▂▁▂▁▁▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.mlp.fc1.weight</td><td>█▃▃▄▃▁▁▂▁▁▂▂▂▂▃▂▅▄▂▁▃▂▁▁▁▂▁▁▂▂▄▂▁▁▁▃▂▁▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.mlp.fc2.bias</td><td>▃█▂▁▁▂▄▃▂▃▂▆▂▁▂▃▂▁▂▂▁▂▁▁▁▂▂▂▁▁▁▁▂▂▁▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.mlp.fc2.weight</td><td>▄▆▆▁▇▁▁▂▁▁▄▃▃▃▇█▂▃▃▆▄▅▃▂▄▁▂▂▂▄▂▂▇▁▂▁▁▃▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.norm1.bias</td><td>█▃▁▁▆▁▂▁▂▄▄█▂▃▅▆▂▄▂▂▂▁▂▄▆▃▅▃▂▃▂▄▁▂▂▆▃▂▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.norm1.weight</td><td>█▄▁▃▁▁▃▄▃▄▅█▇▆▃▃▂▂▂▂▂▃▂▂▅▃▅▃▁▂▁▁▃▂▂▁▁▃▅▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.norm2.bias</td><td>█▃▁▂▂▂▁▂▂▃▃▂▄▂▄▂▂▁▁▂▁▁▁▂▂▂▁▁▂▁▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.norm2.weight</td><td>▂▃▂▄▁▁▂▂█▂▇▆▅▆▄▅▂▃▁▁▁▂▁▂▁▂▂▁▂▁▁▁▂▂▁▂▁▁▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.attn.proj.bias</td><td>█▃▂▂▂▁▁▁▂▂▁▁▁▅▃▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.attn.proj.weight</td><td>█▁▂▂▁▂▃▄▃▃▂▂▂▅▂▁▂▂█▃▂▃▂▂▂▁▁▁▁▅▂▁▂▂▂▂▃▂▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.attn.qkv.bias</td><td>█▁▄▁▂▃▃▄▃▂▆▂▄▄▇▂▅▃▂▂▂▃▅▃▁▂▂▁▁▄▁▁▂▂▂▂▂▁▇▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.attn.qkv.weight</td><td>▅█▁▁▁▄▂▃▁▂▃▆▄▄▂▅▅▂▂▅▃▂▂▃▂▅▄▂▂▂▁▃▂▁▂▂▁▃▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.attn.relative_position_bias_table</td><td>▄▁▄▁▂▄▃▄▇▅▆█▂▅▂▂▂▁▄▅▄▂▄▂▂▃▅▁▂▃▂▃▄▁▂▃▃▂▂▄</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.mlp.fc1.bias</td><td>▄▃▁▁▃▂▂█▄▂▃▅▃▄▂▂▁▃▃▃▂▂▄▂▃▂▃▂▁▆▂▁▂▂▁▂▅▂▃▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.mlp.fc1.weight</td><td>█▄▂▁▁▂▂▁▂▂▂▂▂▁▂▂▁▁▂▁▁▁▁▂▁▁▂▂▁▂▃▁▁▁▂▁▃▂▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.mlp.fc2.bias</td><td>█▂▃▂▁▁▂▂▁▂▁▂▁▂▂▁▃▂▁▁▁▂▁▁▁▁▃▁▁▁▁▁▁▁▁▃▁▂▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.mlp.fc2.weight</td><td>▂▄▂▂▂▂▂▂▃▂█▁▄▄▂▃▄▂▁▂▁▂▁▃▂▁▄▁▁▁▁▂▂▂▂▂▁▂▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.norm1.bias</td><td>█▂▁▁▁▂▁▂▂▂▂▆▃▁▄▂▃▂▃▂▂▂▂▁▁▁▁▁▃▂▁▁▁▁▁▂▁▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.norm1.weight</td><td>▄▂▄▂▂▄▅▃▃█▅▂▅▅▃▂▄▆▅▃▂▁▅▂▁▁▁▂▁▂▂▂▂▃▄▁▁▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.norm2.bias</td><td>█▃▃▁▁▁▁▁▂▂▂▁▁▁▁▁▃▂█▂▁▁▁▂▁▂▁▂▂▁▁▁▁▂▁▁▂▄▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.norm2.weight</td><td>▄▁▃▅▃▂▁▂▂▃▄▃▇▂█▂▅▃█▃▅▂▄▃▂▂▂▂▂▃▂▆▁▆▁▂▂▁▃▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.attn.proj.bias</td><td>█▅▅▄▁▁▁▂▃▂▃▄▄▄▄▄█▃▃▇▂▂▃▅▄▂▁▂▂▁▂▁▂▁▄▂▁▁▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.attn.proj.weight</td><td>▃▅▃▁▃▂▃▃▃▃▂▃▃▃▃█▄▂▆▂▃▂▁▂▂▂▂▂▁▃▁▆▁▂▂▂▂▃▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.attn.qkv.bias</td><td>▅█▃▃▂▂▂▂▂▂▃▃▂▁▁▁▁▂▃▃▂▁▃▂▂▁▁▁▂▂▁▂▁▁▁▂▂▂▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.attn.qkv.weight</td><td>▃█▂▁▁▁▂▂▂▂▃▄▂▂▁▃▁▂▁▁▁▁▂▂▁▂▁▂▁▂▁▂▁▁▃▁▂▂▃▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.attn.relative_position_bias_table</td><td>▃▂▅▁▇▂▂▂▄▅█▆▅▆▃▃▃▂▄▂▂▄▁▃▂▂▂▁▂▁▁▄▂▁▂▁▁▃▆▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.mlp.fc1.bias</td><td>▆▇▁▄▂▂▁▂▄▅▅▄▄▄▂▁▄▃▇▅▄▂▃▃▄▅▄▂▁▃▃▂▂▁▂▃█▅▁▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.mlp.fc1.weight</td><td>▅█▄▃▁▁▂▂▂▂▂▂▂▅▁▂▃▃▁▂▂▁▁▂▁▂▂▁▂▁▁▂▁▂▃▁▁▂▂▄</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.mlp.fc2.bias</td><td>█▄▆▃▁▁▂▁▁▂▂▃▂▅▄▁▃▅▂▃▃▁▁▄▂▃▂▁▂▂▄▂▁▁▁▂▁▂▅▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.mlp.fc2.weight</td><td>█▃▁▂▁▁▁▁▁▂▂▂▂▁▂▂▂▁▂▁▁▂▂▃▂▂▂▁▂▁▁▁▁▁▁▁▂▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.norm1.bias</td><td>▂▁▄▁▂▃█▄▅▂▅▆▄▆▅▂▆▂▂▁▇▇▂▇▃▃▂▃▃▆▄▃▃▁▂▁▁▆▁▄</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.norm1.weight</td><td>▄█▄▁▁▃▃▂▄▄▁▅▄▃▅▂▆▄▄▂▂▃▂▃▂▁▂▁▁▁▄▂▁▁▂▆▂▁▃▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.norm2.bias</td><td>█▃▂▁▂▁▁▂▂▂▂▂▂▂▂▁▂▂▂▁▃▂▁▂▁▂▁▁▁▁▁▃▁▁▁▁▁▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.norm2.weight</td><td>▃▃▆▂▂▂▂▂▁▂█▃▂█▅▂▄▂▂▃▂▁▃▂▃▂▁▂▁▁▁▁▁▂▂▂▁▃▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.attn.proj.bias</td><td>▇▂▁▁▄▄▄▆▂▃▃▇▄▆▃▅▂▄▆▆▂▃▂▂▂▅▇▂▁▂▄▂▂█▃▂▂▂▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.attn.proj.weight</td><td>▇▃▁▁▁▂▃▃▄▃█▆▂▅▃▂▃▂█▃▃▂▃▂▅▁▂▃▁▁▄▂▂▁▁▂▂▁▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.attn.qkv.bias</td><td>▁▄▂▁▁▂▂▂▂█▃▅▃▃▄▁▂▂▂▂▁▄▂▂▂▂▃▁▁▁▁▁▂▃▂▁▁▂▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.attn.qkv.weight</td><td>▅▃▁▂▂█▂▃▃▇█▃▂▂▄▃▄▃▁▃▂▃▂▃▁▂▂▃▃▁▂▃▁▁▂▁▂▁▃▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.attn.relative_position_bias_table</td><td>█▂▃▁▁▁▁▁▁▁▁▁▁▂▁▃▁▂▂▃▁▁▂▂▁▁▁▂▁▁▁▁▁▁▂▁▁▂▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.mlp.fc1.bias</td><td>▄▂▂▁▁▂▂▃▂▂▂█▂▃▂▃▂▁▂▂▁▂▂▂▂▂▂▃▂▁▁▃▁▁▁▂▂▁▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.mlp.fc1.weight</td><td>█▂▃▁▁▁▁▁▂▂▂▂▂▂▁▂▂▁▃▂▁▂▁▁▂▂▂▂▁▁▁▁▂▁▂▁▂▂▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.mlp.fc2.bias</td><td>▁▂▂▃▂█▄▆█▃▂▂▄▂▂▂▂▆▁▂▂▂▃▁▁▁▁▁▅▁▂▂▄▁▂▁▂▃▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.mlp.fc2.weight</td><td>█▁▄▁▁▂▁▂▂▃▃▄▂▄▄▂▃▄▁▃▂▄▃▂▂▃▃▂▂▂▂▁▂▁▂▂▁▂▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.norm1.bias</td><td>▅▂▅▃▄▄▃▆▃▄▂█▇▂▂▃▃▂▃▄▃▇▂▁▂▂▁▅▂▂▁▃▂▁▂▃▂▂▃▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.norm1.weight</td><td>█▄▂▂▁▂▂▂▃▂▂▂▁▂▂▃▁▁▁▂▂▃▁▂▁▁▁▂▂▁▁▁▂▁▁▁▁▁▁▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.norm2.bias</td><td>▄▇▁▂▂▄▃▂█▆▁█▆▇▇▂▅▃▂▁▁▆▃▄▅▃▂▂▁▂▁▁▁▂▃▁▂▁▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.norm2.weight</td><td>▅█▁▁▃▂▁▂▃▅▅▇▄▄▄▄▂▂▂▂▃▂▂▃▂▃▁▁▁▁▂▂▂▁▃▃▂▆▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.attn.proj.bias</td><td>▄█▁▂▁▂▂▂▃▂▃▁▂▂▁▂▄▃▂▁▂▁▃▂▁▃▁▁▁▁▁▁▂▂▂▁▁▁▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.attn.proj.weight</td><td>▆▇▇▁▂▂▃▃▅▄▄▄▅▂█▂▄▇▅▂▂█▂▃▃▃▂▂▃▂▁▃▂▂▃▄▂▁▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.attn.qkv.bias</td><td>▅█▁▃▁▂▁▂▂▂▃▂▆▂▃▂▂▄▂▂▃▂▂▂▃▂▂▁▁▂▂▁▂▁▁▅▂▁▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.attn.qkv.weight</td><td>█▄▃▁▁▁▁▁▂▁▃▂▁▁▃▁▁▂▁▁▂▁▂▁▁▂▁▁▁▁▂▁▁▂▁▁▁▁▁▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.attn.relative_position_bias_table</td><td>▆▁▁▇▂▂▂▃▅▃▃▂▃▄▂█▂▇▃▃▄▅▃▅▂▂▂▅▃▂▂▄▂▁▂▃▇▄▂▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.mlp.fc1.bias</td><td>█▃▃▁▁▁▁▁▁▁▃▁▂▂▁▂▂▁▁▁▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.mlp.fc1.weight</td><td>▅█▂▄▃▁▁▁▁▂▂▆▂▂▂▄▁▂▂▁▂▁▁▂▁▂▂▁▁▁▁▁▁▁▂▂▁▂▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.mlp.fc2.bias</td><td>▆▁▁▁▂▂▃▃▃▃▆█▂▃▂▄▇▄▂▄▃▁▂▂▂▁▁▂▂▂▁▆▂▄▂▃▂▂▂▄</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.mlp.fc2.weight</td><td>█▁▁▃▃▃▃▇▃▄▃▃▆▂▃▂▃▃▂▃▂▄▃▂▂▁▁▁▂▁▁▁▂▆▄▂▂▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.norm1.bias</td><td>▆▆▁▂▂▂▂▃▄▃▂▂▃▆█▄▂▅▄▁▁▂▄▂▂▃▁▁▃▁▁▂▁▁▁▃▂▂▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.norm1.weight</td><td>▆█▃▂▁▁▄▃▁▂▁▂▂▃▇▄▅▂▂▄▂▂▂▂▂▂▂▂▂▄▁▁▁▂▁▂▁▂▁▄</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.norm2.bias</td><td>▁▄▃▂▂▁▃▄▅█▄▅▆▃▆▅▂▆▃▆▂▄▆▄▃▂▇▂▃█▆▂▁▂▁▁▃▁▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.norm2.weight</td><td>▄▁▁▄▃▂▃▁▃▄▇▃▅█▆▄▂▅▂▃▂▃▁▂▄▂▂▃▂▂▃▃▂▁▅▂▁▂▂▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.attn.proj.bias</td><td>█▁▁▁▁▂▂▂▂▂▂▄▂▂▂▂▂▂▁▁▂▂▁▂▂▃▂▂▂▃▂▁▁▃▁▁▂▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.attn.proj.weight</td><td>▁▁▅▂▃▃▂▃█▃▁▅▃▄▄▄▂▃▃▃▁▃▂▂▂▂▃▇▁▂▂▂▁▁▅▂▄▁▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.attn.qkv.bias</td><td>▇▇▁▂▁▄▂▃▃▅▄▄▅▄▆▃▅▃▂▃▃█▅▅▁▃▂▁▄▅▃▁▄▁▂█▄▄▃▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.attn.qkv.weight</td><td>▆▄▂▁▁▄▃▄▃██▅▃▁▃▅▃▃▃▂▃▃▄▄▄▃▂▁▇▄▃▂▁▂▃▂▁▁▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.attn.relative_position_bias_table</td><td>█▁▁▄▁▂▂▂▃▄▂▅▅▂▅▂▃▁▁▂▂▂▁▃▂▃▃▂▂▂▂▂▁▆▃▂▁▁▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.mlp.fc1.bias</td><td>▅█▆▂▁▂▂▂▂▃▂▃▂▅▂▂▆▂▂▂▂▄▂▂▂▄▁▁▁▁▁▁▂▂▂▁▁▁▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.mlp.fc1.weight</td><td>█▅▁▁▄▁▂▁▁▁▃▂▃▂▂▁▁▂▄▂▂▂▂▂▃▁▁▂▂▃▂▂▁▁▁▂▂▄▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.mlp.fc2.bias</td><td>▂█▁▃▁▁▂▁▁▂▃▁▂▂▁▁▃▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.mlp.fc2.weight</td><td>▄▅▁▅▃▃▁▂▁▂▇▃▆▁█▃▄▂▂▄▄▆▂▂▂▂▂▃▁▂▃▂▁▂▂▃▂▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.norm1.bias</td><td>▅█▁▁▃▂▄▂▄▃▃▄▂▂▄▆▄▂▃▄▂▃▂▁▂▂▂▃▁▁▁▁▂▁▂▂▁▂▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.norm1.weight</td><td>▅▅▅▄▁▁▂▆▆█▇▁▃▄▂▄▅▂▃▅▄▂▃▅▃▅▂▂▂▄▁▁▁▂▃▂▂▃▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.norm2.bias</td><td>█▅▁▁▁▂▁▃▁▁▃▂▄▄▂▃▆▃▃▂▃▃▂▃▂▃▂▃▂▄▂▂▁▅▁▂▂▃▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.norm2.weight</td><td>▃▄▁▄▁▂▂▄▂█▂▂▁▂▂▃▃▁▂▅▂▂▂▁▂▂▂▁▁▂▂▂▁▂▁▁▂▁▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.attn.proj.bias</td><td>▃█▄▂▁▁▂▂▁▁▁▁▁▁▂▁▂▅▁▂▂▁▁▁▂▁▁▁▁▂▁▁▂▁▁▁▁▂▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.attn.proj.weight</td><td>█▂▂▁▃▁▂▂▂▂▃▁▅▁▁▁▁▃▃▁▂▂▂▂▂▄▁▁▂▁▁▃▂▂▂▁▂▂▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.attn.qkv.bias</td><td>▄▇▁▂▁▃▃▃▄▅▂▂▁█▂▄▅▄▁▄▁▂▂▃▁▂▂▁▂▁▁▃▃▁▂▂▂▃▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.attn.qkv.weight</td><td>█▃▅▁▁▃▃▃▂▄▃▂▂▂▂▃▃▂▁▃▂▁▂▂▃▁▂▁▁▁▁▄▃▁▂▁▂▂▁▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.attn.relative_position_bias_table</td><td>▃▃▁▁▂▂▁▂▃▄▃▃▃▆▂▃▄▇▃█▃▃▆▃▄▃▂▂▂▃▁▂▃▁▁▄▁▂▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.mlp.fc1.bias</td><td>▇▂▁▁▁▁▁▂▃▂▄▇█▄▂▂▂▂▅▂▂▂▁▃▃▃▁▃▁▆▁▁▂▁▁▃▁▂▆▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.mlp.fc1.weight</td><td>▄▆▁▃▁▂▂▂▃▄▂▂▂█▆▄▃▂▃▃▂▂▂▄▄▁▂▁▃▂▄▂▁▁▁▁▁▂▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.mlp.fc2.bias</td><td>▇▅█▁▇▁▃▂▂▃▃▃▂▃▄▆▄▂▂▇▆▃▂▃▁▁▁▂▂▁▄▁▁▁▂▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.mlp.fc2.weight</td><td>█▄▁▅▂▂▂▃▃▇█▁▃▇▆▅▂▃▃▄▁▇▂▂▇▃▂▁▃▆▂▂▃▆▂▅▁▂▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.norm1.bias</td><td>▂▆▁▂▂▄▂▃▆▅▃▄▅▂▆▃▂▄▂▄▂▂▂▆█▃▃▂▃▂▆▂▁▂▁▂▂▂▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.norm1.weight</td><td>▄▁▂▂▃▃▃▂▃▂▄▂▃▂█▃▄▅▃▁▂▂▁▃▁▃▂▂▁▂▁▁▂▂▂▁▂▂▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.norm2.bias</td><td>▁▁▄▂▁▃▂▃▃▅▅▆▅▃▃▄▂▂█▂▃▃▅▁▃▁▁▁▃▄▂▆▃▂▂▃▂▃▂▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.norm2.weight</td><td>█▆▁▁▂▂▃▃▆▅▇▂▄▄▆▄▂█▆▅▂▂▁▂▃▄▂▁▃▂▂▁▁▁▂▃▃▂▂▆</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.attn.proj.bias</td><td>▇▁▁▅▄▂▂▃▂▅▇▂▂▆▂▂▃▂▂▅▆█▂▃▂▂▂▇▂▁▃▄▂▅▂▂▂▂▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.attn.proj.weight</td><td>▆█▂▁▁▁▁▂▃▂▃▃▃▆▅▃▂▄▁▃▂▃▂▂▂▂▂▂▁▁▁▂▃▁▁▁▁▂▁▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.attn.qkv.bias</td><td>▃▂▁▄▁▃▂▃▂▂▃█▁▂▃▁▃▃▁▃▄▂▂▁▁▂▂▁▁▁▅▁▂▂▂▂▂▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.attn.qkv.weight</td><td>█▁▁▆▁▄▄▃▃▃▃▂▇▂▂█▆▄▃▄▅▂▄▂▅▂▁▁▃▂▂▂▁▁▁▁▂▂▂▄</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.attn.relative_position_bias_table</td><td>█▃▁▁▁▁▃▂▂▃▂▃▃▂▂▂▂▁▁▁▂▁▁▁▂▂▁▂▂▁▃▁▁▂▁▁▁▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.mlp.fc1.bias</td><td>▃▄▁▁▁▂▃▁▄▂▅▃▃▇▃█▄▄▃▆▂▃▂▂▄▂▂▃▂▂▄▂▁▁▁▂▁▃▄▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.mlp.fc1.weight</td><td>▃▁▂▂▂▃▃▅▂▄▃▁▃▃▇▄█▆▃▂▄▂▃▂▃▂▆▃▂▇▄▂▃▂▁▂▂▃▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.mlp.fc2.bias</td><td>█▁▁▂▂▁▂▂▃▅▆▃▄▁▃▁▃▁▄▂▁▁▁▃▂▃▅▂▂▂▂▂▁▁▄▃▂▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.mlp.fc2.weight</td><td>█▃▅▁▁▁▃▂▄▃▅▃▂▂▄▆▂▃▄▆▃▄▃▂▂▂▇▂▃▄▁▂▂▁▂▂▃▂▃▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.norm1.bias</td><td>█▆▁▄▂▃▃▂▃▂▄▃▂▆▂▁▂▁▁▁▆▁▁▃▂▁▂▁▁▂▁▇▁▂▃▂▆▁▃▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.norm1.weight</td><td>▆▁▆▁▁▃▃▅▃▂█▃▄▄▂▂▂▃▂▃▂▃▂▃▃▂▄▂▃▂▂▂▁▁▂▂▁▂▇▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.norm2.bias</td><td>▅▂▁▁▂▂▄▄▄▃▃█▃▄▂▆▂▃▁▇▁▃▂▂▂▂▃▂▆▁▂▂▁▁▂▂▄▂▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.norm2.weight</td><td>▇▂▁▁▁▅▃▆▃▅▂▃▄▂▃▆▃▁▂▂▃▂▅▃▃▂▂▁▁▂▂▃▃▁▃▂▁▄▂█</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.attn.proj.bias</td><td>█▂▂▁▂▁▂▂▁▃▁▂▃▂▃▂▂▁▃▂▂▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.attn.proj.weight</td><td>▄█▁▁▁▂▂▂▂▂▂▂▄▃▃▅▂▃▃▁▃▁▂▂▃▂▂▂▂▂▁▁▂▄▁▁▁▂▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.attn.qkv.bias</td><td>▅█▃▁▁▁▂▂▂▂▂▃▂▂▁▃▄▂▁▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.attn.qkv.weight</td><td>▃▄▁▁▂▁▂▃▂▃█▁▂▃▆▃▂▂▃▂▂▁▁▂▂▂▂▄▁▁▁▂▁▂▅▂▂▂▂▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.attn.relative_position_bias_table</td><td>█▂▁▁▁▁▂▂▄▂▂▇▃▂▃▂▂▂▃▂▁▂▂▂▂▂▂▂▃▁▂▂▁▂▁▁▁▂▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.mlp.fc1.bias</td><td>█▁▁█▁▃▁▂▂▄▃▇▅▃▂▃▃▁▂▇▄▇▃▂▃▁▂▁▂▂▂▁▃▂▂▁▂▁▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.mlp.fc1.weight</td><td>▄█▆▃▁▄▂▂▄▄▆▂▇▃▃▃▃▇▃▄▂▄▂▇▃▃▂▃▂▃▆▂▃▁▁▂▂▁▂▄</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.mlp.fc2.bias</td><td>▅▁▁▃▄▂▄▄▃▃▂█▇▃▁▂▁▄▁▁▁▂▁▂▂▂▂▁▇▁▂▁▂▃▂▂▂▂▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.mlp.fc2.weight</td><td>▄▅▁▁▁▂▁▂▂▁▂▃▃▂▃▆▂█▂▂▄▃▆▂▅▂▂▂▁▁▃▃▂▁▁▃▂▂▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.norm1.bias</td><td>▅▂▁▁▄▂▂▂▂█▁▂▂▂▁▁▂▂▂▂▁▂▂▃▁▁▁▁▁▁▁▁▁▁▂▃▁▁▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.norm1.weight</td><td>▆▃▁▁█▃▂▅▃▄▄█▄▃▅▃▅▂▅▇▄▂▄▄▃▃▂▃▂▂▅▄▁▄▂▂▅▂▅▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.norm2.bias</td><td>▃█▅▂▁▁▂▁▂▁▂▂▂▂▄▂▂▂▂▂▁▂▁▁▁▂▂▂▁▂▁▁▁▁▂▂▁▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.norm2.weight</td><td>▂█▂▂▁▂▁▁▂▂▂▂▁▁▃▁▂▁▁▁▂▂▃▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.attn.proj.bias</td><td>██▂▂▂▂▂▄▅▄▃▂▆▃▃▁▂▂▆▂▂▂▄▂▃▃▂▂▂▁▂▂▂▁▁▄▂▂▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.attn.proj.weight</td><td>█▂▅▂▂▂▄▂▃▄▄▆▃▃▄▂▃▇▁▄▁▄▆▂▃▃▂▇▁▂▁▂▄▁▄▁▅▃▁▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.attn.qkv.bias</td><td>▆▂▁▁▃▂▅▃▃▃▂▅▆▂█▂▁▇█▆▂▁▂▂▁▁▂▂▁▃▁▁▁▄▃▂▁▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.attn.qkv.weight</td><td>▇▁▂▂▂▂▃▃▃▃▃█▁▂▁▄▃▂▃▂▇▃▂▂▃▁▂▂▂▁▂▂▁▂▃▁▁▁▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.attn.relative_position_bias_table</td><td>▂▂▃▂▂▄▄▅▄▅▇▅▄▇▃▇▃▆▆▄▃▄▅█▄▅▃▂▆▃▃▄▃▃▅▁▃▄▃▄</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.mlp.fc1.bias</td><td>█▁▁▃▃▂▂▂▂▂▃▄▂▁▂▂▆▄▃▂▂▃▃▂▄▂▂▁▁▁▁▁▂▁▁▁▂▂▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.mlp.fc1.weight</td><td>█▄▃▁▃▂▂▃▃▃▂▁▃▂▂▄▃▁▃▂▃▂▂▂▂▂▂▃▂▂▁▁▂▁▁▂▂▁▆▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.mlp.fc2.bias</td><td>█▃▁▁▁▁▃▂▂▃▃▄▁▃▃▁▁▄▂▁▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▂▂▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.mlp.fc2.weight</td><td>▄█▁▃▁▂▁▃▃▄▂▃▃▆▂▁▂▁▂▁▂▂▁▂▂▁▂▁▅▂▁▁▁▄▄▁▁▂▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.norm1.bias</td><td>▅▅▄▃▂▃▃▃▄▃▄▄█▂▃█▂▁▄▂▁▃▂▅▁▁▁▁▂▂▁▂▁▂▂▁▂▂▃▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.norm1.weight</td><td>▇▃▇▂▄▃▂▃▄▄▆▅▁█▃▃▅▃▄▅▃▂▂▄▃▃▄▃▄▂▂▇▂▂▂▄▅▄▂▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.norm2.bias</td><td>█▁▃▁▃▃▂▆▃▆▄▄▂▅▁▂▅▂▁▂▂▁▃▂▂▁▁▂▁▂▁▄▃▅▄▁▁▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.norm2.weight</td><td>█▁▁▃▃▁▂▂▃▄▃▃▅▂▃▂▄▃▄▄▂▂▂▃▂▃▂▃▃▁▁▂▂▁▂▁▁▂▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.attn.proj.bias</td><td>▇▁▄▂▂▂▃▃▄▃▃▃▄█▃▂▄▂▄▃▂▃▂▂▃▄▁▁▄▂▁▁▂▂▁▁▂▁▄▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.attn.proj.weight</td><td>▄▁▁▁▁▂▂█▂▂▁▁▂▃▃▂▂▁▂▃▂▁▁▁▁▁▁▂▂▁▁▁▁▂▁▁▂▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.attn.qkv.bias</td><td>▃█▅▂▁▁▁▂▂▂▄▂▂▂▁▁▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.attn.qkv.weight</td><td>█▁▁▁▇▄▁▂▂▃▄▄▃▇▁▃▄▂▂▂▃▆▃▃▃▂▃▅▁▁▁▁▂▄▁▂▁▂▁▄</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.attn.relative_position_bias_table</td><td>█▅▂▃▁▁▁▁▁▁▁▁▂▂▂▂▂▁▂▂▃▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.mlp.fc1.bias</td><td>▂▂▃█▃▄▃▂▄▂▂▃▂▂▂▂▂▅▂▁▆▅▂▂▂▁▂▂▂▁▂▁▁▁▂▂▂▁▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.mlp.fc1.weight</td><td>▅█▄▂▄▁▁▁▃▂▁▃▂▅▁▃▃▃▁▂▁▂▂▁▂▁▃▂▂▂▂▁▁▁▂▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.mlp.fc2.bias</td><td>█▅▂▂▃▂▂▃▄▃▃▃▃▄▆▄▂▅▆▃▅▂▂▁▂▂▄▂▇▃▁▁▁▂▃▂▂▂▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.mlp.fc2.weight</td><td>█▆▁▃▂▃▄▃▃▄█▃▆▇▅▃▂▄▂▅▂▃▅▂▂▂▅▂▁▁▂▂▁▁▃▂▅▂▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.norm1.bias</td><td>▂█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.norm1.weight</td><td>▅▄▁█▁▂▄▃▃▂▄▇█▂▁▇▃▂▃▂▃▃▃▂▂▁▅▂▂▃▂▃▂▁▁▂█▂▁▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.norm2.bias</td><td>▃█▂▁▁▁▁▂▂▂▂▃▂▂▁▄▂▂▁▂▂▂▁▁▂▃▁▁▁▂▂▁▁▁▁▂▁▁▃▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.norm2.weight</td><td>▁▁▁▃▂▄▂▅▃▄▄▅▄▄▂▄▃▃▂█▄▂▂▂▄▅▁▂▁▆▁▁▁▁▃▂▁▂▂▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.attn.proj.bias</td><td>█▃▂▁▂▂▁▃▇▄▂▂▂▄▂▃▂▂▄▄▂▂▁▄▂▁▂▂▁▂▄▁▃▂▃▂▁▂▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.attn.proj.weight</td><td>█▁▄▁▂▂▂▂▂▂▂▇▂▂▂▂▃▃▂▁▂▂▁▁▂▂▂▂▂▂▂▁▁▁▃▁▂▂▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.attn.qkv.bias</td><td>▂▁▁▁▁▂▁▂▃▃█▃▂▂▄▂▃▁▃▆▁▂▁▂▂▂▂▁▂▁▁▁▁▂▁▄▂▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.attn.qkv.weight</td><td>█▃▂▂▁▁▁▁▂▂▂▂▂▁▂▂▁▁▂▂▂▂▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▂▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.attn.relative_position_bias_table</td><td>▄▅▁▂▂▂▃▃▃▃▄▆▄▄▆█▅▃▂▅▂▄▅▂▄▄▁▁▂▃▁▅▂▂▁▄▂▂▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.mlp.fc1.bias</td><td>▅▄▂▃▁▁▄▄▃▃▄█▃▃▄▂▆▄▂▄▂▂▁▃▄▁▆▃▃▁▁▂▇▂▃▂▁▃█▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.mlp.fc1.weight</td><td>▅▄▇▁▁▁▃▃▃▂▃█▇▂▂▂▂▂▂▂▃▁▂▁▃▃▄▂▃▁▆▃▁▄▂▂▁▁▃▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.mlp.fc2.bias</td><td>▄▁▁▆▃▂▂▂█▄▂▆▂▂▃▄▂▅▂▁▂▁▂▃▂▂▁▂▁▅▂▁▂▅▂▂▂▂▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.mlp.fc2.weight</td><td>▅█▅▄▃▂▂▂▅▄▆▂▄▇▁█▃▅▂▇▃▂▃▂▃▃▁▂▄▅▁▂▃▅▃▂▃▁▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.norm1.bias</td><td>█▂▁▂▁▁▁▁▁▁▃▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▂▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.norm1.weight</td><td>▆▁▃▁▁▂▂▂█▂▃▂▄▄▃▂▁▄▁▂▂▁▁▂▂▁▁▂▁▂▂▁▂▁▁▁▁▂▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.norm2.bias</td><td>█▂▃▁▅▃▂▂▂▂▃▃▃▂▃█▃▅▂▂▂▂▅▁▄▂▂▁▄▂▃▂▁▁▁▂▂▂▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.norm2.weight</td><td>▇▃▂▃▃█▂▃▃▃▇▃▂▂▅▃▄▂▂▂▂▁▄▂▂▁▆▂▂▁▁▂▂▁▂▁▁▂▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.attn.proj.bias</td><td>▁▁▁▂▂▂▂▃█▅▁▂▃▂▃▁▂▄▃▃▁▁▁▁▂▁▁▁▁▂▁▁▂▁▁▁▁▂▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.attn.proj.weight</td><td>▆█▆▂▁▂▂▂▃▃▃▃▃▃▃▇▃▅▂▂▆▂▂▁▂▂▂▂▂▂▂▃▆▂▁▁▂▂▃▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.attn.qkv.bias</td><td>▅▁▁▁▂▂▂▃▄█▆▃▂▄▇▃▁▃▇▃▂▂▃▃▂▃▂▃▂▅▂▁▆▂▁▁▂▃▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.attn.qkv.weight</td><td>█▄▂▁▂▂▂▂▂▃▂▃▁▁▁▂▂▁▁▁▂▁▁▂▁▁▂▁▂▁▃▁▁▂▁▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.attn.relative_position_bias_table</td><td>▄▃█▂▁▂▂▂▂▂▂▂▅▅▃▅▄▄▄▇█▂▄▃▂▁▁▂▁▂▁▁▂▂▄▁▂▁▃▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.mlp.fc1.bias</td><td>▅▂▆▂▃▁▄▂▃▃▃▆▅▅▇▃▄▅▇▂▂▄█▆▃▆▃▂▅▃▄▁▁▁▂▂▃▃▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.mlp.fc1.weight</td><td>▁▃▂▂▃▁▁▄▃▄▂▄▁▆▆▂▅▄▃▂▃▄▃▂▂▃▄▂▁▂▃▁▇▄▆▂▅▃█▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.mlp.fc2.bias</td><td>█▆▂▁▁▂▂▂▂▂▃▆▃▃▅▃█▂▂▂█▆▂▂▄▂▁▂▃▂▁▂▂▂▂▂▂▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.mlp.fc2.weight</td><td>▄▁▁▁▁▂▂▂▂▂█▁▁▅▃▁▁▃▂▁▁▁▄▁▂▁▂▂▁▂▁▂▂▁▃▁▁▂▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.norm1.bias</td><td>█▄▂▅▂▂▃█▅▇▅▇▂▂▅▃▂▃▅▂▁▁▂▂▄▂▂▁▅▂▁▅▃▂▁▁▁▃▂▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.norm1.weight</td><td>▂▂▂▄▂▄▃▆▄▅█▄▇▂▃▂▁▆▁▂▂▃▂▂▁▂▂▂▁▁▅▂▂▁▁▂▂▂▄▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.norm2.bias</td><td>▆▁▁▂▂▂▁▃▂▃▄▃▄▃▅▃▆▅▃▁▂▄▄▃▂▃▂▂▂▁▁▃▃▇▁▁▁▂█▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.norm2.weight</td><td>█▂▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▁▁▃▂▃▁▁▃▂▁▁▂▂▁▁▁▁▂▂▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.attn.proj.bias</td><td>█▁▁▁▁▁▂▂▁▂▂▂▂▃▂▂▁▂▂▂▂▁▃▂▂▁▂▂▁▁▁▁▂▂▁▁▁▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.attn.proj.weight</td><td>▆█▄▁▁▂▂▃▆▇▃▄▅▆▄▅▂▃▄▅▂▁▂▂▂▂▁▁▃▂▂▁▃▂▂▄▃▂▃▅</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.attn.qkv.bias</td><td>▁▇▆▁▁▃▄█▄▄▃▃▅▁▃▄▃▂▄▄▂▆▂▂▃▁▂▂▃▁▁▁▂▂▂▂▂▂▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.attn.qkv.weight</td><td>▁▁▆▂▁▂▂▂▃▃▁█▅▆▄▂▄▂▂▄▂▃▁▃▂▃▃▁▁▃▁▂▁▃▂▂▂▂▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.attn.relative_position_bias_table</td><td>█▂▁▅▃▂▃▆▂▃▅▄▃▄▂▂▂▁▂▂▃▂▂▂▂▂▂▃▃▂▂▃▂▁▂▂▁▁▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.mlp.fc1.bias</td><td>▆█▅▁▁▃▁▂▂▃▄▆▅▂▂▃▃▁▂▃▂▂▃▃▃▁▃▂▁▁▂▁▁▃▃▁▁▂▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.mlp.fc1.weight</td><td>█▃▄▂▁▂▂▂▂▁▂▂▂▂▅▂▃▂▃▂▂▂▁▄▂▁▂▂▂▂▁▁▂▂▂▁▁▁▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.mlp.fc2.bias</td><td>█▅▇▂▁▆▁▃▂▂▂▂▁▃▃▅▂▄▁▃▂▃▅▅▂▂▁▂▃▆▁▁▂▁▂▄▂▂▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.mlp.fc2.weight</td><td>▄█▃▁▂▁▂▂▂▂█▂▂▁▁▄▂▃▂▃▃▁▂▁▂▂▁▃▁▂▁▁▂▂▂▁▁▂▄▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.norm1.bias</td><td>▅▁▄▁▂▂▁▂▂▂▂█▃▃▃▁▂▂▅▂▂▁▂▂▁▂▂▂▁▁▁▄▁▁▁▃▁▂▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.norm1.weight</td><td>▅▄▁▁▁▁▁▄▃▆▄▄▅██▃▆▃▄▄▃▆▃▄▃▂▄▃▃▁▂▁▁▁▁▂▂▁▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.norm2.bias</td><td>▄█▁▁▁▁▁▁▂▂▃▂▂█▅▂▃▂▂▄▃▁▃▁▁▂▁▁▁▂▁▁▁▂▁▁▂▁▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.norm2.weight</td><td>▃█▅▂▁▂▁▂▁▃▃▃▁▃▁▂▃▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.attn.proj.bias</td><td>█▂▄▃▁▁▂▁▁▁▁▂▃▂▂▃▂▂▁▂▂▂▁▂▂▁▁▁▁▁▂▁▁▁▁▁▂▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.attn.proj.weight</td><td>█▁▁▂▂▂▁▂▂▃▂▂▁▂▃▃▂▁▄▃▂▁▁▁▂▃▁▂▁▂▁▁▂▂▁▁▂▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.attn.qkv.bias</td><td>▇▂▁▇▆▂▂▂▂▂▃▁█▃▆▂▅▂▂▃▄▂▆▂▁▃▃▁▂▂▄▁▂▃▁▂▃▄▂▄</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.attn.qkv.weight</td><td>█▃▂▄▁▂▃▂▄▂▂▃▂▃▃▂▃▂▂▂▂▁▁▃▂▁▁▂▁▂▃▁▂▁▂▂▂▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.attn.relative_position_bias_table</td><td>▃▂▃▁▁▂▂▂▄▂▂▄▅▃▃▂▃▃▂▂▃▃▃▃▂▂▂▂▁▁█▁▃▁▂▂▂▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.mlp.fc1.bias</td><td>█▄▁▁▃▁▁▁▂▂▁▂▂▂▂▂▁▁▁▂▃▂▂▂▁▂▁▂▃▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.mlp.fc1.weight</td><td>▂▅▃▁▁▂▃▃▃▇█▃▅▂▂▂▄▃▂▆▁▅▂▂▂▂▃▅▁▂▁▁▂▄▃▂▂▂▁▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.mlp.fc2.bias</td><td>█▂▁▃▁▁▁▂▂▃▁▃▁▂▂▁▂▁▃▁▂▁▁▂▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.mlp.fc2.weight</td><td>▄▇▂▂▂▄▅▆▂▄▄▇▇▄▂▂█▁▇▁▅▃▂▂▂▅▂▃▃▇▁▄▁▂▂▂▂▁▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.norm1.bias</td><td>▅█▂▁▂▁▂▁▂▂█▂▁▃▃▃▂▁▂▃▂▂▂▁▁▂▂▁▂▁▃▁▁▂▃▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.norm1.weight</td><td>█▃▁▂▁▂▁▁▁▁▁▂▂▅▂▃▂▂▂▂▂▁▁▂▂▁▂▁▂▁▁▁▁▁▂▁▁▂▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.norm2.bias</td><td>▄▅▅▁▁▆▄▂▂▂▅▃▅▅▇▅▅▃▅▄▃▂▂▃█▅▆▁▂▃▂▂▁▆▄▁▂▂▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.norm2.weight</td><td>▅▂▁▂▂▂▂▃█▃▂▂▁▄▃▄▂▅▃▅▁▂▂▂▂▁▄▁▂▁▂▁▁▂▂▂▂▂▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.attn.proj.bias</td><td>█▃▁▁▃▁▁▂▂▃▅▂▂▃▂▁▁▂▁▂▂▁▁▁▁▁▁▁▃▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.attn.proj.weight</td><td>▂█▁▁▁▁▁▂▂▃▃▃▂▂▁▁▂▁▂▂▁▁▁▁▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.attn.qkv.bias</td><td>█▆▁▂▁▃▃▂▃▇▄▃▁▂▄▇▃▆▂▁▁▂▁▅▁▂▂▂▁▂▁▁▂▁▃▂▂▁▃▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.attn.qkv.weight</td><td>█▃▂▁▁▁▂▂▄▂▂▂▃▂▂▁▁▂▁▃▁▂▂▂▂▁▃▁▁▁▁▁▂▁▁▁▁▂▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.attn.relative_position_bias_table</td><td>█▄▁▁▃▁▁▂▁▂▂▃▃▂▂▃▂▁▄▃▂▁▁▁▁▂▃▂▂▂▁▁▂▄▂▂▂▁▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.mlp.fc1.bias</td><td>█▂▅▃▁▄▂▂▂▁▂▂▁▂▂▂▂▄▁▃▄▂▁▂▁▁▂▁▂▁▂▁▂▃▁▂▁▂▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.mlp.fc1.weight</td><td>▅▁▅▁▃▃█▅▃▃▄▁▃▅▂▄▂▃▂▃▃▂▄▃▂▃▁▃▂▁▂▁▃▂▁▂▁▂▂▄</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.mlp.fc2.bias</td><td>▁▂▁▁▂▂▁▂▂▂█▄▂▄▁▂▁▁▁▃▂▁▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▂▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.mlp.fc2.weight</td><td>▁▂▃▂▂▄▃▅▄▄▂▇█▂▂▂▃▃▃▅▃▂▆▃▅▂▂▂▅▆▃▁▁▁▁▃▂▁▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.norm1.bias</td><td>▄█▁▄▁▂▂▂▃▃▃▃▁▃▃▂▂▂▂▂▁▁▁▂▂▁▁▂▁▁▂▁▂▁▂▂▂▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.norm1.weight</td><td>▄▁▁▃▄▄▃▅▂▁▅▂▃▂▂█▂▄▂▃▅▇▂▂▁▁▂▃▄▁▂▁▂▁▁▂▂▁▃▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.norm2.bias</td><td>█▃▃▄▁▁▁▂▁▂▃▃▃█▄▄▄▂▃▃▁▃▃▃▄▂▂▄▂▂▁▁▄▁▂▂▁▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.norm2.weight</td><td>█▃▁▁▃▁▁▁▁▁▂▂▃▃▁▂▃▁▁▂▄▂▁▁▂▁▂▂▁▁▃▂▂▁▁▁▁▁▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.attn.proj.bias</td><td>█▂▁▂▁▂▂▂▂▂▁▂▂▂▁▂▁▂▁▂▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▃▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.attn.proj.weight</td><td>▃█▂▂▃▂▁▁▂▁▁▁▂▁▂▂▂▂▂▁▁▁▃▂▁▁▁▁▂▁▁▁▁▂▂▂▁▂▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.attn.qkv.bias</td><td>█▂▁▁▁▁▂▂▂▂▂▄▂▁▁▂▄▂▁▂▁▂▁▁▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.attn.qkv.weight</td><td>▅▁▁▄▃▄▄▄▄▇▂▇▅▇▅▂▄█▁▅▂▂▃▂▂▄▂▃▇▁▁▂▃▁▁▂▃▁▁█</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.attn.relative_position_bias_table</td><td>█▂▁▁▁▁▁▁▁▂▃▂▂▂▁▂▂▂▁▂▁▁▂▁▂▂▁▁▂▁▂▁▁▁▂▂▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.mlp.fc1.bias</td><td>▅▄▆▁▁▂▂▂▂▄▅▄▃█▃▃▃▂▂▃▁▂▂▃▂▂▆▁▄▂▂▃▁▂▁▂▂▂▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.mlp.fc1.weight</td><td>█▃▅▁▃▁▃▃▄▃▃▃▃▅▄▃▄▁▃▆▆▂▆▃▂▃▃▂▁▃▂▁▁▁▃▂▂▂▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.mlp.fc2.bias</td><td>█▆▁▁▄▂▂▃▃▃▅▃▃▅▄▂▂▄▄▄▄▁▂▂▁▁▂▂▂▂▂▂▂▂▂▂▆▂▃▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.mlp.fc2.weight</td><td>█▆▃▁▃▅▂▂▂▃▃▅▃▃▃▄▄▂▂▂▃▇▂▂▂▃▁▃▃▂▂▁▁▂▂▂▁▂▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.norm1.bias</td><td>█▄▁▂▁▂▂▂▂▂▂▃▅▂▂▁▂▂▂▂▁▁▁▂▁▁▂▁▄▁▁▃▁▁▁▂▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.norm1.weight</td><td>▇█▃▆▁▃▂▂▃▃▃▆▃▂▂▂▃▇▂▄▂▂▁▂▃▃▃▁▃▂▁▁▂▂▃▁▁▂▁▄</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.norm2.bias</td><td>█▂▁▁▂▃▂▃▄▂▇▂▁▂▆▄▂▄▂▄▂▂▃▂▂▁▁▁▅▃▃▂▃▂▂▁▂▂▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.norm2.weight</td><td>█▃▂▁▂▂▂▂▃▄▄▂▂▃▂▃▂▃▃▂▂▁▂▂▂▃▂▂▁▂▂▂▂▁▁▂▂▁▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.attn.proj.bias</td><td>▅▁▂▂▂▄▆▄▆▇▃▂▅█▂▂▅▂▂▃▃▃▃▂▂▆▂▂▁▁▄▂▃▂▂▁▁▂▁▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.attn.proj.weight</td><td>█▁▅▁▂▂▃▇▃▅▂▄▂▇▃▄▃▄▃▃▁▁▂▅▆▄▄▂▁▆▁▂▁▂▅▂▁▂▁▃</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.attn.qkv.bias</td><td>▆█▂▁▄▁▂▂▂▂▂▁▂▃▄▂▃▂▁▂▁▂▃▄▂▂▂▁▂▂▂▂▂▁▂▁▁▁▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.attn.qkv.weight</td><td>█▃▁▁▁▂▂▁▃▅▄▃▂▅▂▅▂▂▂▁▃▂▂▂▁▁▁▃▂▃▂▂▃▃▁▂▂▂▁▄</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.attn.relative_position_bias_table</td><td>▅▄█▁▁▅▂▂▁▂▂▄▃▃▆▄▄▃▂▂▆▄▄▅▃▄▁▆▃▁▂▂▂▂▁▁▂▂▂▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.mlp.fc1.bias</td><td>▂▁▇▁▃▃▄▄▃█▂▄▅▆▅▅▄▅▄▄▅▂█▃▂▁▄▄▄▃▄▃▂▂▂▄▂▂▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.mlp.fc1.weight</td><td>▂▂▁▁▂▂▁▂▂▄▃▂▃▂▄▆▄▄█▄▄▂▂▃▂▃▄▃▄▅▂▃▁▁▁▁▂▄▃▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.mlp.fc2.bias</td><td>█▃▃▁▁▂▁▁▁▂▄▁▁▁▁▂▂▂▃▁▃▂▂▂▁▂▂▂▁▃▃▁▁▂▂▂▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.mlp.fc2.weight</td><td>█▃▁▁▁▁▁▁▁▁▂▂▃▂▂▃▂▃▂▁▂▁▂▁▂▃▂▁▁▁▁▁▂▁▁▂▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.norm1.bias</td><td>▇█▇▅▂▅▃▁▂▁▂▂▃▃▂█▅▄▄▄▃▂▂▃▁▂▂▂▃▂▂▁▁▁▂▂▂▂▂▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.norm1.weight</td><td>█▃▁▂▂▁▂▃▃▄▂▃▅▅▂▂▆▁▂▂▁▃▅▂▃▃▂▁▅▁▂▃▂▂▂▃▁▁▃▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.norm2.bias</td><td>▁▇▃▂▂▁▁▂▂▂▆▅▄█▃▅▂▂▄▇▂▁▂▃▂▃▃▁▁▃▃▆▁▂▁▂▇▂▃▂</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.norm2.weight</td><td>▁▂▂▄▄▃▅▅▃▄▅▄▂█▆▃▂▄▂▂▂▅▃▇▃▃▅▂▁▂▁▂▂▆▁▇▂▃▂▃</td></tr><tr><td>gradients/backbone.layers.2.downsample.norm.bias</td><td>▆██▄▃▃▂▁▂▁▁▂▂▁▂▃▂▁▂▁▃▂▃▂▂▁▁▁▁▂▂▁▂▂▁▁▁▁▃▂</td></tr><tr><td>gradients/backbone.layers.2.downsample.norm.weight</td><td>▆▃▂▁▃▂▂█▃▆▂▂▂▂▇▅▃▄▁▄▁▂▂▂▂▃▁▂▁▅▃▁▁▁▁▁▁▁▂▁</td></tr><tr><td>gradients/backbone.layers.2.downsample.reduction.weight</td><td>█▅▃▁▂▃▄▅▄▆▄▄▆▃█▆▅▇▅▂▇▅▃▇▂▄▂▆▃▄▃▂▂▂▄▃▂▁▂▂</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.attn.proj.bias</td><td>▁█▁▃▁▂▂▂▃▂▄▄▁▂▂▄▁▅▂▄▂▂▁▂▄▃▄▂▆▁▂▁▂▅▁▂▁▁▂▂</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.attn.proj.weight</td><td>██▆▁▂▁▁▁▁▁▂▂▂▂▃▂▂▂▂▂▁▂▂▂▁▁▁▂▁▂▁▁▁▁▂▂▂▁▁▁</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.attn.qkv.bias</td><td>▅▂▂▃▁▂▄▅▆▄█▄▆▂▄▅▂▄▂▂█▄▄▅▁▁▆▂▁▂▂▂▇▁▂▁▁▁▁▄</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.attn.qkv.weight</td><td>▄█▅▆▂▁▁▂▁▂▂▂▂▁▂▁▂▃▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.attn.relative_position_bias_table</td><td>█▁▁▁▆▃▂▄▃▃▅▄▃▂▅▃▄▇▄▂▄▄▃▃▃▂▁▄▂▂▃▂▂▁▂▂▂▂▂▃</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.mlp.fc1.bias</td><td>▄▇▁▁▂▁▁▁▂▂▂█▃▃▂▃▂▃▁▃▂▄▁▁▃▁▁▂▁▃▁▁▁▂▁▁▃▁▁▁</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.mlp.fc1.weight</td><td>▆▁▆▅▃▄▆▄▆▃▆▄▃▃▄█▂▅█▂▃▂▂▃▂▃▃▂▃▂▃▂▂▅▂▂▃▁▂▂</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.mlp.fc2.bias</td><td>█▇▂▁▁▆▁▁▂▂▂▃▄▂▄▄▁▂▃▂▁▂▁▁▂▁▂▂▂▁▁▁▁▁▂▁▁▁▁▂</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.mlp.fc2.weight</td><td>▇▁▃▁▂▃▂█▂▂▄▁▂▁▃▂▁▁▂▁▁▂▂▁▁▁▁▁▃▂▃▁▂▂▁▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.norm1.bias</td><td>▁▁▅▃▁▁▃▄▃▃█▅▂▁▂▃▁▄▁▃▂▁▁▂▃▁▂▁▁▂▁▂▃▁▂▁▁▂▂▃</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.norm1.weight</td><td>▃█▂▁▁▂▂▂▂▂▃▂▄▃▄▂▄▂▂▂▁▁▂▂▄▃▂▁▁▄▂▁▁▂▂▂▂▁▁▁</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.norm2.bias</td><td>█▅▁▂▂▃▄▂▃▄▃▃▅▄▂▄▃▂▂▃▂▂▂▅▃▃▂▂▃▁▁▃▃▂▂▂▂▂▂▄</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.norm2.weight</td><td>▆█▁▂▂▂▂▂▃▃▃▂▃▃▁▁▁▃▃▃▁▁▂▁▁▂▂▁▁▂▄▁▁▁▁▁▁▁▁▂</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.attn.proj.bias</td><td>█▁▄▅▁▁▁▂▁▂▂▄▁▁▅▂▂▁▃▁▁▁▁▁▂▁▁▁▁▁▁▁▃▂▁▃▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.attn.proj.weight</td><td>▇█▃▁▄▂▂▂▃▂▂▂▄▂▁▁▃▁▂▁▁▁▂▂▂▁▁▂▁▂▁▁▁▂▂▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.attn.qkv.bias</td><td>▄█▂▅▄▄▁▂▁▂▂▂▃▃▂▂▁▃▂▁▁▄▂▁▁▁▁▁▁▃▂▁▁▃▃▁▁▁▂▁</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.attn.qkv.weight</td><td>▂█▆▃▃▂▃▅▄▅▃▆▅▃▂▆▃▄▂▂▄▃▂▃▂▃▂▃▁▃▁▁▃▃▂▁▂▅▂▃</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.attn.relative_position_bias_table</td><td>▇▆▆▄▂▃▂▄█▄▂█▆▃▃▃▄▅▃▂▄▃▅▃▂▁▃▂▄▁▃▁▁▁▁▃▂▃▂▃</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.mlp.fc1.bias</td><td>█▃▂▁▁▁▁▁▂▂▁▂▁▂▂▂▁▂▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.mlp.fc1.weight</td><td>▇▄▂▁▁▂▂▂▂▂▃▁█▃▂▃▁▁▁▂▂▁▁▁▂▂▂▂▁▁▂▁▂▁▁▁▁▁▂▁</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.mlp.fc2.bias</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.mlp.fc2.weight</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.norm1.bias</td><td>█▃▁▂▂▁▂▃▁▂▁▂▁▂▃▁▂▁▂▁▁▁▁▁▁▂▁▁▂▂▁▁▁▁▁▁▂▁▁▂</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.norm1.weight</td><td>█▁▁▄▄▄▃▃▄▂▃▃▅▁▂▄▂▂▃▂▂▃▃▂▁▂▂▂▂▃▂▂▁▁▃▂▂▄▂▂</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.norm2.bias</td><td>▁▁▂▂▁▃▃▂▄▁▄▃▁▁█▂▆▁▂▂▂▂▂▁▁▃▁▂▂▁▂▁▅▁▂▁▁▁▂▁</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.norm2.weight</td><td>▅▃▂▁▁▂▂▂▄▂█▂▄▂▂▁▁▂▁▂▁▁▁▂▂▂▁▂▂▂▁▁▃▁▁▁▄▁▁▁</td></tr><tr><td>gradients/backbone.norm.bias</td><td>█▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>gradients/backbone.norm.weight</td><td>▃▁█▃▄▃▃▃▂▂▂▁▁▁▄▅▁▁▄▂▂▂▂▂▅▁▂▂▁▁▂▅▁▁▂▁▁▂▁▁</td></tr><tr><td>gradients/backbone.patch_embed.norm.bias</td><td>█▁█▅▂▃▁▂▃▄▆▂▂▃▅▃▅▂▂▂▂▇▁▄▄▂▁▁▂▂▃▂▁▂▁▁▂▂▃▃</td></tr><tr><td>gradients/backbone.patch_embed.norm.weight</td><td>▇▇▁▂▃▅▆▄▃▄█▃▂▃▁▅▂▆▇▂▄▂▂▇▂▂▃▆▃▃▂▂▁▇▅▃▁▂▂▄</td></tr><tr><td>gradients/backbone.patch_embed.proj.bias</td><td>▂█▂▂▂▁▂▁▁▁▂▂▂▁▂▂▁▁▃▁▁▂▁▁▁▁▂▁▂▁▁▂▁▁▂▁▃▁▁▁</td></tr><tr><td>gradients/backbone.patch_embed.proj.weight</td><td>█▃▃▁▄▁▂▂▂▂▂▂▃▂▂▃▂▂▁▂▁▃▂▁▂▂▁▁▂▁▁▂▁▂▁▁▁▂▁▁</td></tr><tr><td>gradients/segmentation_head.decoder.0.bias</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>gradients/segmentation_head.decoder.0.weight</td><td>▆▃▅▆▆▆▆█▅▇▃▂▂▂▁▂▂▁▁▁▁▁▁▂▂▂▂▁▂▂▂▂▁▁▁▁▁▂▂▂</td></tr><tr><td>gradients/segmentation_head.decoder.1.bias</td><td>▆█▃▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>gradients/segmentation_head.decoder.1.weight</td><td>█▃▅▅▅▅▄▄▄▃▅▃▂▂▂▂▂▂▂▁▂▂▁▁▁▂▂▂▂▁▃▁▂▂▂▂▃▁▁▁</td></tr><tr><td>gradients/segmentation_head.decoder.10.bias</td><td>▇▇▇████▇▇▆▅▅▄▄▄▄▄▃▂▂▃▃▂▂▂▆▂▂▃▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>gradients/segmentation_head.decoder.10.weight</td><td>▃▄▅▄▅▆▆▇██▇▇▆▆▆▆▅▄▄▄▄▃▃▃▂▃▂▂▂▂▂▂▃▂▂▁▁▁▂▁</td></tr><tr><td>gradients/segmentation_head.decoder.12.bias</td><td>██▇▇▅▅▄▄▃▃▃▃▂▂▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>gradients/segmentation_head.decoder.12.weight</td><td>█▆▆▆▆▇▆▆▆▆▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>gradients/segmentation_head.decoder.3.bias</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>gradients/segmentation_head.decoder.3.weight</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>gradients/segmentation_head.decoder.4.bias</td><td>█▆▃▃▂▃▃▄▄▃▄▂▃▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>gradients/segmentation_head.decoder.4.weight</td><td>█▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▃▂▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>gradients/segmentation_head.decoder.6.bias</td><td>▅█▂▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▂▁▂▂▁▁▂▁</td></tr><tr><td>gradients/segmentation_head.decoder.6.weight</td><td>█▃▃▃▂▂▂▂▂▂▁▂▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>gradients/segmentation_head.decoder.7.bias</td><td>▄▅█▆▆▄▄▃▃▂▁▁▁▁▂▂▂▃▁▂▂▂▁▂▁▁▁▁▁▁▂▂▁▁▁▁▁▁▃▂</td></tr><tr><td>gradients/segmentation_head.decoder.7.weight</td><td>▅▅▆▇▃▃▃▅▂▂▂▂▂▂▂█▃▃▂▁▁▂▂▂▂▂▁▃▂▂▂▁▁▁▂▁▂▄▃▁</td></tr><tr><td>gradients/segmentation_head.decoder.9.bias</td><td>▃█▄▅▃▃▃▃▂▂▃▂▂▂▂▃▂▁▂▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>gradients/segmentation_head.decoder.9.weight</td><td>█▆▆▆▆▄▄▃▄▅▄▃▃▃▃▂▂▄▁▁▁▁▂▂▂▁▂▂▁▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>gradients/segmentation_head.fc.bias</td><td>█▂▂▂▂▃▃▂▂▃▂▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>gradients/segmentation_head.fc.weight</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▇▆▅▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>260</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>gradients/backbone.head.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.head.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.0.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.blocks.1.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.downsample.norm.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.downsample.norm.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.0.downsample.reduction.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.0.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.blocks.1.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.downsample.norm.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.downsample.norm.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.1.downsample.reduction.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.0.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.1.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.10.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.11.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.12.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.13.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.14.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.15.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.16.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.17.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.2.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.3.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.4.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.5.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.6.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.7.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.8.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.blocks.9.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.downsample.norm.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.downsample.norm.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.2.downsample.reduction.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.0.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.attn.proj.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.attn.proj.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.attn.qkv.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.attn.qkv.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.attn.relative_position_bias_table</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.mlp.fc1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.mlp.fc1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.mlp.fc2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.mlp.fc2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.norm1.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.norm1.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.norm2.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.layers.3.blocks.1.norm2.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.norm.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.norm.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.patch_embed.norm.bias</td><td>0.0</td></tr><tr><td>gradients/backbone.patch_embed.norm.weight</td><td>0.0</td></tr><tr><td>gradients/backbone.patch_embed.proj.bias</td><td>3e-05</td></tr><tr><td>gradients/backbone.patch_embed.proj.weight</td><td>1e-05</td></tr><tr><td>gradients/segmentation_head.decoder.0.bias</td><td>0.0</td></tr><tr><td>gradients/segmentation_head.decoder.0.weight</td><td>0.0</td></tr><tr><td>gradients/segmentation_head.decoder.1.bias</td><td>0.0</td></tr><tr><td>gradients/segmentation_head.decoder.1.weight</td><td>1e-05</td></tr><tr><td>gradients/segmentation_head.decoder.10.bias</td><td>0.00333</td></tr><tr><td>gradients/segmentation_head.decoder.10.weight</td><td>0.00267</td></tr><tr><td>gradients/segmentation_head.decoder.12.bias</td><td>0.0154</td></tr><tr><td>gradients/segmentation_head.decoder.12.weight</td><td>0.00178</td></tr><tr><td>gradients/segmentation_head.decoder.3.bias</td><td>0.0</td></tr><tr><td>gradients/segmentation_head.decoder.3.weight</td><td>0.0</td></tr><tr><td>gradients/segmentation_head.decoder.4.bias</td><td>1e-05</td></tr><tr><td>gradients/segmentation_head.decoder.4.weight</td><td>1e-05</td></tr><tr><td>gradients/segmentation_head.decoder.6.bias</td><td>0.0</td></tr><tr><td>gradients/segmentation_head.decoder.6.weight</td><td>1e-05</td></tr><tr><td>gradients/segmentation_head.decoder.7.bias</td><td>9e-05</td></tr><tr><td>gradients/segmentation_head.decoder.7.weight</td><td>8e-05</td></tr><tr><td>gradients/segmentation_head.decoder.9.bias</td><td>0.0</td></tr><tr><td>gradients/segmentation_head.decoder.9.weight</td><td>5e-05</td></tr><tr><td>gradients/segmentation_head.fc.bias</td><td>0.0</td></tr><tr><td>gradients/segmentation_head.fc.weight</td><td>0.0</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>training_loss</td><td>0.02882</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">playful-eon-39</strong> at: <a href='https://wandb.ai/esedx12/secret-runway-detection/runs/q37vwrii' target=\"_blank\">https://wandb.ai/esedx12/secret-runway-detection/runs/q37vwrii</a><br/> View project at: <a href='https://wandb.ai/esedx12/secret-runway-detection' target=\"_blank\">https://wandb.ai/esedx12/secret-runway-detection</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../wandb/run-20241108_185443-q37vwrii/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USING_COLAB:\n",
    "    from google.colab import runtime\n",
    "    runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
