{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Landing Strip Detection Training Pipeline\n",
    "\n",
    "\n",
    "\n",
    " This notebook implements a training pipeline for detecting landing strips using satellite imagery. The pipeline includes:\n",
    "\n",
    "\n",
    "\n",
    " - Loading input landing strip data.\n",
    "\n",
    " - Creating input areas around the landing strips.\n",
    "\n",
    " - Downloading Sentinel-2 imagery from Google Earth Engine.\n",
    "\n",
    " - Preparing a dataset for training.\n",
    "\n",
    " - Loading the Geo Foundation Model (GFM) for transfer learning.\n",
    "\n",
    " - Setting up a training loop with Weights & Biases (wandb) logging.\n",
    "\n",
    "\n",
    "\n",
    " **Note**: Ensure that you have authenticated with Google Earth Engine (GEE) using `ee.Authenticate()` and have initialized it with `ee.Initialize()`. Also, make sure `train_utils.py` is in your working directory or Python path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *TODO*\n",
    "* Max value of model outputs can be rather small (in one case, 0.6244). This leads to binary search setting threshold lower, predicting all zeroes\n",
    "* (buffered_labels.float() == 1).float()\n",
    "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
    "(buffered_labels.float() == 1).float().mean()\n",
    "tensor(0.2678) **(!!!)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'google.colab'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emil/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import timm  # PyTorch Image Models library\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# If on Google colab, chdir to /content/drive/MyDrive/Secret_Runway_Detection\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "    drive.mount('/content/drive')\n",
    "    # Copy the 'Secret Runway Detection Challenge' folder to Colab local storage\n",
    "    !cp -r '/content/drive/MyDrive/Secret Runway Detection Challenge/colab-stuff/' '/content/'\n",
    "    # Change the current working directory to the notebooks folder in local storage\n",
    "    os.chdir('/content/colab-stuff/notebooks')\n",
    "    wandb.login(key=userdata.get('WANDB_API_KEY'))\n",
    "    USING_COLAB = True\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    USING_COLAB = False\n",
    "\n",
    "# Add the src directory to the sys.path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import functions and constants from train_utils\n",
    "from secret_runway_detection.model import (\n",
    "    get_multiscale_segmentation_model\n",
    ")\n",
    "from secret_runway_detection.dataset import LandingStripDataset, SegmentationTransform\n",
    "from secret_runway_detection.train_utils import (\n",
    "    RANDOM_SEED\n",
    ")\n",
    "\n",
    "if not USING_COLAB:\n",
    "    sys.path.append(os.path.abspath('../GFM'))\n",
    "    from GFM.models import build_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Configuration and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Debug flag: Set to True to run on CPU, False to use GPU if available\n",
    "# With DEBUG == True, test and train sets are reduced to 10 samples each\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cpu') if DEBUG else torch.device(\n",
    "    'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# logging.getLogger('secret_runway_detection.train_utils').setLevel(logging.DEBUG)\n",
    "logging.getLogger('secret_runway_detection.train_utils').setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    'training_dataset': 'cross',\n",
    "    'train_percentage': 0.8,\n",
    "    'num_epochs': 50 if not DEBUG else 2,\n",
    "    'batch_size': 32 if USING_COLAB else 4,\n",
    "    'lr_head': 0.001,\n",
    "    'lr_backbone': 0.000001,\n",
    "    'lr_step_size': 10,\n",
    "    'lr_gamma': 0.3,\n",
    "    'early_stopping_patience': 3,\n",
    "}\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project='secret-runway-detection',\n",
    "           mode='online' if not DEBUG else 'dryrun',\n",
    "           dir='..',\n",
    "           tags=[config['training_dataset'],\n",
    "                 'colab' if USING_COLAB else 'local'],\n",
    "           job_type='train',\n",
    "           config=config,\n",
    "           )\n",
    "\n",
    "if not wandb.run.name:\n",
    "    wandb.run.name = f\"Run from {pd.Timestamp.now()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Load Data into Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 2130\n",
      "Total strips: 113\n",
      "Training files: 1700\n",
      "Testing files: 430\n"
     ]
    }
   ],
   "source": [
    "train_dir = Path(\n",
    "    f'../training_data/training_data_{config[\"training_dataset\"]}')\n",
    "\n",
    "if USING_COLAB:\n",
    "    # Unzip the training data which is at f\"{train_dir}.zip\" using python\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(f\"{train_dir}.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"../training_data\")\n",
    "\n",
    "images_dir = train_dir / 'images'\n",
    "labels_dir = train_dir / 'labels'\n",
    "\n",
    "# Get all filenames in the images directory\n",
    "all_filenames = os.listdir(images_dir)\n",
    "\n",
    "# Initialize dictionaries and lists\n",
    "strip_to_files = {}        # For files with strip numbers\n",
    "possibly_empty_files = []  # For 'possibly_empty' files\n",
    "\n",
    "# Regular expression pattern to match filenames with strip numbers\n",
    "pattern = re.compile(r'^area_\\d+_of_strip_(\\d+)\\.npy$')\n",
    "\n",
    "# Process filenames\n",
    "for filename in all_filenames:\n",
    "    if 'possibly_empty' in filename:\n",
    "        # This is a 'possibly_empty' file\n",
    "        possibly_empty_files.append(filename)\n",
    "    else:\n",
    "        # Try to match the pattern to extract strip number\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            strip_number = int(match.group(1))\n",
    "            # Add filename to the list for this strip number\n",
    "            strip_to_files.setdefault(strip_number, []).append(filename)\n",
    "        else:\n",
    "            print(f\"Filename does not match expected pattern: {filename}\")\n",
    "\n",
    "# List of all unique strip numbers\n",
    "strip_numbers = list(strip_to_files.keys())\n",
    "\n",
    "# Shuffle strip numbers for random splitting\n",
    "random.seed(RANDOM_SEED)  # Ensure reproducibility\n",
    "random.shuffle(strip_numbers)\n",
    "\n",
    "# Calculate split index for strips\n",
    "num_strips = len(strip_numbers)\n",
    "split_index = int(num_strips * config['train_percentage'])\n",
    "\n",
    "# Split strip numbers into train and test sets\n",
    "train_strip_numbers = strip_numbers[:split_index]\n",
    "val_strip_numbers = strip_numbers[split_index:]\n",
    "\n",
    "wandb.config.update({\n",
    "    'num_strips': num_strips,\n",
    "    'train_strip_numbers': train_strip_numbers,\n",
    "    'val_strip_numbers': val_strip_numbers,\n",
    "})\n",
    "\n",
    "# Collect filenames for train and test sets based on strip numbers\n",
    "train_files = []\n",
    "for strip_num in train_strip_numbers:\n",
    "    train_files.extend(strip_to_files[strip_num])\n",
    "\n",
    "val_files = []\n",
    "for strip_num in val_strip_numbers:\n",
    "    val_files.extend(strip_to_files[strip_num])\n",
    "\n",
    "# Now handle the 'possibly_empty' files\n",
    "# Shuffle the possibly_empty files\n",
    "random.shuffle(possibly_empty_files)\n",
    "\n",
    "# Calculate split index for possibly_empty files\n",
    "num_possibly_empty = len(possibly_empty_files)\n",
    "split_index_empty = int(num_possibly_empty * config['train_percentage'])\n",
    "\n",
    "# Split possibly_empty files into train and test sets\n",
    "train_possibly_empty_files = possibly_empty_files[:split_index_empty]\n",
    "val_possibly_empty_files = possibly_empty_files[split_index_empty:]\n",
    "\n",
    "# Add the possibly_empty files to the train and test file lists\n",
    "train_files.extend(train_possibly_empty_files)\n",
    "val_files.extend(val_possibly_empty_files)\n",
    "\n",
    "# Output some information\n",
    "print(f\"Total files: {len(all_filenames)}\")\n",
    "print(f\"Total strips: {len(strip_numbers)}\")\n",
    "print(f\"Training files: {len(train_files)}\")\n",
    "print(f\"Testing files: {len(val_files)}\")\n",
    "\n",
    "# Define your transform if you have one; otherwise, set to None\n",
    "segmentation_transform = None  # Replace with your actual transform if any\n",
    "\n",
    "# Create train dataset\n",
    "train_dataset = LandingStripDataset(\n",
    "    images_dir=images_dir,\n",
    "    labels_dir=labels_dir,\n",
    "    file_list=train_files,\n",
    "    transform=segmentation_transform\n",
    ")\n",
    "\n",
    "# Create test dataset\n",
    "val_dataset = LandingStripDataset(\n",
    "    images_dir=images_dir,\n",
    "    labels_dir=labels_dir,\n",
    "    file_list=val_files,\n",
    "    transform=segmentation_transform\n",
    ")\n",
    "\n",
    "if DEBUG:\n",
    "    train_dataset.samples = train_dataset.samples[:10]\n",
    "    val_dataset.samples = val_dataset.samples[:10]\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=config['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. Load the Geo Foundation Model (GFM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yacs in /home/emil/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages (0.1.8)\n",
      "Requirement already satisfied: PyYAML in /home/emil/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages (from yacs) (6.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install yacs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emil/Desktop/secret-runway-detection/secret_runway_detection/model.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample keys from filtered_state_dict:\n",
      "mask_token\n",
      "patch_embed.proj.weight\n",
      "patch_embed.proj.bias\n",
      "patch_embed.norm.weight\n",
      "patch_embed.norm.bias\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SwinTransformer:\n\tsize mismatch for layers.0.blocks.0.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 4]) from checkpoint, the shape in current model is torch.Size([169, 4]).\n\tsize mismatch for layers.0.blocks.1.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 4]) from checkpoint, the shape in current model is torch.Size([169, 4]).\n\tsize mismatch for layers.1.downsample.norm.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layers.1.downsample.norm.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layers.1.downsample.reduction.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for layers.1.blocks.0.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 8]) from checkpoint, the shape in current model is torch.Size([169, 8]).\n\tsize mismatch for layers.1.blocks.1.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 8]) from checkpoint, the shape in current model is torch.Size([169, 8]).\n\tsize mismatch for layers.2.downsample.norm.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for layers.2.downsample.norm.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for layers.2.downsample.reduction.weight: copying a param with shape torch.Size([1024, 2048]) from checkpoint, the shape in current model is torch.Size([512, 1024]).\n\tsize mismatch for layers.2.blocks.0.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.1.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.2.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.3.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.4.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.5.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.6.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.7.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.8.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.9.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.10.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.11.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.12.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.13.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.14.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.15.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.16.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.17.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.3.blocks.0.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 32]) from checkpoint, the shape in current model is torch.Size([169, 32]).\n\tsize mismatch for layers.3.blocks.1.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 32]) from checkpoint, the shape in current model is torch.Size([169, 32]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msecret_runway_detection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_upernet_segmentation_model, get_simple_segmentation_model\n\u001b[1;32m      4\u001b[0m backbone_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../simmim_pretrain/gfm.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_simple_segmentation_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackbone_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# model = get_upernet_segmentation_model(backbone_model_path)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/secret_runway_detection/model.py:270\u001b[0m, in \u001b[0;36mget_simple_segmentation_model\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_simple_segmentation_model\u001b[39m(model_path):\n\u001b[0;32m--> 270\u001b[0m     backbone \u001b[38;5;241m=\u001b[39m \u001b[43mSwinBackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     segmentation_head \u001b[38;5;241m=\u001b[39m SimpleSegmentationHead()\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SimpleSegmentationModel(backbone, segmentation_head)\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/secret_runway_detection/model.py:157\u001b[0m, in \u001b[0;36mSwinBackbone.__init__\u001b[0;34m(self, model_path, model_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mswin_base_patch4_window7_224\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28msuper\u001b[39m(SwinBackbone, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone \u001b[38;5;241m=\u001b[39m \u001b[43mload_gfm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/secret_runway_detection/model.py:57\u001b[0m, in \u001b[0;36mload_gfm_model\u001b[0;34m(model_name, model_path)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(k)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Load the filtered state_dict into the timm model\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m missing_keys, unexpected_keys \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_keys:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMissing keys when loading pretrained weights:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SwinTransformer:\n\tsize mismatch for layers.0.blocks.0.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 4]) from checkpoint, the shape in current model is torch.Size([169, 4]).\n\tsize mismatch for layers.0.blocks.1.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 4]) from checkpoint, the shape in current model is torch.Size([169, 4]).\n\tsize mismatch for layers.1.downsample.norm.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layers.1.downsample.norm.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layers.1.downsample.reduction.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for layers.1.blocks.0.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 8]) from checkpoint, the shape in current model is torch.Size([169, 8]).\n\tsize mismatch for layers.1.blocks.1.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 8]) from checkpoint, the shape in current model is torch.Size([169, 8]).\n\tsize mismatch for layers.2.downsample.norm.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for layers.2.downsample.norm.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for layers.2.downsample.reduction.weight: copying a param with shape torch.Size([1024, 2048]) from checkpoint, the shape in current model is torch.Size([512, 1024]).\n\tsize mismatch for layers.2.blocks.0.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.1.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.2.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.3.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.4.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.5.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.6.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.7.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.8.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.9.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.10.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.11.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.12.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.13.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.14.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.15.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.16.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.2.blocks.17.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 16]) from checkpoint, the shape in current model is torch.Size([169, 16]).\n\tsize mismatch for layers.3.blocks.0.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 32]) from checkpoint, the shape in current model is torch.Size([169, 32]).\n\tsize mismatch for layers.3.blocks.1.attn.relative_position_bias_table: copying a param with shape torch.Size([121, 32]) from checkpoint, the shape in current model is torch.Size([169, 32])."
     ]
    }
   ],
   "source": [
    "from secret_runway_detection.model import get_upernet_segmentation_model, get_simple_segmentation_model\n",
    "\n",
    "\n",
    "backbone_model_path = '../simmim_pretrain/gfm.pth'\n",
    "model = get_simple_segmentation_model(backbone_model_path)\n",
    "# model = get_upernet_segmentation_model(backbone_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_children():\n",
    "    print(f\"{name}: {module.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7. Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.backbone.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate parameters for different learning rates\n",
    "backbone_params = []\n",
    "new_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'backbone' in name:\n",
    "        backbone_params.append(param)\n",
    "    else:\n",
    "        new_params.append(param)\n",
    "\n",
    "print(f\"Backbone parameters: {len(backbone_params)}\")\n",
    "print(f\"New parameters: {len(new_params)}\")\n",
    "print(f\"Total parameters: {len(list(model.parameters()))}\")\n",
    "\n",
    "# Define optimizer with differential learning rates\n",
    "optimizer = optim.Adam([\n",
    "    {'params': backbone_params, 'lr': config['lr_backbone']},  # Lower learning rate for pretrained layers\n",
    "    {'params': new_params, 'lr': config['lr_head']}        # Higher learning rate for new layers\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "# Suitable for binary classification with logits\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optionally, define a learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=config['lr_step_size'], gamma=config['lr_gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'checkpoints' directory within the current directory\n",
    "os.makedirs('../checkpoints', exist_ok=True)\n",
    "\n",
    "# Define the model save path within the 'checkpoints' directory\n",
    "model_save_path = f'../checkpoints/{wandb.run.name}.pth'\n",
    "\n",
    "wandb.run.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Ensure numpy is imported\n",
    "\n",
    "def compute_validation_accuracy(model, val_dataloader, device):\n",
    "    \"\"\"\n",
    "    Computes the pixel-wise accuracy over the validation set for multiple thresholds.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model.\n",
    "        val_dataloader: DataLoader for the validation set.\n",
    "        device: The device (CPU or GPU) to perform computations on.\n",
    "\n",
    "    Returns:\n",
    "        best_accuracy (float): The highest accuracy achieved across thresholds.\n",
    "        best_threshold (float): The threshold corresponding to the best accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_pixels = 0\n",
    "    best_accuracy = 0.0\n",
    "    best_threshold = 0.0\n",
    "    thresholds = np.linspace(0.0, 1.0, 11)  # Thresholds from 0.0 to 1.0, inclusive\n",
    "    threshold_correct = {threshold: 0 for threshold in thresholds}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze(1)  # Adjust dimensions if necessary\n",
    "\n",
    "            # Apply sigmoid to get probabilities\n",
    "            probs = torch.sigmoid(outputs)\n",
    "\n",
    "            # Move tensors to CPU and flatten for numpy operations\n",
    "            probs_np = probs.cpu().numpy().flatten()\n",
    "            labels_np = labels.cpu().numpy().flatten()\n",
    "\n",
    "            total_pixels += labels_np.size\n",
    "\n",
    "            # Evaluate at multiple thresholds\n",
    "            for threshold in thresholds:\n",
    "                preds = (probs_np >= threshold).astype(np.uint8)\n",
    "                correct = (preds == labels_np).sum()\n",
    "                threshold_correct[threshold] += correct\n",
    "\n",
    "    # Compute average accuracy for each threshold\n",
    "    for threshold in thresholds:\n",
    "        accuracy = threshold_correct[threshold] / total_pixels\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = threshold\n",
    "\n",
    "    return best_accuracy, best_threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 8. Training Loop with wandb Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = model.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb.feature_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, lab = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = bb(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actual feature map shapes:\")\n",
    "for i, feat in enumerate(features):\n",
    "    print(f\"Feature {i}: shape {feat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize early stopping variables before the training loop\n",
    "best_val_loss = float('inf')\n",
    "# Number of epochs to wait before stopping\n",
    "patience = config['early_stopping_patience']\n",
    "counter = 0   # Counter for early stopping\n",
    "\n",
    "# Before the training loop, watch the model\n",
    "wandb.watch(model, criterion=criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze(1)  # Adjust dimensions if necessary\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Log every 10 batches or last batch\n",
    "        if (i + 1) % 10 == 0 or i == len(train_dataloader):\n",
    "            avg_loss = running_loss / 10\n",
    "            print(f\"[Epoch {epoch + 1}, Batch {i + 1}] Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # Log metrics to wandb\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'batch': i + 1,\n",
    "                'training_loss': avg_loss,\n",
    "                'learning_rate': optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze(1)  # Adjust dimensions if necessary\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Accumulate validation loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch + 1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Compute validation accuracy and best threshold\n",
    "    best_accuracy, best_threshold = compute_validation_accuracy(model, val_dataloader, device)\n",
    "    print(f\"Best Validation Accuracy: {best_accuracy:.4f} at Threshold: {best_threshold:.2f}\")\n",
    "\n",
    "    # Log validation loss and accuracy to wandb\n",
    "    wandb.log({\n",
    "        'epoch': epoch + 1,\n",
    "        'validation_loss': avg_val_loss,\n",
    "        'validation_accuracy': best_accuracy,\n",
    "        'best_threshold': best_threshold\n",
    "    })\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        # Save the model\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Validation loss decreased to {avg_val_loss:.4f}, saving model to {model_save_path}\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"No improvement in validation loss for {counter} epoch(s).\")\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            # Optionally log early stopping to wandb\n",
    "            wandb.log({'early_stopping_epoch': epoch + 1})\n",
    "            break\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Find accuracy-optimizing thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 9. Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a W&B Artifact for the model\n",
    "artifact = wandb.Artifact('model', type='model')\n",
    "\n",
    "# Add the saved model file to the artifact\n",
    "artifact.add_file(model_save_path)\n",
    "\n",
    "# Log the artifact to W&B\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USING_COLAB:\n",
    "    from google.colab import runtime\n",
    "    runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
