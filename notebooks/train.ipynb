{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Landing Strip Detection Training Pipeline\n",
    "\n",
    "\n",
    "\n",
    " This notebook implements a training pipeline for detecting landing strips using satellite imagery. The pipeline includes:\n",
    "\n",
    "\n",
    "\n",
    " - Loading input landing strip data.\n",
    "\n",
    " - Creating input areas around the landing strips.\n",
    "\n",
    " - Downloading Sentinel-2 imagery from Google Earth Engine.\n",
    "\n",
    " - Preparing a dataset for training.\n",
    "\n",
    " - Loading the Geo Foundation Model (GFM) for transfer learning.\n",
    "\n",
    " - Setting up a training loop with Weights & Biases (wandb) logging.\n",
    "\n",
    "\n",
    "\n",
    " **Note**: Ensure that you have authenticated with Google Earth Engine (GEE) using `ee.Authenticate()` and have initialized it with `ee.Initialize()`. Also, make sure `train_utils.py` is in your working directory or Python path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import ee\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import timm  # PyTorch Image Models library\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src directory to the sys.path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import functions and constants from train_utils\n",
    "from secret_runway_detection.train_utils import (\n",
    "    TILES_PER_AREA_LEN,\n",
    "    RANDOM_SEED\n",
    ")\n",
    "\n",
    "from secret_runway_detection.dataset import LandingStripDataset\n",
    "\n",
    "from secret_runway_detection.model import (\n",
    "    SegmentationHead,\n",
    "    CombinedModel,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Configuration and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "# Debug flag: Set to True to run on CPU, False to use GPU if available\n",
    "DEBUG = True\n",
    "\n",
    "# Number of epochs to train for\n",
    "NUM_EPOCHS = 10  # Adjust as needed\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cpu') if DEBUG else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# logging.getLogger('secret_runway_detection.train_utils').setLevel(logging.DEBUG)\n",
    "logging.getLogger('secret_runway_detection.train_utils').setLevel(logging.INFO)\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project='secret-runway-detection', mode='online' if not DEBUG else 'dryrun')\n",
    "\n",
    "# Authenticate and initialize Earth Engine\n",
    "ee.Authenticate()\n",
    "ee.Initialize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Load Data into Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 9 samples\n"
     ]
    }
   ],
   "source": [
    "# Define paths to the images and labels directories\n",
    "images_dir = Path('../training_data/images')\n",
    "labels_dir = Path('../training_data/labels')\n",
    "\n",
    "# # Optionally, define transformations\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Standard ImageNet normalization\n",
    "#                          std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# Create the dataset\n",
    "dataset = LandingStripDataset(images_dir, labels_dir)\n",
    "print(f\"Dataset size: {len(dataset)} samples\")\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. Load the Geo Foundation Model (GFM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_311594/2527543172.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and moved to device.\n"
     ]
    }
   ],
   "source": [
    "def load_gfm_model(model_path):\n",
    "    \"\"\"\n",
    "    Loads the Geo Foundation Model (GFM) from a checkpoint.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_path (str): Path to the model checkpoint.\n",
    "    \n",
    "    Returns:\n",
    "    - model (torch.nn.Module): Loaded model.\n",
    "    \"\"\"\n",
    "    model = timm.create_model(\n",
    "        'swin_base_patch4_window7_224',\n",
    "        pretrained=False,\n",
    "        num_classes=0,  # Assuming binary classification\n",
    "    )\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    \n",
    "    # Extract the state dictionary\n",
    "    if 'model' in checkpoint:\n",
    "        state_dict = checkpoint['model']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    # Clean the state dictionary (remove 'module.' prefix if present)\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith('module.'):\n",
    "            new_state_dict[k[len('module.'):]] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    \n",
    "    # Load the state dictionary\n",
    "    model.load_state_dict(new_state_dict, strict=False)\n",
    "    model = model.to(device)\n",
    "    print(\"Model loaded and moved to device.\")\n",
    "    return model\n",
    "\n",
    "# Path to the pre-trained GFM model\n",
    "backbone_model_path = '../simmim_pretrain/gfm.pth'  # Replace with your actual model path\n",
    "\n",
    "# Load the model\n",
    "backbone_model = load_gfm_model(backbone_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Add Segmentation Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_head = SegmentationHead()\n",
    "\n",
    "model = CombinedModel(backbone_model, segmentation_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for forward pass: 0.39 seconds\n",
      "torch.Size([200, 200])\n"
     ]
    }
   ],
   "source": [
    "# Time one forward pass\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "model(dataset[0][0].unsqueeze(0))\n",
    "\n",
    "print(f\"Time taken for forward pass: {time.time() - start_time:.2f} seconds\")\n",
    "print(dataset[0][1].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7. Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Suitable for binary classification with logits\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Optionally, define a learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 8. Training Loop with wandb Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch 3] Loss: 0.2594\n",
      "Epoch 1 completed. Learning Rate: 0.0001\n",
      "[Epoch 2, Batch 3] Loss: 0.2598\n",
      "Epoch 2 completed. Learning Rate: 0.0001\n",
      "[Epoch 3, Batch 3] Loss: 0.2570\n",
      "Epoch 3 completed. Learning Rate: 0.0001\n",
      "[Epoch 4, Batch 3] Loss: 0.2569\n",
      "Epoch 4 completed. Learning Rate: 0.0001\n",
      "[Epoch 5, Batch 3] Loss: 0.2561\n",
      "Epoch 5 completed. Learning Rate: 0.0001\n",
      "[Epoch 6, Batch 3] Loss: 0.2540\n",
      "Epoch 6 completed. Learning Rate: 0.0001\n",
      "[Epoch 7, Batch 3] Loss: 0.2553\n",
      "Epoch 7 completed. Learning Rate: 1e-05\n",
      "[Epoch 8, Batch 3] Loss: 0.2537\n",
      "Epoch 8 completed. Learning Rate: 1e-05\n",
      "[Epoch 9, Batch 3] Loss: 0.2527\n",
      "Epoch 9 completed. Learning Rate: 1e-05\n",
      "[Epoch 10, Batch 3] Loss: 0.2528\n",
      "Epoch 10 completed. Learning Rate: 1e-05\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze(1)  # Adjust dimensions if necessary\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9 or i == len(dataloader) - 1:  # Log every 10 batches or last batch\n",
    "            avg_loss = running_loss / 10\n",
    "            print(f\"[Epoch {epoch + 1}, Batch {i + 1}] Loss: {avg_loss:.4f}\")\n",
    "            wandb.log({'epoch': epoch + 1, 'batch': i + 1, 'loss': avg_loss})\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Optionally, log learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    wandb.log({'learning_rate': current_lr})\n",
    "    print(f\"Epoch {epoch + 1} completed. Learning Rate: {current_lr}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 9. Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to 'trained_model.pth'.\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model_save_path = '../checkpoints/trained_model.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to '{model_save_path}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 10. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Training Summary\n",
      "\n",
      "- **Model**: Swin Transformer (GFM) loaded from pre-trained checkpoint.\n",
      "- **Dataset**: Landing strips with Sentinel-2 imagery.\n",
      "- **Loss Function**: BCEWithLogitsLoss.\n",
      "- **Optimizer**: Adam with learning rate scheduler.\n",
      "- **Logging**: Weights & Biases (wandb) for experiment tracking.\n",
      "- **Device**: cpu\n",
      "- **Epochs**: 10\n",
      "\n",
      "Training has been completed and the model has been saved.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "# Training Summary\n",
    "\n",
    "- **Model**: Swin Transformer (GFM) loaded from pre-trained checkpoint.\n",
    "- **Dataset**: Landing strips with Sentinel-2 imagery.\n",
    "- **Loss Function**: BCEWithLogitsLoss.\n",
    "- **Optimizer**: Adam with learning rate scheduler.\n",
    "- **Logging**: Weights & Biases (wandb) for experiment tracking.\n",
    "- **Device**: {}\n",
    "- **Epochs**: {}\n",
    "\n",
    "Training has been completed and the model has been saved.\n",
    "\"\"\".format(device, NUM_EPOCHS))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
