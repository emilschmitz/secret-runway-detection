{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Things to try to increase LB score\n",
    "\n",
    " 1. Change way in which buffer region is constucted (Cross, Square, Ball)\n",
    "\n",
    " 1. Change submission csv method to transpose rows and cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Submitting first 200 000, we have\n",
    "\n",
    " * 0.025533852 accuracy with only zeros.\n",
    "\n",
    " * 0.00043453 accuracy with only ones.\n",
    "\n",
    " * ie, 0.00043453 / (0.00043453 + 0.025533852) = 0.01673304097 ~= **1.67 %** rate of ones\n",
    "\n",
    "\n",
    "\n",
    " ....We should maybe consider that when setting threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Estimated time for getting **25m rows**:\n",
    "\n",
    " * One forward pass gives 40 000 rows\n",
    "\n",
    " * One forward pass takes approx. 0.5 sec\n",
    "\n",
    " * 25m / 40 000 = 625\n",
    "\n",
    " * 625 * 0.5 ~= 300 sec = **5 min**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Inference Notebook\n",
    "\n",
    "\n",
    "\n",
    " This notebook performs inference on test AOIs using the trained model. It reads the AOIs from shapefiles, processes each AOI through the model, and aggregates the results into final prediction tensors. The predictions are then converted into submission CSV files.\n",
    "\n",
    "\n",
    "\n",
    " The methods from `inference_utils.py` and `train_utils.py` are imported and used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** change model to do 224 -> 224 or 224 -> 112. Otherwise, rounding errors m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add the src directory to the sys.path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import functions and constants from inference_utils and train_utils\n",
    "from secret_runway_detection.inference_utils import (\n",
    "    has_strip_tensors_to_submission_csv,\n",
    ")\n",
    "\n",
    "from secret_runway_detection.train_utils import (\n",
    "    add_buffer_to_label,\n",
    ")\n",
    "\n",
    "from secret_runway_detection.model import get_model\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "\n",
    "ADD_BUFFER = True\n",
    "BUFFER_TYPE = 'cross'\n",
    "\n",
    "# Tile and AOI parameters\n",
    "TILE_SIDE_LEN = 10.0  # in meters\n",
    "# AOI_HEIGHT = 15270.0  # in meters\n",
    "# AOI_WIDTH = 15410.0   # in meters\n",
    "\n",
    "# # ROWS_COUNT = 1527  # Number of tile rows\n",
    "# # COLUMNS_COUNT = 1541  # Number of tile columns\n",
    "\n",
    "# assert (TILE_SIDE_LEN == AOI_HEIGHT / ROWS_COUNT) and (TILE_SIDE_LEN == AOI_WIDTH / COLUMNS_COUNT)\n",
    "\n",
    "# Model input and output dimensions\n",
    "INPUT_IMAGE_SIDE_LEN_PX = 224  # in pixels\n",
    "TILES_PER_INPUT_AREA_LEN = 224  # Number of tiles per side in one input area\n",
    "\n",
    "# Number of input areas to cover the AOI\n",
    "INPUT_AREAS_VERTICALLY = 10\n",
    "INPUT_AREAS_HORIZONTALLY = 10\n",
    "\n",
    "# Threshold for converting model outputs to binary predictions\n",
    "THRESHOLD = 0.5  # Adjust based on validation performance\n",
    "\n",
    "# Path to the trained model checkpoint\n",
    "RUN_PATH = 'esedx12/secret-runway-detection/f1e9mc58'\n",
    "\n",
    "# Path to save the submission CSVs\n",
    "SUBMISSION_CSV_DIR = 'submission_csvs'\n",
    "os.makedirs(SUBMISSION_CSV_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Load the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wild-forest-84'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the W&B run\n",
    "import wandb\n",
    "\n",
    "\n",
    "train_run = wandb.Api().run(RUN_PATH)\n",
    "train_run.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model:v29, 406.63MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: midfeat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone_config gfm_config:v1\n",
      "model model:v29\n",
      "=> merge config from ../configs/gfm_config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emil/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading pretrained weights...\n",
      "\n",
      "Missing keys when loading pretrained weights:\n",
      "  - head.weight\n",
      "  - head.bias\n",
      "\n",
      "Unexpected keys when loading pretrained weights:\n",
      "  - mask_token\n",
      "\n",
      "Pretrained weights loaded with some missing/unexpected keys.\n",
      "Incompatible keys: _IncompatibleKeys(missing_keys=['decode_head.resblocks.1.conv1.weight', 'decode_head.resblocks.1.conv1.bias', 'decode_head.resblocks.1.bn1.weight', 'decode_head.resblocks.1.bn1.bias', 'decode_head.resblocks.1.bn1.running_mean', 'decode_head.resblocks.1.bn1.running_var', 'decode_head.resblocks.1.conv2.weight', 'decode_head.resblocks.1.conv2.bias', 'decode_head.resblocks.1.bn2.weight', 'decode_head.resblocks.1.bn2.bias', 'decode_head.resblocks.1.bn2.running_mean', 'decode_head.resblocks.1.bn2.running_var', 'decode_head.resblocks.2.conv1.weight', 'decode_head.resblocks.2.conv1.bias', 'decode_head.resblocks.2.bn1.weight', 'decode_head.resblocks.2.bn1.bias', 'decode_head.resblocks.2.bn1.running_mean', 'decode_head.resblocks.2.bn1.running_var', 'decode_head.resblocks.2.conv2.weight', 'decode_head.resblocks.2.conv2.bias', 'decode_head.resblocks.2.bn2.weight', 'decode_head.resblocks.2.bn2.bias', 'decode_head.resblocks.2.bn2.running_mean', 'decode_head.resblocks.2.bn2.running_var'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from more_itertools import one\n",
    "\n",
    "# Fetch the model artifact from the W&B run\n",
    "artifacts = train_run.logged_artifacts()\n",
    "model_artifacts = [a for a in artifacts if a.type == 'model']\n",
    "artifact = one(model_artifacts)\n",
    "state_dict_dir = artifact.download(root='../artifacts/')\n",
    "state_dict_dir = Path(state_dict_dir)\n",
    "state_dict_path = state_dict_dir / f'{train_run.name}.pth'\n",
    "state_dict_path\n",
    "\n",
    "# %%\n",
    "model_type = train_run.config['model_type']\n",
    "print(f\"Model type: {model_type}\")\n",
    "\n",
    "# %%\n",
    "# Now load the type backbone_config artifact the same way you loaded the model and put it in a temp directory\n",
    "backbone_config_artifacts = [a for a in artifacts if a.type == 'backbone_config']\n",
    "backbone_config_artifact = one(backbone_config_artifacts)\n",
    "backbone_config_dir = backbone_config_artifact.download(root='../artifacts/')\n",
    "backbone_config_dir = Path(backbone_config_dir)\n",
    "backbone_config_path = backbone_config_dir / f'{train_run.name}.yaml'\n",
    "backbone_config_path\n",
    "\n",
    "\n",
    "# %%\n",
    "list(artifacts)\n",
    "\n",
    "for artifact in artifacts:\n",
    "    print(artifact.type, artifact.name)\n",
    "\n",
    "# %%\n",
    "# Then load the model like this\n",
    "# model = get_model(config['model_type'], BACKBONE_CFG_PATH, backbone_weights_path, output_size=config['resolution']).to(device)\n",
    "sys.path.append(os.path.abspath('../GFM'))\n",
    "\n",
    "from secret_runway_detection.model import get_model\n",
    "\n",
    "BACKBONE_CFG_PATH = '../configs/gfm_config.yaml'\n",
    "backbone_weights_path = '../simmim_pretrain/gfm.pth'\n",
    "\n",
    "\n",
    "model = get_model(train_run.config['model_type'], BACKBONE_CFG_PATH, backbone_weights_path, output_size=train_run.config['resolution']).to(device)\n",
    "\n",
    "# %%\n",
    "# Load the Model from WandB, which we saved as state dict\n",
    "# Load the Model from WandB, which we saved as state dict\n",
    "state_dict = torch.load(state_dict_path, map_location=device)\n",
    "incompatible_keys = model.load_state_dict(state_dict, strict=False)\n",
    "print(f\"Incompatible keys: {incompatible_keys}\")\n",
    "model.eval()\n",
    "\n",
    "# %%\n",
    "best_threshold = train_run.summary['best_threshold']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images of AOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AOI: aoi_2021_04\n",
      "Mosaic image loaded for AOI: aoi_2021_04\n",
      "\n",
      "Loaded 1 AOI images into the dictionary.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Load AOI Mosaics Based on Shapefile Names and Check for Missing Mosaics\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "# Directory containing AOI shapefiles\n",
    "AOI_SHAPEFILES_DIR = '../shp_test_AOIs/shp'  # Adjust the path as necessary\n",
    "\n",
    "# Directory where the mosaic images are saved\n",
    "mosaic_images_dir = '../aoi_mosaic_images'  # Adjust the path as necessary\n",
    "\n",
    "# Initialize an empty dictionary to store the images\n",
    "aoi_images_dict = {}\n",
    "\n",
    "# List all shapefiles in the directory\n",
    "aoi_shapefiles = [f for f in os.listdir(AOI_SHAPEFILES_DIR) if f.endswith('.shp')]\n",
    "\n",
    "# Extract AOI IDs from the shapefile names\n",
    "aoi_ids = [os.path.splitext(f)[0] for f in aoi_shapefiles]\n",
    "\n",
    "if DEBUG:\n",
    "    aoi_ids = aoi_ids[:1]  # Limit the number of AOIs to load for debugging\n",
    "\n",
    "# Iterate over each AOI ID and attempt to load the corresponding mosaic\n",
    "for aoi_id in aoi_ids:\n",
    "    mosaic_filename = f'{aoi_id}_mosaic.tif'\n",
    "    mosaic_file_path = os.path.join(mosaic_images_dir, mosaic_filename)\n",
    "    print(f\"Processing AOI: {aoi_id}\")\n",
    "    \n",
    "    # Check if the mosaic file exists\n",
    "    if not os.path.exists(mosaic_file_path):\n",
    "        raise FileNotFoundError(f\"Mosaic file not found for AOI {aoi_id}: {mosaic_file_path}\")\n",
    "    else:\n",
    "        # Open the image using rasterio\n",
    "        with rasterio.open(mosaic_file_path) as src:\n",
    "            # Read the image bands\n",
    "            img_data = src.read()\n",
    "            # Store the image data in the dictionary\n",
    "            aoi_images_dict[aoi_id] = img_data\n",
    "        print(f\"Mosaic image loaded for AOI: {aoi_id}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(aoi_images_dict)} AOI images into the dictionary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Run Inference on Each AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from memory_profiler import profile\n",
    "import cv2\n",
    "\n",
    "\n",
    "# @profile\n",
    "def aoi_image_to_has_strip_confidence(model: torch.nn.Module, aoi_image: np.ndarray, tiles_heightwise: int, tiles_lengthwise: int, \n",
    "                         model_input_pixels_per_side: int, model_output_tiles_per_side: int, stride: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Function to run inference on an AOI image and return the confidence map for the presence of a strip in each tile.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model used for inference.\n",
    "        aoi_image (np.ndarray): The input AOI image as a NumPy array.\n",
    "        tiles_heightwise (int): The number of tiles along the height of the image.\n",
    "        tiles_lengthwise (int): The number of tiles along the length of the image.\n",
    "        model_input_pixels_per_side (int): The number of pixels per side of the model's input.\n",
    "        model_output_tiles_per_side (int): The number of tiles per side of the model's output.\n",
    "        stride (int): The stride value for sliding the window over the image.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor representing the confidence map for the presence of a strip in each tile.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Resample aoi_image so that it has dims (tiles_heightwise * model_input_pixels_per_side / model_output_tiles_per_side, tiles_lengthwise * model_input_pixels_per_side / model_output_tiles_per_side)\n",
    "    if not model_input_pixels_per_side % model_output_tiles_per_side == 0:\n",
    "        raise ValueError(\"model_input_pixels_per_side must be divisible by model_output_tiles_per_side.\")\n",
    "    pixels_per_tile = model_input_pixels_per_side // model_output_tiles_per_side\n",
    "    pixels_height = tiles_heightwise * pixels_per_tile\n",
    "    pixels_width = tiles_lengthwise * pixels_per_tile\n",
    "\n",
    "    image_t = np.transpose(aoi_image, axes=(1, 2, 0))\n",
    "    aoi_image_resampled_t = cv2.resize(image_t, (pixels_width, pixels_height), interpolation=cv2.INTER_CUBIC)\n",
    "    aoi_image_resampled = np.transpose(aoi_image_resampled_t, axes=(2, 0, 1))\n",
    "    aoi_tensor = torch.from_numpy(aoi_image_resampled).unsqueeze(0).float()\n",
    "\n",
    "    has_strip = torch.zeros(tiles_heightwise, tiles_lengthwise)\n",
    "    \n",
    "    end_idx_list = list(range(model_input_pixels_per_side, tiles_heightwise, stride)) + [tiles_heightwise]\n",
    "    end_jdx_list = list(range(model_input_pixels_per_side, tiles_lengthwise, stride)) + [tiles_lengthwise]\n",
    "    \n",
    "    for end_idx in tqdm(end_idx_list, desc=f\"Scanning AOI Rowwise\", leave=False):\n",
    "        for end_jdx in end_jdx_list:\n",
    "            start_idx = end_idx - model_input_pixels_per_side\n",
    "            start_jdx = end_jdx - model_input_pixels_per_side\n",
    "\n",
    "            input_tensor = aoi_tensor[:, :, start_idx * pixels_per_tile:end_idx * pixels_per_tile, start_jdx * pixels_per_tile:end_jdx * pixels_per_tile]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_tensor = model(input_tensor).squeeze()\n",
    "\n",
    "            has_strip[start_idx:end_idx, start_jdx:end_jdx] = torch.max(\n",
    "                output_tensor, \n",
    "                has_strip[start_idx:end_idx, start_jdx:end_jdx]\n",
    "            )\n",
    "            gc.collect()\n",
    "\n",
    "    return has_strip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15fa7dad9bcb44ad907ce6e16a618f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing AOIs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AOI: aoi_2021_04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050e758c110945a89c95112e3652162c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scanning AOI Rowwise:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected AOI name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maoi_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing AOI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maoi_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m confidence_map \u001b[38;5;241m=\u001b[39m \u001b[43maoi_image_to_has_strip_confidence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43maoi_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maoi_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtiles_heightwise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtiles_heightwise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtiles_lengthwise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtiles_lengthwise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_input_pixels_per_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresolution\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_output_tiles_per_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresolution\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINPUT_IMAGE_SIDE_LEN_PX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m aoi_confidence_maps[aoi_name] \u001b[38;5;241m=\u001b[39m confidence_map\n\u001b[1;32m     35\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "Cell \u001b[0;32mIn[8], line 52\u001b[0m, in \u001b[0;36maoi_image_to_has_strip_confidence\u001b[0;34m(model, aoi_image, tiles_heightwise, tiles_lengthwise, model_input_pixels_per_side, model_output_tiles_per_side, stride)\u001b[0m\n\u001b[1;32m     49\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m aoi_tensor[:, :, start_idx \u001b[38;5;241m*\u001b[39m pixels_per_tile:end_idx \u001b[38;5;241m*\u001b[39m pixels_per_tile, start_jdx \u001b[38;5;241m*\u001b[39m pixels_per_tile:end_jdx \u001b[38;5;241m*\u001b[39m pixels_per_tile]\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 52\u001b[0m     output_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     54\u001b[0m has_strip[start_idx:end_idx, start_jdx:end_jdx] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(\n\u001b[1;32m     55\u001b[0m     output_tensor, \n\u001b[1;32m     56\u001b[0m     has_strip[start_idx:end_idx, start_jdx:end_jdx]\n\u001b[1;32m     57\u001b[0m )\n\u001b[1;32m     58\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/secret_runway_detection/model.py:314\u001b[0m, in \u001b[0;36mMidFeatSegmentationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    308\u001b[0m     features[key] \u001b[38;5;241m=\u001b[39m features[key]\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), features[key]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \n\u001b[1;32m    309\u001b[0m         sqrt_dim, sqrt_dim)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# features['layer1'] = self.conv_double(features['layer1'])\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# Pass the ordered feature list to the decode head  \u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlayer0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/secret_runway_detection/model.py:405\u001b[0m, in \u001b[0;36mDeepSegmentationHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m resblock \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresblocks:\n\u001b[1;32m    403\u001b[0m     x \u001b[38;5;241m=\u001b[39m resblock(x)\n\u001b[0;32m--> 405\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_conv(x)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Ensure output size matches desired dimensions\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/secret-runway-detection/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "aoi_confidence_maps = {}\n",
    "\n",
    "for aoi_name, aoi_image in tqdm(aoi_images_dict.items(), desc=\"Processing AOIs\"):\n",
    "    # Areas 2021_01 and 2021_02 have 1525 tiles heightwise\n",
    "    tiles_lengthwise = 1541\n",
    "    if aoi_name in ['aoi_2021_01', 'aoi_2021_02']:\n",
    "        tiles_heightwise = 1525\n",
    "    elif aoi_name in [\n",
    "            'aoi_2021_04',\n",
    "            'aoi_2022_01',\n",
    "            'aoi_2024_01',\n",
    "            'aoi_2020_02',\n",
    "            'aoi_2020_03',\n",
    "            'aoi_2020_01',\n",
    "            'aoi_2022_02',\n",
    "            'aoi_2023_01',\n",
    "            'aoi_2021_03',\n",
    "    ]:\n",
    "        tiles_heightwise = 1527\n",
    "    else:\n",
    "        tiles_heightwise = None\n",
    "        raise ValueError(f\"Unexpected AOI name: {aoi_name}\")\n",
    "\n",
    "    print(f\"Processing AOI: {aoi_name}\")\n",
    "    confidence_map = aoi_image_to_has_strip_confidence(\n",
    "        model=model,\n",
    "        aoi_image=aoi_image,\n",
    "        tiles_heightwise=tiles_heightwise,\n",
    "        tiles_lengthwise=tiles_lengthwise,\n",
    "        model_input_pixels_per_side=train_run.config['resolution'],\n",
    "        model_output_tiles_per_side=train_run.config['resolution'],\n",
    "        stride=INPUT_IMAGE_SIDE_LEN_PX // 2\n",
    "    )\n",
    "    aoi_confidence_maps[aoi_name] = confidence_map\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_strip_maps = {}\n",
    "for aoi_name, confidence_map in aoi_confidence_maps.items():\n",
    "    has_strip_map = confidence_map > THRESHOLD\n",
    "    if ADD_BUFFER:\n",
    "        has_strip_map = add_buffer_to_label(has_strip_map, num_buffer_tiles=20, buffer_type=BUFFER_TYPE)\n",
    "    has_strip_maps[aoi_name] = has_strip_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = has_strip_tensors_to_submission_csv(has_strip_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize one AOI confidence map and the corresponding image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from skimage.transform import resize\n",
    "import matplotlib.colors as mcolors\n",
    "import torch\n",
    "\n",
    "# Function to normalize and prepare the satellite image for display\n",
    "def prepare_satellite_image(image_data):\n",
    "    # Stack bands into RGB format\n",
    "    img_rgb = np.dstack((image_data[0], image_data[1], image_data[2]))\n",
    "    # Normalize the image for display\n",
    "    img_rgb = img_rgb.astype(float)\n",
    "    img_rgb = (img_rgb - img_rgb.min()) / (img_rgb.max() - img_rgb.min())\n",
    "    return img_rgb\n",
    "\n",
    "# Function to overlay the confidence map on the satellite image\n",
    "def overlay_confidence_map(satellite_image, confidence_map):\n",
    "    print(\"Overlaying confidence map...\")\n",
    "    # Convert confidence_map to NumPy array if it's a PyTorch tensor\n",
    "    if isinstance(confidence_map, torch.Tensor):\n",
    "        confidence_map = confidence_map.detach().cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(satellite_image)\n",
    "    plt.title(\"Satellite Image with Confidence Map Overlay\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Resize the confidence map to match the satellite image dimensions\n",
    "    confidence_map_resized = resize(confidence_map, (satellite_image.shape[0], satellite_image.shape[1]),\n",
    "                                    order=1, preserve_range=True, anti_aliasing=False)\n",
    "    \n",
    "    # Create a color map for the confidence map\n",
    "    cmap = plt.cm.Reds\n",
    "    cmap.set_under(color='none')  # Make values below the threshold transparent\n",
    "    \n",
    "    # Define a threshold for visualization\n",
    "    THRESHOLD = 0.5  # Adjust based on your data\n",
    "    \n",
    "    # Overlay the confidence map\n",
    "    plt.imshow(confidence_map_resized, cmap=cmap, alpha=0.5, vmin=THRESHOLD, vmax=1)\n",
    "    \n",
    "    # Add a colorbar\n",
    "    plt.colorbar(label='Confidence Score')\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"Confidence map plot displayed.\")\n",
    "\n",
    "# Function to overlay the has-strip map on the satellite image\n",
    "def overlay_has_strip_map(satellite_image, has_strip_map):\n",
    "    print(\"Overlaying has-strip map...\")\n",
    "    # Convert has_strip_map to NumPy array if it's a PyTorch tensor\n",
    "    if isinstance(has_strip_map, torch.Tensor):\n",
    "        has_strip_map = has_strip_map.detach().cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(satellite_image)\n",
    "    plt.title(\"Satellite Image with Has-Strip Map Overlay\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Resize the has-strip map to match the satellite image dimensions\n",
    "    has_strip_map_resized = resize(has_strip_map.astype(float), (satellite_image.shape[0], satellite_image.shape[1]),\n",
    "                                   order=0, preserve_range=True, anti_aliasing=False)\n",
    "    \n",
    "    # Create a colormap for the has-strip map\n",
    "    cmap = mcolors.ListedColormap(['none', 'red'])\n",
    "    bounds = [0, 0.5, 1]\n",
    "    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "    \n",
    "    # Overlay the has-strip map\n",
    "    plt.imshow(has_strip_map_resized, cmap=cmap, norm=norm, alpha=0.5)\n",
    "    \n",
    "    # Add a legend\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='red', edgecolor='red', label='Has-Strip Area')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='lower right')\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"Has-strip map plot displayed.\")\n",
    "\n",
    "# Example usage\n",
    "# Assuming you have 'aoi_image', 'confidence_map', and 'has_strip_map' variables\n",
    "satellite_image = prepare_satellite_image(aoi_image)\n",
    "overlay_confidence_map(satellite_image, confidence_map)\n",
    "overlay_has_strip_map(satellite_image, has_strip_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# - Ensure that the coordinate reference systems (CRS) are consistent throughout the process.\n",
    "# - Verify the indexing of rows and columns in the `tensor_to_submission_csv` function to match the competition requirements.\n",
    "# - The code assumes that the helper functions are correctly defined in `inference_utils.py` and `train_utils.py`.\n",
    "# - Adjust paths and constants as necessary based on your project structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "print(\"Inference process completed for all AOIs. Submission files are ready.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
