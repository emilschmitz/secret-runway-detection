{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'GFM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      6\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../GFM\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mGFM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_model\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mGFM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'GFM'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "sys.path.append(os.path.abspath('../GFM'))\n",
    "from GFM.models import build_model\n",
    "from GFM.config import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import necessary modules\n",
    "import torch\n",
    "from types import SimpleNamespace\n",
    "import yaml\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 4. Define a helper class to simulate argparse.Namespace\n",
    "class Args:\n",
    "    def __init__(self, cfg, opts=None, batch_size=None, data_path=None, pretrained=None,\n",
    "                 resume=None, accumulation_steps=None, use_checkpoint=False, amp_opt_level=None,\n",
    "                 output=None, tag=None, eval=False, throughput=False, train_frac=None,\n",
    "                 no_val=False, alpha=None, local_rank=0):\n",
    "        self.cfg = cfg\n",
    "        self.opts = opts\n",
    "        self.batch_size = batch_size\n",
    "        self.data_path = data_path\n",
    "        self.pretrained = pretrained\n",
    "        self.resume = resume\n",
    "        self.accumulation_steps = accumulation_steps\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.amp_opt_level = amp_opt_level\n",
    "        self.output = output\n",
    "        self.tag = tag\n",
    "        self.eval = eval\n",
    "        self.throughput = throughput\n",
    "        self.train_frac = train_frac\n",
    "        self.no_val = no_val\n",
    "        self.alpha = alpha\n",
    "        self.local_rank = local_rank\n",
    "\n",
    "# 5. Create an instance of Args with the necessary attributes\n",
    "args = Args(\n",
    "    cfg='../configs/gfm_config.yaml',                # Path to BEN.yaml\n",
    "    # opts=None,                                        # Additional options (if any)\n",
    "    # batch_size=None,                                  # Use the value from BEN.yaml\n",
    "    # data_path='/path/to/your/dataset',                # **Replace with your actual dataset path**\n",
    "    pretrained='../simmim_pretrain/gfm.pth',           # Path to gfm.pth\n",
    "    # resume=None,                                      # Resume from checkpoint (if any)\n",
    "    # accumulation_steps=None,                          # Gradient accumulation steps\n",
    "    # use_checkpoint=False,                             # Whether to use gradient checkpointing\n",
    "    # amp_opt_level='O1',                               # Mixed precision opt level ('O0', 'O1', 'O2')\n",
    "    # output='output',                                  # Output directory\n",
    "    # tag='simmim_finetune__swin_base__img128_window4__100ep',  # Experiment tag\n",
    "    # eval=False,                                       # Set to True for evaluation only\n",
    "    # throughput=False,                                 # Set to True to test throughput only\n",
    "    # train_frac=1.0,                                   # Fraction of training data to use\n",
    "    # no_val=False,                                     # Whether to skip validation\n",
    "    # alpha=None,                                       # Mixup/Cutmix alpha (if applicable)\n",
    "    # local_rank=0                                      # Local rank for DistributedDataParallel\n",
    ")\n",
    "\n",
    "# 6. Load the configuration using get_config\n",
    "config = get_config(args)\n",
    "\n",
    "# # 7. Print the configuration for verification\n",
    "# print(\"Configuration:\")\n",
    "# print(config)\n",
    "\n",
    "\n",
    "# # 10. Determine the device to run the model on (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# 11. Create the output directory if it doesn't exist\n",
    "os.makedirs(config.OUTPUT, exist_ok=True)\n",
    "\n",
    "# 12. Instantiate the model using build_model\n",
    "print(\"\\nBuilding the model...\")\n",
    "model = build_model(config, is_pretrain=False)  # Set is_pretrain=True if building pretraining model\n",
    "\n",
    "# 13. Move the model to the specified device\n",
    "model = model.to(device)\n",
    "print(\"Model has been moved to the device.\")\n",
    "\n",
    "# 14. Print the model architecture (optional)\n",
    "print(\"\\nModel Architecture:\")\n",
    "# print(model)\n",
    "\n",
    "# 15. Load the pretrained weights into the model\n",
    "# Assuming that 'build_model' does not automatically load the weights, perform it manually\n",
    "\n",
    "print(\"\\nLoading pretrained weights...\")\n",
    "state_dict = torch.load(config.PRETRAINED, map_location=device)\n",
    "state_dict = state_dict['model']\n",
    "\n",
    "# Remove 'module.' prefix if the model was saved using DataParallel\n",
    "state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "state_dict = {k.replace('encoder.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "# Optionally, filter out keys related to 'teacher.' and 'projector.' if they exist\n",
    "filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith(('teacher.', 'projector.'))}\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "missing_keys, unexpected_keys = model.load_state_dict(filtered_state_dict, strict=False)\n",
    "\n",
    "# Print missing and unexpected keys for debugging\n",
    "if missing_keys:\n",
    "    print(\"\\nMissing keys when loading pretrained weights:\")\n",
    "    for key in missing_keys:\n",
    "        print(f\"  - {key}\")\n",
    "if unexpected_keys:\n",
    "    print(\"\\nUnexpected keys when loading pretrained weights:\")\n",
    "    for key in unexpected_keys:\n",
    "        print(f\"  - {key}\")\n",
    "\n",
    "if not missing_keys and not unexpected_keys:\n",
    "    print(\"\\nAll pretrained weights loaded successfully!\")\n",
    "else:\n",
    "    print(\"\\nPretrained weights loaded with some missing/unexpected keys.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 16. Test the model with a dummy input (optional)\n",
    "# This helps verify that the model is operational\n",
    "dummy_input = torch.randn(1, 3, config.DATA.IMG_SIZE, config.DATA.IMG_SIZE).to(device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "print(f\"\\nDummy input output shape: {output.shape}\")  # Expected: [1, NUM_CLASSES, H, W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
